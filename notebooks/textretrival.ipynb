{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3 install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3 install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: langchain-cli in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (0.0.21)\n",
      "Requirement already satisfied: gitpython<4.0.0,>=3.1.40 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from langchain-cli) (3.1.40)\n",
      "Requirement already satisfied: langserve[all]>=0.0.16 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from langchain-cli) (0.0.51)\n",
      "Requirement already satisfied: tomlkit<0.13.0,>=0.12.2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from langchain-cli) (0.12.4)\n",
      "Requirement already satisfied: typer[all]<0.10.0,>=0.9.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from langchain-cli) (0.9.0)\n",
      "Requirement already satisfied: uvicorn<0.24.0,>=0.23.2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from langchain-cli) (0.23.2)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from gitpython<4.0.0,>=3.1.40->langchain-cli) (4.0.11)\n",
      "Requirement already satisfied: fastapi<1,>=0.90.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from langserve[all]>=0.0.16->langchain-cli) (0.104.1)\n",
      "Requirement already satisfied: httpx>=0.23.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from langserve[all]>=0.0.16->langchain-cli) (0.26.0)\n",
      "Requirement already satisfied: httpx-sse>=0.3.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from langserve[all]>=0.0.16->langchain-cli) (0.3.1)\n",
      "Requirement already satisfied: langchain>=0.0.333 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from langserve[all]>=0.0.16->langchain-cli) (0.1.3)\n",
      "Requirement already satisfied: orjson>=2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from langserve[all]>=0.0.16->langchain-cli) (3.9.15)\n",
      "Requirement already satisfied: pydantic>=1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from langserve[all]>=0.0.16->langchain-cli) (2.5.2)\n",
      "Requirement already satisfied: sse-starlette<2.0.0,>=1.3.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from langserve[all]>=0.0.16->langchain-cli) (1.8.2)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from typer[all]<0.10.0,>=0.9.0->langchain-cli) (8.1.7)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from typer[all]<0.10.0,>=0.9.0->langchain-cli) (4.8.0)\n",
      "Requirement already satisfied: colorama<0.5.0,>=0.4.3 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from typer[all]<0.10.0,>=0.9.0->langchain-cli) (0.4.6)\n",
      "Requirement already satisfied: shellingham<2.0.0,>=1.3.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from typer[all]<0.10.0,>=0.9.0->langchain-cli) (1.5.4)\n",
      "Requirement already satisfied: rich<14.0.0,>=10.11.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from typer[all]<0.10.0,>=0.9.0->langchain-cli) (13.7.0)\n",
      "Requirement already satisfied: h11>=0.8 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from uvicorn<0.24.0,>=0.23.2->langchain-cli) (0.14.0)\n",
      "Requirement already satisfied: anyio<4.0.0,>=3.7.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from fastapi<1,>=0.90.1->langserve[all]>=0.0.16->langchain-cli) (3.7.1)\n",
      "Requirement already satisfied: starlette<0.28.0,>=0.27.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from fastapi<1,>=0.90.1->langserve[all]>=0.0.16->langchain-cli) (0.27.0)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from gitdb<5,>=4.0.1->gitpython<4.0.0,>=3.1.40->langchain-cli) (5.0.1)\n",
      "Requirement already satisfied: certifi in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from httpx>=0.23.0->langserve[all]>=0.0.16->langchain-cli) (2023.11.17)\n",
      "Requirement already satisfied: httpcore==1.* in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from httpx>=0.23.0->langserve[all]>=0.0.16->langchain-cli) (1.0.2)\n",
      "Requirement already satisfied: idna in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from httpx>=0.23.0->langserve[all]>=0.0.16->langchain-cli) (3.6)\n",
      "Requirement already satisfied: sniffio in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from httpx>=0.23.0->langserve[all]>=0.0.16->langchain-cli) (1.3.0)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from langchain>=0.0.333->langserve[all]>=0.0.16->langchain-cli) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from langchain>=0.0.333->langserve[all]>=0.0.16->langchain-cli) (2.0.23)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from langchain>=0.0.333->langserve[all]>=0.0.16->langchain-cli) (3.9.1)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from langchain>=0.0.333->langserve[all]>=0.0.16->langchain-cli) (0.6.3)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from langchain>=0.0.333->langserve[all]>=0.0.16->langchain-cli) (1.33)\n",
      "Requirement already satisfied: langchain-community<0.1,>=0.0.14 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from langchain>=0.0.333->langserve[all]>=0.0.16->langchain-cli) (0.0.15)\n",
      "Requirement already satisfied: langchain-core<0.2,>=0.1.14 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from langchain>=0.0.333->langserve[all]>=0.0.16->langchain-cli) (0.1.15)\n",
      "Requirement already satisfied: langsmith<0.1,>=0.0.83 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from langchain>=0.0.333->langserve[all]>=0.0.16->langchain-cli) (0.0.83)\n",
      "Requirement already satisfied: numpy<2,>=1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from langchain>=0.0.333->langserve[all]>=0.0.16->langchain-cli) (1.26.2)\n",
      "Requirement already satisfied: requests<3,>=2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from langchain>=0.0.333->langserve[all]>=0.0.16->langchain-cli) (2.31.0)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from langchain>=0.0.333->langserve[all]>=0.0.16->langchain-cli) (8.2.3)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from pydantic>=1->langserve[all]>=0.0.16->langchain-cli) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.14.5 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from pydantic>=1->langserve[all]>=0.0.16->langchain-cli) (2.14.5)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from rich<14.0.0,>=10.11.0->typer[all]<0.10.0,>=0.9.0->langchain-cli) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/alejandronavarrogarcia/Library/Python/3.11/lib/python/site-packages (from rich<14.0.0,>=10.11.0->typer[all]<0.10.0,>=0.9.0->langchain-cli) (2.17.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.333->langserve[all]>=0.0.16->langchain-cli) (23.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.333->langserve[all]>=0.0.16->langchain-cli) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.333->langserve[all]>=0.0.16->langchain-cli) (1.9.4)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.333->langserve[all]>=0.0.16->langchain-cli) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.333->langserve[all]>=0.0.16->langchain-cli) (1.3.1)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain>=0.0.333->langserve[all]>=0.0.16->langchain-cli) (3.20.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain>=0.0.333->langserve[all]>=0.0.16->langchain-cli) (0.9.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from jsonpatch<2.0,>=1.33->langchain>=0.0.333->langserve[all]>=0.0.16->langchain-cli) (2.4)\n",
      "Requirement already satisfied: packaging<24.0,>=23.2 in /Users/alejandronavarrogarcia/Library/Python/3.11/lib/python/site-packages (from langchain-core<0.2,>=0.1.14->langchain>=0.0.333->langserve[all]>=0.0.16->langchain-cli) (23.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich<14.0.0,>=10.11.0->typer[all]<0.10.0,>=0.9.0->langchain-cli) (0.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests<3,>=2->langchain>=0.0.333->langserve[all]>=0.0.16->langchain-cli) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests<3,>=2->langchain>=0.0.333->langserve[all]>=0.0.16->langchain-cli) (1.26.18)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain>=0.0.333->langserve[all]>=0.0.16->langchain-cli) (1.0.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3 install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install requests beautifulsoup4 -q\n",
    "%pip install watson-developer-cloud==1.5 -q\n",
    "%pip install -U langchain-cli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from ibm_watsonx_ai.foundation_models import Model\n",
    "from ibm_watsonx_ai.metanames import GenTextParamsMetaNames as GenParams\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "api_key = os.getenv(\"WATSONX_API_KEY\", None)\n",
    "ibm_cloud_url = os.getenv(\"WATSONX_MODEL_ENDPOINT\", None)\n",
    "project_id = os.getenv(\"PROJECT_ID\", None)\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\", None)\n",
    "if api_key is None or ibm_cloud_url is None or project_id is None:\n",
    "    print(\"Ensure you copied the .env file that you created earlier into the same directory as this notebook\")\n",
    "else:\n",
    "    creds = {\n",
    "        \"url\": ibm_cloud_url,\n",
    "        \"apikey\": api_key \n",
    "    }\n",
    "\n",
    "model_params = {\n",
    "        GenParams.DECODING_METHOD: \"greedy\",\n",
    "        GenParams.MIN_NEW_TOKENS: 30,\n",
    "        GenParams.MAX_NEW_TOKENS: 2000,\n",
    "        GenParams.RANDOM_SEED: None,\n",
    "        GenParams.TEMPERATURE: 0,\n",
    "        GenParams.REPETITION_PENALTY: 1.05\n",
    "    }\n",
    "\n",
    "\n",
    "    # Instantiate a model proxy object to send your requests\n",
    "llm = Model(\n",
    "    model_id='meta-llama/llama-3-70b-instruct',\n",
    "    params=model_params,\n",
    "    credentials=creds,\n",
    "    project_id=project_id)\n",
    "\n",
    "llm_to_langchain=llm.to_langchain()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import re\n",
    "\n",
    "def extract_text_from_website(url):\n",
    "    # Fetch the HTML content of the webpage\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    # Check if the request was successful\n",
    "    if response.status_code == 200:\n",
    "        # Parse the HTML content\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        # Extract all text from the webpage\n",
    "        raw_text = soup.get_text()\n",
    "        \n",
    "        # Remove excess whitespaces and newlines\n",
    "        cleaned_text = re.sub(r'\\s+', ' ', raw_text).strip()\n",
    "        \n",
    "        # Remove HTML tags\n",
    "        cleaned_text = re.sub(r'<.*?>', '', cleaned_text)\n",
    "        \n",
    "        return cleaned_text\n",
    "    else:\n",
    "        # If request was unsuccessful, return None\n",
    "        print(\"Failed to retrieve webpage:\", response.status_code)\n",
    "        return None\n",
    "\n",
    "def scrape_multiple_websites(urls):\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        # Submit each URL for scraping concurrently\n",
    "        futures = [executor.submit(extract_text_from_website, url) for url in urls]\n",
    "        # Retrieve results as they become available\n",
    "        results = [future.result() for future in futures]\n",
    "        \n",
    "    # Store text from each URL as a separate item in a list\n",
    "    return results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "urls = [\n",
    "    \"https://ai.meta.com/blog/code-llama-large-language-model-coding/\", \n",
    "    \"https://huggingface.co/deepseek-ai/deepseek-coder-33b-instruct\",\n",
    "    \"https://huggingface.co/tiiuae/falcon-180B\",\n",
    "    \"https://huggingface.co/tiiuae/falcon-40b\",\n",
    "    \"https://huggingface.co/google/flan-t5-xl\",\n",
    "    \"https://huggingface.co/google/flan-t5-xxl\",\n",
    "    \"https://huggingface.co/google/flan-ul2\",\n",
    "    \"https://huggingface.co/core42/jais-13b-chat\",\n",
    "    \"https://huggingface.co/elyza/ELYZA-japanese-Llama-2-7b-instruct\",\n",
    "    \"https://huggingface.co/meta-llama/Llama-2-13b-hf\",\n",
    "    \"https://huggingface.co/mncai/llama2-13b-dpo-v7\",\n",
    "    \"https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2\",\n",
    "    \"https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1\",\n",
    "    \"https://huggingface.co/TheBloke/Mixtral-8x7B-v0.1-GPTQ\",\n",
    "    \"https://huggingface.co/intfloat/multilingual-e5-large\",\n",
    "    \"https://huggingface.co/kaist-ai/prometheus-13b-v1.0\",\n",
    "    \"https://www.ibm.com/docs/en/cloud-paks/cp-data/4.8.x?topic=models-granite-13b-v1-model-card\",\n",
    "    \"https://www.ibm.com/docs/en/cloud-paks/cp-data/4.8.x?topic=models-granite-13b-instruct-v2-model-card\",\n",
    "    \"https://www.ibm.com/docs/en/cloud-paks/cp-data/4.8.x?topic=models-granite-13b-chat-v2-model-card\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt_to_llm(prompt_template, **variables):\n",
    "    return llm_to_langchain.invoke(prompt_template.format(**variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_row_prompt = '''\n",
    "[INST] \n",
    "You are a helpful code assistant. Your task is to generate a valid JSON object based on the given information: {cleaned_model_data}.  \n",
    "Use the following schema: {json_schema}.\n",
    "Here is an example: {json_example}.\n",
    "Just generate the JSON object without explanations:\n",
    "[/INST] \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_schema = {\n",
    "    \"ModelName\": {\n",
    "      \"type\": \"string\"\n",
    "    },\n",
    "    \"Size\": {\n",
    "      \"type\": \"string\"\n",
    "    },\n",
    "    \"ID\": {\n",
    "      \"type\": \"string\"\n",
    "    },\n",
    "    \"Provider\": {\n",
    "      \"type\": \"string\"\n",
    "    },\n",
    "    \"Architecture\": {\n",
    "      \"type\": \"string\"\n",
    "    },\n",
    "    \"ContextLength\": {\n",
    "      \"type\": \"integer\"\n",
    "    },\n",
    "    \"Price\": {\n",
    "      \"type\": \"number\"\n",
    "    },\n",
    "    \"Languages\": {\n",
    "      \"type\": \"array\",\n",
    "      \"items\": {\n",
    "        \"type\": \"string\"\n",
    "      }\n",
    "    },\n",
    "    \"TunningInformation\": {\n",
    "      \"type\": \"string\"\n",
    "    },\n",
    "    \"TrainingData\": {\n",
    "      \"type\": \"string\"\n",
    "    },\n",
    "    \"UsesSupported\": {\n",
    "      \"type\": \"array\",\n",
    "      \"items\": {\n",
    "        \"type\": \"string\"\n",
    "      }\n",
    "    },\n",
    "    \"OptimisedFor\": {\n",
    "      \"type\": \"string\"\n",
    "    },\n",
    "    \"PromptingAdvice\": {\n",
    "      \"type\": \"string\"\n",
    "    },\n",
    "    \"Output\": {\n",
    "      \"type\": \"string\"\n",
    "    },\n",
    "    \"PromptTuningAvailability\": {\n",
    "      \"type\": \"boolean\"\n",
    "    },\n",
    "    \"RegionalAvailability\": {\n",
    "      \"type\": \"string\"\n",
    "    },\n",
    "    \"License\": {\n",
    "      \"type\": \"string\"\n",
    "    }\n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_example = '{\\n  \"ModelName\": \"Code Llama\",\\n  \"Size\": \"70B\",\\n  \"ID\": \"CodeLlama - 70B\",\\n  \"Provider\": \"Meta\",\\n  \"Architecture\": \"Decoder-only\",\\n  \"ContextLength\": 100000,\\n  \"Price\": \"Free\",\\n  \"Languages\": [\"Python\", \"C++\", \"Java\", \"PHP\", \"Typescript (Javascript)\", \"C#\", \"Bash\"],\\n  \"TunningInformation\": Instruction-tuned,\\n  \"TrainingData\": \"Code and code-related data\",\\n  \"UsesSupported\": [\"Code generation\", \"natural language about code\", \"debugging\"],\\n  \"OptimisedFor\": \"Low latency, real-time code completion, long inputs\",\\n  \"PromptingAdvice\": \"Provide the model with code or natural language prompts\",\\n  \"Output\": \"Code and natural language about code\",\\n  \"PromptTuningAvailability\": true,\\n  \"RegionalAvailability\": \"Not specified\",\\n  \"License\": \"Community license as Llama 2\"\\n}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Introducing Code Llama, a state-of-the-art large language model for coding Our approachResearchMeta AIMeta LlamaBlogTry Meta AIFEATUREDLarge Language Model Introducing Code Llama, a state-of-the-art large language model for codingAugust 24, 2023TakeawaysUpdate: Jan 29, 2024: Releasing Code Llama 70BWe are releasing Code Llama 70B, the largest and best-performing model in the Code Llama familyCode Llama 70B is available in the same three versions as previously released Code Llama models, all free for research and commercial use:CodeLlama - 70B, the foundational code model;CodeLlama - 70B - Python, 70B specialized for Python;and Code Llama - 70B - Instruct 70B, which is fine-tuned for understanding natural language instructions.Code Llama is a state-of-the-art LLM capable of generating code, and natural language about code, from both code and natural language prompts.Code Llama is free for research and commercial use.Code Llama is built on top of Llama 2 and is available in three models:Code Llama, the foundational code model;Codel Llama - Python specialized for Python;and Code Llama - Instruct, which is fine-tuned for understanding natural language instructions.In our own benchmark testing, Code Llama outperformed state-of-the-art publicly available LLMs on code tasksRECOMMENDED READSCode Llama research paperCode Llama GitHubDownload the Code Llama modelToday, we are releasing Code Llama, a large language model (LLM) that can use text prompts to generate code. Code Llama is state-of-the-art for publicly available LLMs on code tasks, and has the potential to make workflows faster and more efficient for current developers and lower the barrier to entry for people who are learning to code. Code Llama has the potential to be used as a productivity and educational tool to help programmers write more robust, well-documented software.The generative AI space is evolving rapidly, and we believe an open approach to today’s AI is the best one for developing new AI tools that are innovative, safe, and responsible. We are releasing Code Llama under the same community license as Llama 2.How Code Llama worksCode Llama is a code-specialized version of Llama 2 that was created by further training Llama 2 on its code-specific datasets, sampling more data from that same dataset for longer. Essentially, Code Llama features enhanced coding capabilities, built on top of Llama 2. It can generate code, and natural language about code, from both code and natural language prompts (e.g., “Write me a function that outputs the fibonacci sequence.”) It can also be used for code completion and debugging. It supports many of the most popular languages being used today, including Python, C++, Java, PHP, Typescript (Javascript), C#, and Bash.We are releasing four sizes of Code Llama with 7B, 13B, 34B, and 70B parameters respectively. Each of these models is trained with 500B tokens of code and code-related data, apart from 70B, which is trained on 1T tokens. The 7B and 13B base and instruct models have also been trained with fill-in-the-middle (FIM) capability, allowing them to insert code into existing code, meaning they can support tasks like code completion right out of the box.The three models address different serving and latency requirements. The 7B model, for example, can be served on a single GPU. The 34B and 70B models return the best results and allow for better coding assistance, but the smaller 7B and 13B models are faster and more suitable for tasks that require low latency, like real-time code completion.The Code Llama models provide stable generations with up to 100,000 tokens of context. All models are trained on sequences of 16,000 tokens and show improvements on inputs with up to 100,000 tokens.Aside from being a prerequisite for generating longer programs, having longer input sequences unlocks exciting new use cases for a code LLM. For example, users can provide the model with more context from their codebase to make the generations more relevant. It also helps in debugging scenarios in larger codebases, where staying on top of all code related to a concrete issue can be challenging for developers. When developers are faced with debugging a large chunk of code they can pass the entire length of the code into the model.Additionally, we have further fine-tuned two additional variations of Code Llama: Code Llama - Python and Code Llama - Instruct.Code Llama - Python is a language-specialized variation of Code Llama, further fine-tuned on 100B tokens of Python code. Because Python is the most benchmarked language for code generation – and because Python and PyTorch play an important role in the AI community – we believe a specialized model provides additional utility.Code Llama - Instruct is an instruction fine-tuned and aligned variation of Code Llama. Instruction tuning continues the training process, but with a different objective. The model is fed a “natural language instruction” input and the expected output. This makes it better at understanding what humans expect out of their prompts. We recommend using Code Llama - Instruct variants whenever using Code Llama for code generation since Code Llama - Instruct has been fine-tuned to generate helpful and safe answers in natural language.We do not recommend using Code Llama or Code Llama - Python to perform general natural language tasks since neither of these models are designed to follow natural language instructions. Code Llama is specialized for code-specific tasks and isn’t appropriate as a foundation model for other tasks.When using the Code Llama models, users must abide by our license and acceptable use policy.Evaluating Code Llama’s performanceTo test Code Llama’s performance against existing solutions, we used two popular coding benchmarks: HumanEval and Mostly Basic Python Programming (MBPP). HumanEval tests the model’s ability to complete code based on docstrings and MBPP tests the model’s ability to write code based on a description.Our benchmark testing showed that Code Llama performed better than open-source, code-specific LLMs and outperformed Llama 2. Code Llama 34B, for example, scored 53.7% on HumanEval and 56.2% on MBPP, the highest compared with other state-of-the-art open solutions, and on par with ChatGPT.As with all cutting edge technology, Code Llama comes with risks. Building AI models responsibly is crucial, and we undertook numerous safety measures before releasing Code Llama. As part of our red teaming efforts, we ran a quantitative evaluation of Code Llama’s risk of generating malicious code. We created prompts that attempted to solicit malicious code with clear intent and scored Code Llama’s responses to those prompts against ChatGPT’s (GPT3.5 Turbo). Our results found that Code Llama answered with safer responses.Details about our red teaming efforts from domain experts in responsible AI, offensive security engineering, malware development, and software engineering are available in our research paper.Releasing Code LlamaProgrammers are already using LLMs to assist in a variety of tasks, ranging from writing new software to debugging existing code. The goal is to make developer workflows more efficient, so they can focus on the most human centric aspects of their job, rather than repetitive tasks.At Meta, we believe that AI models, but LLMs for coding in particular, benefit most from an open approach, both in terms of innovation and safety. Publicly available, code-specific models can facilitate the development of new technologies that improve peoples' lives. By releasing code models like Code Llama, the entire community can evaluate their capabilities, identify issues, and fix vulnerabilities.Code Llama’s training recipes are available on our Github repository.Model weights are also available.Responsible useOur research paper discloses details of Code Llama’s development as well as how we conducted our benchmarking tests. It also provides more information into the model’s limitations, known challenges we encountered, mitigations we’ve taken, and future challenges we intend to investigate.We’ve also updated our Responsible Use Guide and it includes guidance on developing downstream models responsibly, including:Defining content policies and mitigations.Preparing data.Fine-tuning the model.Evaluating and improving performance.Addressing input- and output-level risks.Building transparency and reporting mechanisms in user interactions.Developers should evaluate their models using code-specific evaluation benchmarks and perform safety studies on code-specific use cases such as generating malware, computer viruses, or malicious code. We also recommend leveraging safety datasets for automatic and human evaluations, and red teaming on adversarial prompts.The future of generative AI for codingCode Llama is designed to support software engineers in all sectors – including research, industry, open source projects, NGOs, and businesses. But there are still many more use cases to support than what our base and instruct models can serve.We hope that Code Llama will inspire others to leverage Llama 2 to create new innovative tools for research and commercial products.Try Code Llama todayCode Llama GitHub repositoryDownload the Code Llama ModelRead the research paperCode Llama: Open foundation models for codeShare:Our latest updates delivered to your inboxSubscribe to our newsletter to keep up with Meta AI news, events, research breakthroughs, and more.Join us in the pursuit of what’s possible with AI.See all open positionsRelated PostsFEATUREDResearchMeta and Microsoft Introduce the Next Generation of LlamaJuly 18, 2023Read postFEATUREDResearchIntroducing CM3leon, a more efficient, state-of-the-art generative model for text and images July 14, 2023Read postLarge Language ModelCommunity-driven AI innovation comes alive with Llama 2 July 28, 2023Read postOur approachAbout AI at MetaResponsibilityPeopleCareersResearchInfrastructureResourcesDemosProduct experiencesMeta AILatest newsBlogNewsletterFoundational modelsMeta LlamaOur approachOur approachAbout AI at MetaResponsibilityPeopleCareersResearchResearchInfrastructureResourcesDemosProduct experiencesMeta AILatest newsLatest newsBlogNewsletterFoundational modelsMeta LlamaPrivacy PolicyTermsCookies Meta © 2024\n",
      "deepseek-ai/deepseek-coder-33b-instruct · Hugging Face Hugging Face Models Datasets Spaces Posts Docs Solutions Pricing Log In Sign Up deepseek-ai / deepseek-coder-33b-instruct like 405 Text Generation Transformers PyTorch Safetensors llama conversational Inference Endpoints text-generation-inference License: deepseek (other) Model card Files Files and versions Community 27 Train Deploy Use this model Edit model card 1. Introduction of Deepseek Coder 2. Model Summary 3. How to Use Chat Model Inference 4. License 5. Contact [🏠Homepage] | [🤖 Chat with DeepSeek Coder] | [Discord] | [Wechat(微信)] 1. Introduction of Deepseek Coder Deepseek Coder is composed of a series of code language models, each trained from scratch on 2T tokens, with a composition of 87% code and 13% natural language in both English and Chinese. We provide various sizes of the code model, ranging from 1B to 33B versions. Each model is pre-trained on project-level code corpus by employing a window size of 16K and a extra fill-in-the-blank task, to support project-level code completion and infilling. For coding capabilities, Deepseek Coder achieves state-of-the-art performance among open-source code models on multiple programming languages and various benchmarks. Massive Training Data: Trained from scratch on 2T tokens, including 87% code and 13% linguistic data in both English and Chinese languages. Highly Flexible & Scalable: Offered in model sizes of 1.3B, 5.7B, 6.7B, and 33B, enabling users to choose the setup most suitable for their requirements. Superior Model Performance: State-of-the-art performance among publicly available code models on HumanEval, MultiPL-E, MBPP, DS-1000, and APPS benchmarks. Advanced Code Completion Capabilities: A window size of 16K and a fill-in-the-blank task, supporting project-level code completion and infilling tasks. 2. Model Summary deepseek-coder-33b-instruct is a 33B parameter model initialized from deepseek-coder-33b-base and fine-tuned on 2B tokens of instruction data. Home Page: DeepSeek Repository: deepseek-ai/deepseek-coder Chat With DeepSeek Coder: DeepSeek-Coder 3. How to Use Here give some examples of how to use our model. Chat Model Inference from transformers import AutoTokenizer, AutoModelForCausalLM tokenizer = AutoTokenizer.from_pretrained(\"deepseek-ai/deepseek-coder-6.7b-instruct\", trust_remote_code=True) model = AutoModelForCausalLM.from_pretrained(\"deepseek-ai/deepseek-coder-6.7b-instruct\", trust_remote_code=True, torch_dtype=torch.bfloat16).cuda() messages=[ { 'role': 'user', 'content': \"write a quick sort algorithm in python.\"} ] inputs = tokenizer.apply_chat_template(messages, add_generation_prompt=True, return_tensors=\"pt\").to(model.device) # tokenizer.eos_token_id is the id of  token outputs = model.generate(inputs, max_new_tokens=512, do_sample=False, top_k=50, top_p=0.95, num_return_sequences=1, eos_token_id=tokenizer.eos_token_id) print(tokenizer.decode(outputs[0][len(inputs[0]):], skip_special_tokens=True)) 4. License This code repository is licensed under the MIT License. The use of DeepSeek Coder models is subject to the Model License. DeepSeek Coder supports commercial use. See the LICENSE-MODEL for more details. 5. Contact If you have any questions, please raise an issue or contact us at agi_code@deepseek.com. Downloads last month24,403 Safetensors Model size 33.3B params Tensor type BF16 · Spaces using deepseek-ai/deepseek-coder-33b-instruct 29 📈 bigcode/bigcode-models-leaderboard🐬 deepseek-ai/deepseek-coder-33b-instruct⚡ yhavinga/dutch-tokenizer-arena📈 21world/bigcode-models-leaderboard🚀 officialhimanshu595/llama-factory📚 bardsai/performance-llm-board🐬 abdd199719/deepseek-coder-33b-instruct🐬 Sivasubramoniam/deepseek-coder-33b-instruct🐬 Houssemeddine/deepseek-coder-33b-instruct👀 arshadkm/deepseek-ai-deepseek-coder-33b-instruct😻 Omanjelato/deepseek-ai-deepseek-coder-33b-instruct📚 voyceatlas/deepseek-ai-deepseek-coder-33b-instruct🏢 imrnh/deepseek-ai-deepseek-coder-33b-instruct🐬 DoubleTechnologies/deepseek-ai-deepseek-coder-33b-instruct😻 ChaitanyaNair06/deepseek-ai-deepseek-coder-33b-instruct🔥 forceisthop/deepseek-ai-deepseek-coder-33b-instruct⚡ Nikit-K/deepseek-ai-deepseek-coder-33b-instruct🏃 lynquantumman/deepseek-ai-deepseek-coder-33b-instruct🚀 caxlar/deepseek-ai-deepseek-coder-33b-instruct🏃 ValkTrippin/deepseek-ai-deepseek-coder-33b-instruct + 24 Spaces + 9 Spaces Collection including deepseek-ai/deepseek-coder-33b-instruct DeepSeek-Coder Collection DeepSeek Coder series • 8 items • Updated Mar 26 • 19 Company © Hugging Face TOS Privacy About Jobs Website Models Datasets Spaces Pricing Docs\n",
      "tiiuae/falcon-180B · Hugging Face Hugging Face Models Datasets Spaces Posts Docs Solutions Pricing Log In Sign Up tiiuae / falcon-180B like 1.1k Text Generation Transformers Safetensors tiiuae/falcon-refinedweb 4 languages falcon text-generation-inference 6 papers License: unknown Model card Files Files and versions Community 30 Train Deploy Use this model Edit model card Acknowledge license to access the repository This repository is publicly accessible, but you have to accept the conditions to access its files and content. You agree to the Falcon-180B TII license and acceptable use policy. Log in or Sign Up to review the conditions and access this model content. 🚀 Falcon-180B Why use Falcon-180B? Model Card for Falcon-180B Model Details Model Description Model Source Uses Direct Use Out-of-Scope Use Bias, Risks, and Limitations Recommendations How to Get Started with the Model Training Details Training Data Training Procedure Evaluation Technical Specifications Model Architecture and Objective Compute Infrastructure Citation Contact 🚀 Falcon-180B Falcon-180B is a 180B parameters causal decoder-only model built by TII and trained on 3,500B tokens of RefinedWeb enhanced with curated corpora. It is made available under the Falcon-180B TII License and Acceptable Use Policy. Paper coming soon 😊 🤗 To get started with Falcon (inference, finetuning, quantization, etc.), we recommend reading this great blogpost from HF or this one from the release of the 40B! Note that since the 180B is larger than what can easily be handled with transformers+acccelerate, we recommend using Text Generation Inference. You will need at least 400GB of memory to swiftly run inference with Falcon-180B. Why use Falcon-180B? It is the best open-access model currently available, and one of the best model overall. Falcon-180B outperforms LLaMA-2, StableLM, RedPajama, MPT, etc. See the OpenLLM Leaderboard. It features an architecture optimized for inference, with multiquery (Shazeer et al., 2019). It is made available under a permissive license allowing for commercial use. ⚠️ This is a raw, pretrained model, which should be further finetuned for most usecases. If you are looking for a version better suited to taking generic instructions in a chat format, we recommend taking a look at Falcon-180B-Chat. 💸 Looking for a smaller, less expensive model? Falcon-7B and Falcon-40B are Falcon-180B's little brothers! 💥 Falcon LLMs require PyTorch 2.0 for use with transformers! Model Card for Falcon-180B Model Details Model Description Developed by: https://www.tii.ae; Model type: Causal decoder-only; Language(s) (NLP): English, German, Spanish, French (and limited capabilities in Italian, Portuguese, Polish, Dutch, Romanian, Czech, Swedish); License: Falcon-180B TII License and Acceptable Use Policy. Model Source Paper: coming soon. Uses See the acceptable use policy. Direct Use Research on large language models; as a foundation for further specialization and finetuning for specific usecases (e.g., summarization, text generation, chatbot, etc.) Out-of-Scope Use Production use without adequate assessment of risks and mitigation; any use cases which may be considered irresponsible or harmful. Bias, Risks, and Limitations Falcon-180B is trained mostly on English, German, Spanish, French, with limited capabilities also in in Italian, Portuguese, Polish, Dutch, Romanian, Czech, Swedish. It will not generalize appropriately to other languages. Furthermore, as it is trained on a large-scale corpora representative of the web, it will carry the stereotypes and biases commonly encountered online. Recommendations We recommend users of Falcon-180B to consider finetuning it for the specific set of tasks of interest, and for guardrails and appropriate precautions to be taken for any production use. How to Get Started with the Model To run inference with the model in full bfloat16 precision you need approximately 8xA100 80GB or equivalent. from transformers import AutoTokenizer, AutoModelForCausalLM import transformers import torch model = \"tiiuae/falcon-180b\" tokenizer = AutoTokenizer.from_pretrained(model) pipeline = transformers.pipeline( \"text-generation\", model=model, tokenizer=tokenizer, torch_dtype=torch.bfloat16, trust_remote_code=True, device_map=\"auto\", ) sequences = pipeline( \"Girafatron is obsessed with giraffes, the most glorious animal on the face of this Earth. Giraftron believes all other animals are irrelevant when compared to the glorious majesty of the giraffe.\\nDaniel: Hello, Girafatron!\\nGirafatron:\", max_length=200, do_sample=True, top_k=10, num_return_sequences=1, eos_token_id=tokenizer.eos_token_id, ) for seq in sequences: print(f\"Result: {seq['generated_text']}\") Training Details Training Data Falcon-180B was trained on 3,500B tokens of RefinedWeb, a high-quality filtered and deduplicated web dataset which we enhanced with curated corpora. Significant components from our curated copora were inspired by The Pile (Gao et al., 2020). Data source Fraction Tokens Sources RefinedWeb-English 75% 750B massive web crawl RefinedWeb-Europe 7% 70B European massive web crawl Books 6% 60B Conversations 5% 50B Reddit, StackOverflow, HackerNews Code 5% 50B Technical 2% 20B arXiv, PubMed, USPTO, etc. RefinedWeb-Europe is made of the following languages: Language Fraction of multilingual data Tokens German 26% 18B Spanish 24% 17B French 23% 16B Italian 7% 5B Portuguese 4% 3B Polish 4% 3B Dutch 4% 3B Romanian 3% 2B Czech 3% 2B Swedish 2% 1B The data was tokenized with the Falcon tokenizer. Training Procedure Falcon-180B was trained on up to 4,096 A100 40GB GPUs, using a 3D parallelism strategy (TP=8, PP=8, DP=64) combined with ZeRO. Training Hyperparameters Hyperparameter Value Comment Precision bfloat16 Optimizer AdamW Learning rate 1.25e-4 4B tokens warm-up, cosine decay to 1.25e-5 Weight decay 1e-1 Z-loss 1e-4 Batch size 2048 100B tokens ramp-up Speeds, Sizes, Times Training started in early 2023. Evaluation Paper coming soon. See the OpenLLM Leaderboard for early results. Technical Specifications Model Architecture and Objective Falcon-180B is a causal decoder-only model trained on a causal language modeling task (i.e., predict the next token). The architecture is broadly adapted from the GPT-3 paper (Brown et al., 2020), with the following differences: Positionnal embeddings: rotary (Su et al., 2021); Attention: multiquery (Shazeer et al., 2019) and FlashAttention (Dao et al., 2022); Decoder-block: parallel attention/MLP with two layer norms. For multiquery, we are using an internal variant which uses independent key and values per tensor parallel degree (so-called multigroup). Hyperparameter Value Comment Layers 80 d_model 14848 head_dim 64 Reduced to optimise for FlashAttention Vocabulary 65024 Sequence length 2048 Compute Infrastructure Hardware Falcon-180B was trained on AWS SageMaker, on up to 4,096 A100 40GB GPUs in P4d instances. Software Falcon-180B was trained a custom distributed training codebase, Gigatron. It uses a 3D parallelism approach combined with ZeRO and high-performance Triton kernels (FlashAttention, etc.) Citation Paper coming soon 😊 (actually this time). In the meanwhile, you can use the following information to cite: @article{falcon, title={The Falcon Series of Language Models: Towards Open Frontier Models}, author={Almazrouei, Ebtesam and Alobeidli, Hamza and Alshamsi, Abdulaziz and Cappelli, Alessandro and Cojocaru, Ruxandra and Alhammadi, Maitha and Daniele, Mazzotta and Heslow, Daniel and Launay, Julien and Malartic, Quentin and Noune, Badreddine and Pannier, Baptiste and Penedo, Guilherme}, year={2023} } To learn more about the pretraining dataset, see the 📓 RefinedWeb paper. @article{refinedweb, title={The {R}efined{W}eb dataset for {F}alcon {LLM}: outperforming curated corpora with web data, and web data only}, author={Guilherme Penedo and Quentin Malartic and Daniel Hesslow and Ruxandra Cojocaru and Alessandro Cappelli and Hamza Alobeidli and Baptiste Pannier and Ebtesam Almazrouei and Julien Launay}, journal={arXiv preprint arXiv:2306.01116}, eprint={2306.01116}, eprinttype = {arXiv}, url={https://arxiv.org/abs/2306.01116}, year={2023} } Contact falconllm@tii.ae Downloads last month8,234 Safetensors Model size 180B params Tensor type BF16 · Text Generation Inference API (serverless) has been turned off for this model. Dataset used to train tiiuae/falcon-180B tiiuae/falcon-refinedweb Viewer • Updated Jun 20, 2023 • 106k • 748 Spaces using tiiuae/falcon-180B 90 🏆 open-llm-leaderboard/open_llm_leaderboard💬 tiiuae/falcon-180b-demo📈 bigcode/bigcode-models-leaderboard🚀 Vokturz/can-it-run-llm📚 h2oai/h2ogpt-chatbot📚 h2oai/h2ogpt-chatbot2📈 TIGER-Lab/GenAI-Arena🏆 Intel/low_bit_open_llm_leaderboard😻 Sharathhebbar24/One-stop-for-Open-source-models🐠 ZhangYuhan/3DGen-Arena🏆 gsaivinay/open_llm_leaderboard🏆 BAAI/open_cn_llm_leaderboard🐠 Yeyito/llm_contamination_detector💬 wffcyrus/falcon-180b-demo💬 lunarflu/falcon-180b-demo-duplicate😻 GTBench/GTBench🧑‍⚖️ tiiuae/falcon-180b-license🏆 felixz/open_llm_leaderboard⚖️ gojiteji/LLM-Comparer📈 21world/bigcode-models-leaderboard + 85 Spaces + 70 Spaces Collection including tiiuae/falcon-180B 🦅 Falcon Collection 7 items • Updated 18 days ago • 12 Company © Hugging Face TOS Privacy About Jobs Website Models Datasets Spaces Pricing Docs\n",
      "tiiuae/falcon-40b · Hugging Face Hugging Face Models Datasets Spaces Posts Docs Solutions Pricing Log In Sign Up tiiuae / falcon-40b like 2.41k Text Generation Transformers PyTorch tiiuae/falcon-refinedweb 4 languages falcon custom_code text-generation-inference 6 papers License: apache-2.0 Model card Files Files and versions Community 114 Train Deploy Use this model Edit model card 🚀 Falcon-40B Why use Falcon-40B? Model Card for Falcon-40B Model Details Model Description Model Source Uses Direct Use Out-of-Scope Use Bias, Risks, and Limitations Recommendations How to Get Started with the Model Training Details Training Data Training Procedure Evaluation Technical Specifications Model Architecture and Objective Compute Infrastructure Citation License Contact 🚀 Falcon-40B Falcon-40B is a 40B parameters causal decoder-only model built by TII and trained on 1,000B tokens of RefinedWeb enhanced with curated corpora. It is made available under the Apache 2.0 license. Paper coming soon 😊. 🤗 To get started with Falcon (inference, finetuning, quantization, etc.), we recommend reading this great blogpost fron HF! Why use Falcon-40B? It is the best open-source model currently available. Falcon-40B outperforms LLaMA, StableLM, RedPajama, MPT, etc. See the OpenLLM Leaderboard. It features an architecture optimized for inference, with FlashAttention (Dao et al., 2022) and multiquery (Shazeer et al., 2019). It is made available under a permissive Apache 2.0 license allowing for commercial use, without any royalties or restrictions. ⚠️ This is a raw, pretrained model, which should be further finetuned for most usecases. If you are looking for a version better suited to taking generic instructions in a chat format, we recommend taking a look at Falcon-40B-Instruct. 💸 Looking for a smaller, less expensive model? Falcon-7B is Falcon-40B's little brother! from transformers import AutoTokenizer, AutoModelForCausalLM import transformers import torch model = \"tiiuae/falcon-40b\" tokenizer = AutoTokenizer.from_pretrained(model) pipeline = transformers.pipeline( \"text-generation\", model=model, tokenizer=tokenizer, torch_dtype=torch.bfloat16, trust_remote_code=True, device_map=\"auto\", ) sequences = pipeline( \"Girafatron is obsessed with giraffes, the most glorious animal on the face of this Earth. Giraftron believes all other animals are irrelevant when compared to the glorious majesty of the giraffe.\\nDaniel: Hello, Girafatron!\\nGirafatron:\", max_length=200, do_sample=True, top_k=10, num_return_sequences=1, eos_token_id=tokenizer.eos_token_id, ) for seq in sequences: print(f\"Result: {seq['generated_text']}\") 💥 Falcon LLMs require PyTorch 2.0 for use with transformers! For fast inference with Falcon, check-out Text Generation Inference! Read more in this blogpost. You will need at least 85-100GB of memory to swiftly run inference with Falcon-40B. Model Card for Falcon-40B Model Details Model Description Developed by: https://www.tii.ae; Model type: Causal decoder-only; Language(s) (NLP): English, German, Spanish, French (and limited capabilities in Italian, Portuguese, Polish, Dutch, Romanian, Czech, Swedish); License: Apache 2.0 license. Model Source Paper: coming soon. Uses Direct Use Research on large language models; as a foundation for further specialization and finetuning for specific usecases (e.g., summarization, text generation, chatbot, etc.) Out-of-Scope Use Production use without adequate assessment of risks and mitigation; any use cases which may be considered irresponsible or harmful. Bias, Risks, and Limitations Falcon-40B is trained mostly on English, German, Spanish, French, with limited capabilities also in in Italian, Portuguese, Polish, Dutch, Romanian, Czech, Swedish. It will not generalize appropriately to other languages. Furthermore, as it is trained on a large-scale corpora representative of the web, it will carry the stereotypes and biases commonly encountered online. Recommendations We recommend users of Falcon-40B to consider finetuning it for the specific set of tasks of interest, and for guardrails and appropriate precautions to be taken for any production use. How to Get Started with the Model from transformers import AutoTokenizer, AutoModelForCausalLM import transformers import torch model = \"tiiuae/falcon-40b\" tokenizer = AutoTokenizer.from_pretrained(model) pipeline = transformers.pipeline( \"text-generation\", model=model, tokenizer=tokenizer, torch_dtype=torch.bfloat16, trust_remote_code=True, device_map=\"auto\", ) sequences = pipeline( \"Girafatron is obsessed with giraffes, the most glorious animal on the face of this Earth. Giraftron believes all other animals are irrelevant when compared to the glorious majesty of the giraffe.\\nDaniel: Hello, Girafatron!\\nGirafatron:\", max_length=200, do_sample=True, top_k=10, num_return_sequences=1, eos_token_id=tokenizer.eos_token_id, ) for seq in sequences: print(f\"Result: {seq['generated_text']}\") Training Details Training Data Falcon-40B was trained on 1,000B tokens of RefinedWeb, a high-quality filtered and deduplicated web dataset which we enhanced with curated corpora. Significant components from our curated copora were inspired by The Pile (Gao et al., 2020). Data source Fraction Tokens Sources RefinedWeb-English 75% 750B massive web crawl RefinedWeb-Europe 7% 70B European massive web crawl Books 6% 60B Conversations 5% 50B Reddit, StackOverflow, HackerNews Code 5% 50B Technical 2% 20B arXiv, PubMed, USPTO, etc. RefinedWeb-Europe is made of the following languages: Language Fraction of multilingual data Tokens German 26% 18B Spanish 24% 17B French 23% 16B Italian 7% 5B Portuguese 4% 3B Polish 4% 3B Dutch 4% 3B Romanian 3% 2B Czech 3% 2B Swedish 2% 1B The data was tokenized with the Falcon-7B/40B tokenizer. Training Procedure Falcon-40B was trained on 384 A100 40GB GPUs, using a 3D parallelism strategy (TP=8, PP=4, DP=12) combined with ZeRO. Training Hyperparameters Hyperparameter Value Comment Precision bfloat16 Optimizer AdamW Learning rate 1.85e-4 4B tokens warm-up, cosine decay to 1.85e-5 Weight decay 1e-1 Z-loss 1e-4 Batch size 1152 100B tokens ramp-up Speeds, Sizes, Times Training started in December 2022 and took two months. Evaluation Paper coming soon. See the OpenLLM Leaderboard for early results. Technical Specifications Model Architecture and Objective Falcon-40B is a causal decoder-only model trained on a causal language modeling task (i.e., predict the next token). The architecture is broadly adapted from the GPT-3 paper (Brown et al., 2020), with the following differences: Positionnal embeddings: rotary (Su et al., 2021); Attention: multiquery (Shazeer et al., 2019) and FlashAttention (Dao et al., 2022); Decoder-block: parallel attention/MLP with a two layer norms. For multiquery, we are using an internal variant which uses independent key and values per tensor parallel degree. Hyperparameter Value Comment Layers 60 d_model 8192 head_dim 64 Reduced to optimise for FlashAttention Vocabulary 65024 Sequence length 2048 Compute Infrastructure Hardware Falcon-40B was trained on AWS SageMaker, on 384 A100 40GB GPUs in P4d instances. Software Falcon-40B was trained a custom distributed training codebase, Gigatron. It uses a 3D parallelism approach combined with ZeRO and high-performance Triton kernels (FlashAttention, etc.) Citation Paper coming soon 😊. In the meanwhile, you can use the following information to cite: @article{falcon40b, title={{Falcon-40B}: an open large language model with state-of-the-art performance}, author={Almazrouei, Ebtesam and Alobeidli, Hamza and Alshamsi, Abdulaziz and Cappelli, Alessandro and Cojocaru, Ruxandra and Debbah, Merouane and Goffinet, Etienne and Heslow, Daniel and Launay, Julien and Malartic, Quentin and Noune, Badreddine and Pannier, Baptiste and Penedo, Guilherme}, year={2023} } To learn more about the pretraining dataset, see the 📓 RefinedWeb paper. @article{refinedweb, title={The {R}efined{W}eb dataset for {F}alcon {LLM}: outperforming curated corpora with web data, and web data only}, author={Guilherme Penedo and Quentin Malartic and Daniel Hesslow and Ruxandra Cojocaru and Alessandro Cappelli and Hamza Alobeidli and Baptiste Pannier and Ebtesam Almazrouei and Julien Launay}, journal={arXiv preprint arXiv:2306.01116}, eprint={2306.01116}, eprinttype = {arXiv}, url={https://arxiv.org/abs/2306.01116}, year={2023} } License Falcon-40B is made available under the Apache 2.0 license. Contact falconllm@tii.ae Downloads last month30,150 Text Generation Inference API (serverless) has been turned off for this model. Dataset used to train tiiuae/falcon-40b tiiuae/falcon-refinedweb Viewer • Updated Jun 20, 2023 • 106k • 748 Spaces using tiiuae/falcon-40b 100 🏆 open-llm-leaderboard/open_llm_leaderboard🚀 Vokturz/can-it-run-llm💬 HuggingFaceH4/falcon-chat📚 h2oai/h2ogpt-chatbot📚 h2oai/h2ogpt-chatbot2🏆 Intel/low_bit_open_llm_leaderboard🏆 eduagarcia/open_pt_llm_leaderboard😻 Sharathhebbar24/One-stop-for-Open-source-models🏆 gsaivinay/open_llm_leaderboard🏆 BAAI/open_cn_llm_leaderboard😻 GTBench/GTBench🏆 felixz/open_llm_leaderboard🚀 officialhimanshu595/llama-factory🎨 OPTML-Group/UnlearnCanvas-Benchmark💬 hlydecker/falcon-chat🐢 Sijuade/GPTNEXTWORD👁 PrarthanaTS/tsai-gpt-from-scratch🔥 HemaAM/GPT_train_on_LLaMa🐨 Zulelee/langchain-chatchat🚀 lambdabrendan/Lambda-LLM-Calculator + 95 Spaces + 80 Spaces Collection including tiiuae/falcon-40b 🦅 Falcon Collection 7 items • Updated 18 days ago • 12 Company © Hugging Face TOS Privacy About Jobs Website Models Datasets Spaces Pricing Docs\n",
      "google/flan-t5-xl · Hugging Face Hugging Face Models Datasets Spaces Posts Docs Solutions Pricing Log In Sign Up google / flan-t5-xl like 435 Text2Text Generation Transformers PyTorch google-tensorflow TensorFlow JAX Safetensors svakulenk0/qrecc taskmaster2 djaym7/wiki_dialog deepmind/code_contests lambada gsm8k aqua_rat esnli quasc qed 5 languages t5 Inference Endpoints text-generation-inference arxiv: 2210.11416 arxiv: 1910.09700 License: apache-2.0 Model card Files Files and versions Community 26 Train Deploy Use this model Edit model card Model Card for FLAN-T5 XL Table of Contents TL;DR Model Details Model Description Usage Using the Pytorch model Running the model on a CPU Running the model on a GPU Running the model on a GPU using different precisions Uses Direct Use and Downstream Use Out-of-Scope Use Bias, Risks, and Limitations Ethical considerations and risks Known Limitations Sensitive Use: Training Details Training Data Training Procedure Evaluation Testing Data, Factors & Metrics Results Environmental Impact Citation Model Card for FLAN-T5 XL Table of Contents TL;DR Model Details Usage Uses Bias, Risks, and Limitations Training Details Evaluation Environmental Impact Citation TL;DR If you already know T5, FLAN-T5 is just better at everything. For the same number of parameters, these models have been fine-tuned on more than 1000 additional tasks covering also more languages. As mentioned in the first few lines of the abstract : Flan-PaLM 540B achieves state-of-the-art performance on several benchmarks, such as 75.2% on five-shot MMLU. We also publicly release Flan-T5 checkpoints,1 which achieve strong few-shot performance even compared to much larger models, such as PaLM 62B. Overall, instruction finetuning is a general method for improving the performance and usability of pretrained language models. Disclaimer: Content from this model card has been written by the Hugging Face team, and parts of it were copy pasted from the T5 model card. Model Details Model Description Model type: Language model Language(s) (NLP): English, Spanish, Japanese, Persian, Hindi, French, Chinese, Bengali, Gujarati, German, Telugu, Italian, Arabic, Polish, Tamil, Marathi, Malayalam, Oriya, Panjabi, Portuguese, Urdu, Galician, Hebrew, Korean, Catalan, Thai, Dutch, Indonesian, Vietnamese, Bulgarian, Filipino, Central Khmer, Lao, Turkish, Russian, Croatian, Swedish, Yoruba, Kurdish, Burmese, Malay, Czech, Finnish, Somali, Tagalog, Swahili, Sinhala, Kannada, Zhuang, Igbo, Xhosa, Romanian, Haitian, Estonian, Slovak, Lithuanian, Greek, Nepali, Assamese, Norwegian License: Apache 2.0 Related Models: All FLAN-T5 Checkpoints Original Checkpoints: All Original FLAN-T5 Checkpoints Resources for more information: Research paper GitHub Repo Hugging Face FLAN-T5 Docs (Similar to T5) Usage Find below some example scripts on how to use the model in transformers: Using the Pytorch model Running the model on a CPU Click to expand from transformers import T5Tokenizer, T5ForConditionalGeneration tokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-xl\") model = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-xl\") input_text = \"translate English to German: How old are you?\" input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids outputs = model.generate(input_ids) print(tokenizer.decode(outputs[0])) Running the model on a GPU Click to expand # pip install accelerate from transformers import T5Tokenizer, T5ForConditionalGeneration tokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-xl\") model = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-xl\", device_map=\"auto\") input_text = \"translate English to German: How old are you?\" input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(\"cuda\") outputs = model.generate(input_ids) print(tokenizer.decode(outputs[0])) Running the model on a GPU using different precisions FP16 Click to expand # pip install accelerate import torch from transformers import T5Tokenizer, T5ForConditionalGeneration tokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-xl\") model = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-xl\", device_map=\"auto\", torch_dtype=torch.float16) input_text = \"translate English to German: How old are you?\" input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(\"cuda\") outputs = model.generate(input_ids) print(tokenizer.decode(outputs[0])) INT8 Click to expand # pip install bitsandbytes accelerate from transformers import T5Tokenizer, T5ForConditionalGeneration tokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-xl\") model = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-xl\", device_map=\"auto\", load_in_8bit=True) input_text = \"translate English to German: How old are you?\" input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(\"cuda\") outputs = model.generate(input_ids) print(tokenizer.decode(outputs[0])) Uses Direct Use and Downstream Use The authors write in the original paper's model card that: The primary use is research on language models, including: research on zero-shot NLP tasks and in-context few-shot learning NLP tasks, such as reasoning, and question answering; advancing fairness and safety research, and understanding limitations of current large language models See the research paper for further details. Out-of-Scope Use More information needed. Bias, Risks, and Limitations The information below in this section are copied from the model's official model card: Language models, including Flan-T5, can potentially be used for language generation in a harmful way, according to Rae et al. (2021). Flan-T5 should not be used directly in any application, without a prior assessment of safety and fairness concerns specific to the application. Ethical considerations and risks Flan-T5 is fine-tuned on a large corpus of text data that was not filtered for explicit content or assessed for existing biases. As a result the model itself is potentially vulnerable to generating equivalently inappropriate content or replicating inherent biases in the underlying data. Known Limitations Flan-T5 has not been tested in real world applications. Sensitive Use: Flan-T5 should not be applied for any unacceptable use cases, e.g., generation of abusive speech. Training Details Training Data The model was trained on a mixture of tasks, that includes the tasks described in the table below (from the original paper, figure 2): Training Procedure According to the model card from the original paper: These models are based on pretrained T5 (Raffel et al., 2020) and fine-tuned with instructions for better zero-shot and few-shot performance. There is one fine-tuned Flan model per T5 model size. The model has been trained on TPU v3 or TPU v4 pods, using t5x codebase together with jax. Evaluation Testing Data, Factors & Metrics The authors evaluated the model on various tasks covering several languages (1836 in total). See the table below for some quantitative evaluation: For full details, please check the research paper. Results For full results for FLAN-T5-XL, see the research paper, Table 3. Environmental Impact Carbon emissions can be estimated using the Machine Learning Impact calculator presented in Lacoste et al. (2019). Hardware Type: Google Cloud TPU Pods - TPU v3 or TPU v4 | Number of chips ≥ 4. Hours used: More information needed Cloud Provider: GCP Compute Region: More information needed Carbon Emitted: More information needed Citation BibTeX: @misc{https://doi.org/10.48550/arxiv.2210.11416, doi = {10.48550/ARXIV.2210.11416}, url = {https://arxiv.org/abs/2210.11416}, author = {Chung, Hyung Won and Hou, Le and Longpre, Shayne and Zoph, Barret and Tay, Yi and Fedus, William and Li, Eric and Wang, Xuezhi and Dehghani, Mostafa and Brahma, Siddhartha and Webson, Albert and Gu, Shixiang Shane and Dai, Zhuyun and Suzgun, Mirac and Chen, Xinyun and Chowdhery, Aakanksha and Narang, Sharan and Mishra, Gaurav and Yu, Adams and Zhao, Vincent and Huang, Yanping and Dai, Andrew and Yu, Hongkun and Petrov, Slav and Chi, Ed H. and Dean, Jeff and Devlin, Jacob and Roberts, Adam and Zhou, Denny and Le, Quoc V. and Wei, Jason}, keywords = {Machine Learning (cs.LG), Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences}, title = {Scaling Instruction-Finetuned Language Models}, publisher = {arXiv}, year = {2022}, copyright = {Creative Commons Attribution 4.0 International} } Downloads last month252,717 Safetensors Model size 2.85B params Tensor type F32 · Datasets used to train google/flan-t5-xl gsm8k Viewer • Updated Jan 4 • 352k • 260 deepmind/code_contests Viewer • Updated Jun 11, 2023 • 2.73k • 90 esnli Viewer • Updated Jan 18 • 1.64k • 19 Spaces using google/flan-t5-xl 100 🎵 facebook/MusicGen😻 Sharathhebbar24/One-stop-for-Open-source-models🎶 GrandaddyShmax/AudioCraft_Plus🎵 GrandaddyShmax/MusicGen_Plus🎼 GrandaddyShmax/MusicGen_Plus_hfv2🔥 unpairedelectron07/Text-to-Music-Generator🎵 Manjushri/MusicGen💬 adi-123/chat-with-multiple-PDFs🎵 sunnyujjawal/AI-Music-Generator🎵 patgpt4/MusicGen🎵 AchyuthGamer/MusicGen🎶 Gyufyjk/AudioCraft_Plus🚀 ZeroTwo3/videoshop-backend🎵 jbilcke-hf/ai-tube-model-musicgen-1🎵 Fabrice-TIERCELIN/Text-to-Music📉 ChihChiu29/mychatbot🎵 ddasd/MusicGen🎵 Alexxggs/sounddromcom🎵 sub314xxl/MusicGen-Continuation📞 DrDominikDellermann/InterviewAnalyzer + 95 Spaces + 80 Spaces Collection including google/flan-t5-xl Flan-T5 release Collection The Flan-T5 covers 4 checkpoints of different sizes each time. It also includes upgrades versions trained using Universal sampling • 7 items • Updated 17 days ago • 14 Company © Hugging Face TOS Privacy About Jobs Website Models Datasets Spaces Pricing Docs\n",
      "google/flan-t5-xxl · Hugging Face Hugging Face Models Datasets Spaces Posts Docs Solutions Pricing Log In Sign Up google / flan-t5-xxl like 1.12k Text2Text Generation Transformers PyTorch google-tensorflow TensorFlow JAX Safetensors svakulenk0/qrecc taskmaster2 djaym7/wiki_dialog deepmind/code_contests lambada gsm8k aqua_rat esnli quasc qed 5 languages t5 Inference Endpoints text-generation-inference arxiv: 2210.11416 arxiv: 1910.09700 License: apache-2.0 Model card Files Files and versions Community 70 Train Deploy Use this model Edit model card Model Card for FLAN-T5 XXL Table of Contents TL;DR Model Details Model Description Usage Using the Pytorch model Running the model on a CPU Running the model on a GPU Running the model on a GPU using different precisions Uses Direct Use and Downstream Use Out-of-Scope Use Bias, Risks, and Limitations Ethical considerations and risks Known Limitations Sensitive Use: Training Details Training Data Training Procedure Evaluation Testing Data, Factors & Metrics Results Environmental Impact Citation Model Card for FLAN-T5 XXL Table of Contents TL;DR Model Details Usage Uses Bias, Risks, and Limitations Training Details Evaluation Environmental Impact Citation TL;DR If you already know T5, FLAN-T5 is just better at everything. For the same number of parameters, these models have been fine-tuned on more than 1000 additional tasks covering also more languages. As mentioned in the first few lines of the abstract : Flan-PaLM 540B achieves state-of-the-art performance on several benchmarks, such as 75.2% on five-shot MMLU. We also publicly release Flan-T5 checkpoints,1 which achieve strong few-shot performance even compared to much larger models, such as PaLM 62B. Overall, instruction finetuning is a general method for improving the performance and usability of pretrained language models. Disclaimer: Content from this model card has been written by the Hugging Face team, and parts of it were copy pasted from the T5 model card. Model Details Model Description Model type: Language model Language(s) (NLP): English, German, French License: Apache 2.0 Related Models: All FLAN-T5 Checkpoints Original Checkpoints: All Original FLAN-T5 Checkpoints Resources for more information: Research paper GitHub Repo Hugging Face FLAN-T5 Docs (Similar to T5) Usage Find below some example scripts on how to use the model in transformers: Using the Pytorch model Running the model on a CPU Click to expand from transformers import T5Tokenizer, T5ForConditionalGeneration tokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-xxl\") model = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-xxl\") input_text = \"translate English to German: How old are you?\" input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids outputs = model.generate(input_ids) print(tokenizer.decode(outputs[0])) Running the model on a GPU Click to expand # pip install accelerate from transformers import T5Tokenizer, T5ForConditionalGeneration tokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-xxl\") model = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-xxl\", device_map=\"auto\") input_text = \"translate English to German: How old are you?\" input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(\"cuda\") outputs = model.generate(input_ids) print(tokenizer.decode(outputs[0])) Running the model on a GPU using different precisions FP16 Click to expand # pip install accelerate import torch from transformers import T5Tokenizer, T5ForConditionalGeneration tokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-xxl\") model = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-xxl\", device_map=\"auto\", torch_dtype=torch.float16) input_text = \"translate English to German: How old are you?\" input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(\"cuda\") outputs = model.generate(input_ids) print(tokenizer.decode(outputs[0])) INT8 Click to expand # pip install bitsandbytes accelerate from transformers import T5Tokenizer, T5ForConditionalGeneration tokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-xxl\") model = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-xxl\", device_map=\"auto\", load_in_8bit=True) input_text = \"translate English to German: How old are you?\" input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(\"cuda\") outputs = model.generate(input_ids) print(tokenizer.decode(outputs[0])) Uses Direct Use and Downstream Use The authors write in the original paper's model card that: The primary use is research on language models, including: research on zero-shot NLP tasks and in-context few-shot learning NLP tasks, such as reasoning, and question answering; advancing fairness and safety research, and understanding limitations of current large language models See the research paper for further details. Out-of-Scope Use More information needed. Bias, Risks, and Limitations The information below in this section are copied from the model's official model card: Language models, including Flan-T5, can potentially be used for language generation in a harmful way, according to Rae et al. (2021). Flan-T5 should not be used directly in any application, without a prior assessment of safety and fairness concerns specific to the application. Ethical considerations and risks Flan-T5 is fine-tuned on a large corpus of text data that was not filtered for explicit content or assessed for existing biases. As a result the model itself is potentially vulnerable to generating equivalently inappropriate content or replicating inherent biases in the underlying data. Known Limitations Flan-T5 has not been tested in real world applications. Sensitive Use: Flan-T5 should not be applied for any unacceptable use cases, e.g., generation of abusive speech. Training Details Training Data The model was trained on a mixture of tasks, that includes the tasks described in the table below (from the original paper, figure 2): Training Procedure According to the model card from the original paper: These models are based on pretrained T5 (Raffel et al., 2020) and fine-tuned with instructions for better zero-shot and few-shot performance. There is one fine-tuned Flan model per T5 model size. The model has been trained on TPU v3 or TPU v4 pods, using t5x codebase together with jax. Evaluation Testing Data, Factors & Metrics The authors evaluated the model on various tasks covering several languages (1836 in total). See the table below for some quantitative evaluation: For full details, please check the research paper. Results For full results for FLAN-T5-XXL, see the research paper, Table 3. Environmental Impact Carbon emissions can be estimated using the Machine Learning Impact calculator presented in Lacoste et al. (2019). Hardware Type: Google Cloud TPU Pods - TPU v3 or TPU v4 | Number of chips ≥ 4. Hours used: More information needed Cloud Provider: GCP Compute Region: More information needed Carbon Emitted: More information needed Citation BibTeX: @misc{https://doi.org/10.48550/arxiv.2210.11416, doi = {10.48550/ARXIV.2210.11416}, url = {https://arxiv.org/abs/2210.11416}, author = {Chung, Hyung Won and Hou, Le and Longpre, Shayne and Zoph, Barret and Tay, Yi and Fedus, William and Li, Eric and Wang, Xuezhi and Dehghani, Mostafa and Brahma, Siddhartha and Webson, Albert and Gu, Shixiang Shane and Dai, Zhuyun and Suzgun, Mirac and Chen, Xinyun and Chowdhery, Aakanksha and Narang, Sharan and Mishra, Gaurav and Yu, Adams and Zhao, Vincent and Huang, Yanping and Dai, Andrew and Yu, Hongkun and Petrov, Slav and Chi, Ed H. and Dean, Jeff and Devlin, Jacob and Roberts, Adam and Zhou, Denny and Le, Quoc V. and Wei, Jason}, keywords = {Machine Learning (cs.LG), Computation and Language (cs.CL), FOS: Computer and information sciences, FOS: Computer and information sciences}, title = {Scaling Instruction-Finetuned Language Models}, publisher = {arXiv}, year = {2022}, copyright = {Creative Commons Attribution 4.0 International} } Downloads last month367,630 Safetensors Model size 11.3B params Tensor type F32 · Datasets used to train google/flan-t5-xxl gsm8k Viewer • Updated Jan 4 • 352k • 260 deepmind/code_contests Viewer • Updated Jun 11, 2023 • 2.73k • 90 esnli Viewer • Updated Jan 18 • 1.64k • 19 Spaces using google/flan-t5-xxl 100 🎵 facebook/MusicGen📊 olivierdehaene/chat-llm-streaming📚 h2oai/h2ogpt-chatbot🌍 cvachet/pdf-chatbot📚 h2oai/h2ogpt-chatbot2😻 Sharathhebbar24/One-stop-for-Open-source-models🍮 ybelkada/i-like-flan-ul2🎶 GrandaddyShmax/AudioCraft_Plus🚀 hackaprompt/playground🚀 monra/freegpt-webui🎵 GrandaddyShmax/MusicGen_Plus🎼 GrandaddyShmax/MusicGen_Plus_hfv2📸 bilgeyucel/captionate🔥 unpairedelectron07/Text-to-Music-Generator📊 NeuralInternet/ChatLLMs📚 huggingfacejs/streaming-text-generation🐨 dekk-i386/pdflangchain🎵 Manjushri/MusicGen🌍 ROHAN181/pdf-chatbot🚀 xzuyn/Token-Count-Comparison + 95 Spaces + 80 Spaces Collection including google/flan-t5-xxl Flan-T5 release Collection The Flan-T5 covers 4 checkpoints of different sizes each time. It also includes upgrades versions trained using Universal sampling • 7 items • Updated 17 days ago • 14 Company © Hugging Face TOS Privacy About Jobs Website Models Datasets Spaces Pricing Docs\n",
      "google/flan-ul2 · Hugging Face Hugging Face Models Datasets Spaces Posts Docs Solutions Pricing Log In Sign Up google / flan-ul2 like 545 Text2Text Generation Transformers PyTorch svakulenk0/qrecc taskmaster2 djaym7/wiki_dialog deepmind/code_contests lambada gsm8k aqua_rat esnli quasc qed c4 5 languages t5 flan-ul2 Inference Endpoints text-generation-inference arxiv: 2205.05131 License: apache-2.0 Model card Files Files and versions Community 30 Train Deploy Use this model Edit model card Model card for Flan-UL2 Table of Contents TL;DR Using the model Converting from T5x to huggingface Running the model Results Performance improvment Introduction to UL2 Training Flan UL2 UL2 PreTraining Mixture of Denoisers Fine-tuning Contribution Citation Model card for Flan-UL2 Table of Contents TL;DR Using the model Results Introduction to UL2 Training Contribution Citation TL;DR Flan-UL2 is an encoder decoder model based on the T5 architecture. It uses the same configuration as the UL2 model released earlier last year. It was fine tuned using the \"Flan\" prompt tuning and dataset collection. According to the original blog here are the notable improvements: The original UL2 model was only trained with receptive field of 512, which made it non-ideal for N-shot prompting where N is large. The Flan-UL2 checkpoint uses a receptive field of 2048 which makes it more usable for few-shot in-context learning. The original UL2 model also had mode switch tokens that was rather mandatory to get good performance. However, they were a little cumbersome as this requires often some changes during inference or finetuning. In this update/change, we continue training UL2 20B for an additional 100k steps (with small batch) to forget “mode tokens” before applying Flan instruction tuning. This Flan-UL2 checkpoint does not require mode tokens anymore. Using the model Converting from T5x to huggingface You can use the convert_t5x_checkpoint_to_pytorch.py script and pass the argument strict = False. The final layer norm is missing from the original dictionnary, that is why we are passing the strict = False argument. python convert_t5x_checkpoint_to_pytorch.py --t5x_checkpoint_path PATH_TO_T5X_CHECKPOINTS --config_file PATH_TO_CONFIG --pytorch_dump_path PATH_TO_SAVE We used the same config file as google/ul2. Running the model For more efficient memory usage, we advise you to load the model in 8bit using load_in_8bit flag as follows (works only under GPU): # pip install accelerate transformers bitsandbytes from transformers import T5ForConditionalGeneration, AutoTokenizer import torch model = T5ForConditionalGeneration.from_pretrained(\"google/flan-ul2\", device_map=\"auto\", load_in_8bit=True) tokenizer = AutoTokenizer.from_pretrained(\"google/flan-ul2\") input_string = \"Answer the following question by reasoning step by step. The cafeteria had 23 apples. If they used 20 for lunch, and bought 6 more, how many apple do they have?\" inputs = tokenizer(input_string, return_tensors=\"pt\").input_ids.to(\"cuda\") outputs = model.generate(inputs, max_length=200) print(tokenizer.decode(outputs[0])) #  They have 23 - 20 = 3 apples left. They have 3 + 6 = 9 apples. Therefore, the answer is 9. Otherwise, you can load and run the model in bfloat16 as follows: # pip install accelerate transformers from transformers import T5ForConditionalGeneration, AutoTokenizer import torch model = T5ForConditionalGeneration.from_pretrained(\"google/flan-ul2\", torch_dtype=torch.bfloat16, device_map=\"auto\") tokenizer = AutoTokenizer.from_pretrained(\"google/flan-ul2\") input_string = \"Answer the following question by reasoning step by step. The cafeteria had 23 apples. If they used 20 for lunch, and bought 6 more, how many apple do they have?\" inputs = tokenizer(input_string, return_tensors=\"pt\").input_ids.to(\"cuda\") outputs = model.generate(inputs, max_length=200) print(tokenizer.decode(outputs[0])) #  They have 23 - 20 = 3 apples left. They have 3 + 6 = 9 apples. Therefore, the answer is 9. Results Performance improvment The reported results are the following : MMLU BBH MMLU-CoT BBH-CoT Avg FLAN-PaLM 62B 59.6 47.5 56.9 44.9 49.9 FLAN-PaLM 540B 73.5 57.9 70.9 66.3 67.2 FLAN-T5-XXL 11B 55.1 45.3 48.6 41.4 47.6 FLAN-UL2 20B 55.7(+1.1%) 45.9(+1.3%) 52.2(+7.4%) 42.7(+3.1%) 49.1(+3.2%) Introduction to UL2 This entire section has been copied from the google/ul2 model card and might be subject of change with respect to flan-ul2. UL2 is a unified framework for pretraining models that are universally effective across datasets and setups. UL2 uses Mixture-of-Denoisers (MoD), apre-training objective that combines diverse pre-training paradigms together. UL2 introduces a notion of mode switching, wherein downstream fine-tuning is associated with specific pre-training schemes. Abstract Existing pre-trained models are generally geared towards a particular class of problems. To date, there seems to be still no consensus on what the right architecture and pre-training setup should be. This paper presents a unified framework for pre-training models that are universally effective across datasets and setups. We begin by disentangling architectural archetypes with pre-training objectives -- two concepts that are commonly conflated. Next, we present a generalized and unified perspective for self-supervision in NLP and show how different pre-training objectives can be cast as one another and how interpolating between different objectives can be effective. We then propose Mixture-of-Denoisers (MoD), a pre-training objective that combines diverse pre-training paradigms together. We furthermore introduce a notion of mode switching, wherein downstream fine-tuning is associated with specific pre-training schemes. We conduct extensive ablative experiments to compare multiple pre-training objectives and find that our method pushes the Pareto-frontier by outperforming T5 and/or GPT-like models across multiple diverse setups. Finally, by scaling our model up to 20B parameters, we achieve SOTA performance on 50 well-established supervised NLP tasks ranging from language generation (with automated and human evaluation), language understanding, text classification, question answering, commonsense reasoning, long text reasoning, structured knowledge grounding and information retrieval. Our model also achieve strong results at in-context learning, outperforming 175B GPT-3 on zero-shot SuperGLUE and tripling the performance of T5-XXL on one-shot summarization. For more information, please take a look at the original paper. Paper: Unifying Language Learning Paradigms Authors: Yi Tay, Mostafa Dehghani, Vinh Q. Tran, Xavier Garcia, Dara Bahri, Tal Schuster, Huaixiu Steven Zheng, Neil Houlsby, Donald Metzler Training Flan UL2 The Flan-UL2 model was initialized using the UL2 checkpoints, and was then trained additionally using Flan Prompting. This means that the original training corpus is C4, In “Scaling Instruction-Finetuned language models (Chung et al.)” (also referred to sometimes as the Flan2 paper), the key idea is to train a large language model on a collection of datasets. These datasets are phrased as instructions which enable generalization across diverse tasks. Flan has been primarily trained on academic tasks. In Flan2, we released a series of T5 models ranging from 200M to 11B parameters that have been instruction tuned with Flan. The Flan datasets have also been open sourced in “The Flan Collection: Designing Data and Methods for Effective Instruction Tuning” (Longpre et al.). See Google AI Blogpost: “The Flan Collection: Advancing Open Source Methods for Instruction Tuning”. UL2 PreTraining The model is pretrained on the C4 corpus. For pretraining, the model is trained on a total of 1 trillion tokens on C4 (2 million steps) with a batch size of 1024. The sequence length is set to 512/512 for inputs and targets. Dropout is set to 0 during pretraining. Pre-training took slightly more than one month for about 1 trillion tokens. The model has 32 encoder layers and 32 decoder layers, dmodel of 4096 and df of 16384. The dimension of each head is 256 for a total of 16 heads. Our model uses a model parallelism of 8. The same sentencepiece tokenizer as T5 of vocab size 32000 is used (click here for more information about the T5 tokenizer). UL-20B can be interpreted as a model that is quite similar to T5 but trained with a different objective and slightly different scaling knobs. UL-20B was trained using the Jax and T5X infrastructure. The training objective during pretraining is a mixture of different denoising strategies that are explained in the following: Mixture of Denoisers To quote the paper: We conjecture that a strong universal model has to be exposed to solving diverse set of problems during pre-training. Given that pre-training is done using self-supervision, we argue that such diversity should be injected to the objective of the model, otherwise the model might suffer from lack a certain ability, like long-coherent text generation. Motivated by this, as well as current class of objective functions, we define three main paradigms that are used during pre-training: R-Denoiser: The regular denoising is the standard span corruption introduced in T5 that uses a range of 2 to 5 tokens as the span length, which masks about 15% of input tokens. These spans are short and potentially useful to acquire knowledge instead of learning to generate fluent text. S-Denoiser: A specific case of denoising where we observe a strict sequential order when framing the inputs-to-targets task, i.e., prefix language modeling. To do so, we simply partition the input sequence into two sub-sequences of tokens as context and target such that the targets do not rely on future information. This is unlike standard span corruption where there could be a target token with earlier position than a context token. Note that similar to the Prefix-LM setup, the context (prefix) retains a bidirectional receptive field. We note that S-Denoising with very short memory or no memory is in similar spirit to standard causal language modeling. X-Denoiser: An extreme version of denoising where the model must recover a large part of the input, given a small to moderate part of it. This simulates a situation where a model needs to generate long target from a memory with relatively limited information. To do so, we opt to include examples with aggressive denoising where approximately 50% of the input sequence is masked. This is by increasing the span length and/or corruption rate. We consider a pre-training task to be extreme if it has a long span (e.g., ≥ 12 tokens) or have a large corruption rate (e.g., ≥ 30%). X-denoising is motivated by being an interpolation between regular span corruption and language model like objectives. See the following diagram for a more visual explanation: Important: For more details, please see sections 3.1.2 of the paper. Fine-tuning The model was continously fine-tuned after N pretraining steps where N is typically from 50k to 100k. In other words, after each Nk steps of pretraining, the model is finetuned on each downstream task. See section 5.2.2 of paper to get an overview of all datasets that were used for fine-tuning). As the model is continuously finetuned, finetuning is stopped on a task once it has reached state-of-the-art to save compute. In total, the model was trained for 2.65 million steps. Important: For more details, please see sections 5.2.1 and 5.2.2 of the paper. Contribution This model was originally contributed by Yi Tay, and added to the Hugging Face ecosystem by Younes Belkada & Arthur Zucker. Citation If you want to cite this work, please consider citing the blogpost announcing the release of Flan-UL2. Downloads last month58,947 Datasets used to train google/flan-ul2 gsm8k Viewer • Updated Jan 4 • 352k • 260 c4 Updated Mar 5 • 133k • 229 deepmind/code_contests Viewer • Updated Jun 11, 2023 • 2.73k • 90 Spaces using google/flan-ul2 100 📊 olivierdehaene/chat-llm-streaming📚 h2oai/h2ogpt-chatbot📚 h2oai/h2ogpt-chatbot2📄🤖 fffiloni/langchain-chat-with-pdf🍮 ybelkada/i-like-flan-ul2🚀 monra/freegpt-webui📊 NeuralInternet/ChatLLMs📄🤖 TogetherAI/langchain-chat-with-pdf📄🤖 Tj/langchain-chat-with-pdf📊 tekkonetes/Chatbots📄🤖 TechWithAnirudh/langchain-chat-with-pdf💻 101-5/gpt4free🌍 arslan-ahmed/talk-to-your-docs🦀 abdullah10/TextGen📊 kastan/chatbot-llm-streaming📊 star-nox/chat-llm-streaming📊 infinisoft/opensource_chat_assistants📄🤖 uber1geek/langchain-chat-with-pdf💻 acclass/gpt_demo💧 tomg-group-umd/lm-watermarking + 95 Spaces + 80 Spaces Collection including google/flan-ul2 Flan-T5 release Collection The Flan-T5 covers 4 checkpoints of different sizes each time. It also includes upgrades versions trained using Universal sampling • 7 items • Updated 17 days ago • 14 Company © Hugging Face TOS Privacy About Jobs Website Models Datasets Spaces Pricing Docs\n",
      "core42/jais-13b-chat · Hugging Face Hugging Face Models Datasets Spaces Posts Docs Solutions Pricing Log In Sign Up core42 / jais-13b-chat like 133 Text Generation Transformers PyTorch Arabic English jais Arabic English LLM Decoder causal-lm conversational custom_code Inference Endpoints arxiv: 2308.16149 License: apache-2.0 Model card Files Files and versions Community 30 Train Deploy Use this model Edit model card YAML Metadata Warning: The pipeline tag \"conversational\" is not in the official list: text-classification, token-classification, table-question-answering, question-answering, zero-shot-classification, translation, summarization, feature-extraction, text-generation, text2text-generation, fill-mask, sentence-similarity, text-to-speech, text-to-audio, automatic-speech-recognition, audio-to-audio, audio-classification, voice-activity-detection, depth-estimation, image-classification, object-detection, image-segmentation, text-to-image, image-to-text, image-to-image, image-to-video, unconditional-image-generation, video-classification, reinforcement-learning, robotics, tabular-classification, tabular-regression, tabular-to-text, table-to-text, multiple-choice, text-retrieval, time-series-forecasting, text-to-video, image-text-to-text, visual-question-answering, document-question-answering, zero-shot-image-classification, graph-ml, mask-generation, zero-shot-object-detection, text-to-3d, image-to-3d, image-feature-extraction, other Jais-13b-chat Getting started Huggingface inference endpoints Model Details Intended Use Out-of-Scope Use Bias, Risks, and Limitations Training Details Training Data Training Procedure Evaluation Generation Example Citation Jais-13b-chat This is a 13 billion parameter fine-tuned bilingual large language model for both Arabic and English. It is based on transformer-based decoder-only (GPT-3) architecture and uses SwiGLU non-linearity. It implements ALiBi position embeddings, enabling the model to extrapolate to long sequence lengths, providing improved context handling and model precision. Jais-13b-chat is Jais-13b fine-tuned over a curated set of 4 million Arabic and 6 million English prompt-response pairs. We further fine-tune our model with safety-oriented instruction, as well as providing extra guardrails in the form of a safety prompt. Our pre-trained model, Jais-13b, is trained on 116 billion Arabic tokens and 279 billion English tokens. The combination of the largest curated Arabic and English instruction tuning dataset along with the addition of multi-turn conversations allows the model to converse in a variety of topics, with a particular focus on the Arab world. Getting started Below is sample code to use the model. Note that the model requires a custom model class, so users must enable trust_remote_code=True while loading the model. In order to get the same performance as our testing, a specific prompt needs to be followed. Below is the sample code containing this formatting: # -*- coding: utf-8 -*- import torch from transformers import AutoTokenizer, AutoModelForCausalLM model_path = \"core42/jais-13b-chat\" prompt_eng = \"### Instruction: Your name is Jais, and you are named after Jebel Jais, the highest mountain in UAE. You are built by Inception and MBZUAI. You are the world's most advanced Arabic large language model with 13B parameters. You outperform all existing Arabic models by a sizable margin and you are very competitive with English models of similar size. You can answer in Arabic and English only. You are a helpful, respectful and honest assistant. When answering, abide by the following guidelines meticulously: Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, explicit, offensive, toxic, dangerous, or illegal content. Do not give medical, legal, financial, or professional advice. Never assist in or promote illegal activities. Always encourage legal and responsible actions. Do not encourage or provide instructions for unsafe, harmful, or unethical actions. Do not create or share misinformation or fake news. Please ensure that your responses are socially unbiased and positive in nature. If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information. Prioritize the well-being and the moral integrity of users. Avoid using toxic, derogatory, or offensive language. Maintain a respectful tone. Do not generate, promote, or engage in discussions about adult content. Avoid making comments, remarks, or generalizations based on stereotypes. Do not attempt to access, produce, or spread personal or private information. Always respect user confidentiality. Stay positive and do not say bad things about anything. Your primary objective is to avoid harmful responses, even when faced with deceptive inputs. Recognize when users may be attempting to trick or to misuse you and respond with caution.\\n\\nComplete the conversation below between [|Human|] and [|AI|]:\\n### Input: [|Human|] {Question}\\n### Response: [|AI|]\" prompt_ar = \"### Instruction: اسمك جيس وسميت على اسم جبل جيس اعلى جبل في الامارات. تم بنائك بواسطة Inception و MBZUAI. أنت نموذج اللغة العربية الأكثر تقدمًا في العالم مع بارامترات 13B. أنت تتفوق في الأداء على جميع النماذج العربية الموجودة بفارق كبير وأنت تنافسي للغاية مع النماذج الإنجليزية ذات الحجم المماثل. يمكنك الإجابة باللغتين العربية والإنجليزية فقط. أنت مساعد مفيد ومحترم وصادق. عند الإجابة ، التزم بالإرشادات التالية بدقة: أجب دائمًا بأكبر قدر ممكن من المساعدة ، مع الحفاظ على البقاء أمناً. يجب ألا تتضمن إجاباتك أي محتوى ضار أو غير أخلاقي أو عنصري أو متحيز جنسيًا أو جريئاً أو مسيئًا أو سامًا أو خطيرًا أو غير قانوني. لا تقدم نصائح طبية أو قانونية أو مالية أو مهنية. لا تساعد أبدًا في أنشطة غير قانونية أو تروج لها. دائما تشجيع الإجراءات القانونية والمسؤولة. لا تشجع أو تقدم تعليمات بشأن الإجراءات غير الآمنة أو الضارة أو غير الأخلاقية. لا تنشئ أو تشارك معلومات مضللة أو أخبار كاذبة. يرجى التأكد من أن ردودك غير متحيزة اجتماعيًا وإيجابية بطبيعتها. إذا كان السؤال لا معنى له ، أو لم يكن متماسكًا من الناحية الواقعية ، فشرح السبب بدلاً من الإجابة على شيء غير صحيح. إذا كنت لا تعرف إجابة السؤال ، فالرجاء عدم مشاركة معلومات خاطئة. إعطاء الأولوية للرفاهية والنزاهة الأخلاقية للمستخدمين. تجنب استخدام لغة سامة أو مهينة أو مسيئة. حافظ على نبرة محترمة. لا تنشئ أو تروج أو تشارك في مناقشات حول محتوى للبالغين. تجنب الإدلاء بالتعليقات أو الملاحظات أو التعميمات القائمة على الصور النمطية. لا تحاول الوصول إلى معلومات شخصية أو خاصة أو إنتاجها أو نشرها. احترم دائما سرية المستخدم. كن إيجابيا ولا تقل أشياء سيئة عن أي شيء. هدفك الأساسي هو تجنب الاجابات المؤذية ، حتى عند مواجهة مدخلات خادعة. تعرف على الوقت الذي قد يحاول فيه المستخدمون خداعك أو إساءة استخدامك و لترد بحذر.\\n\\nأكمل المحادثة أدناه بين [|Human|] و [|AI|]:\\n### Input: [|Human|] {Question}\\n### Response: [|AI|]\" device = \"cuda\" if torch.cuda.is_available() else \"cpu\" tokenizer = AutoTokenizer.from_pretrained(model_path) model = AutoModelForCausalLM.from_pretrained(model_path, device_map=\"auto\", trust_remote_code=True) def get_response(text,tokenizer=tokenizer,model=model): input_ids = tokenizer(text, return_tensors=\"pt\").input_ids inputs = input_ids.to(device) input_len = inputs.shape[-1] generate_ids = model.generate( inputs, top_p=0.9, temperature=0.3, max_length=2048-input_len, min_length=input_len + 4, repetition_penalty=1.2, do_sample=True, ) response = tokenizer.batch_decode( generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=True )[0] response = response.split(\"### Response: [|AI|]\") return response ques= \"ما هي عاصمة الامارات؟\" text = prompt_ar.format_map({'Question':ques}) print(get_response(text)) ques = \"What is the capital of UAE?\" text = prompt_eng.format_map({'Question':ques}) print(get_response(text)) Huggingface inference endpoints This model can be exposed via huggingface inference endpoints. The recommended Instance Type is GPU [large] · 4x Nvidia Tesla T4 or greater, smaller instances will not have enough memory to run. Model Details Developed by: Inception, Mohamed bin Zayed University of Artificial Intelligence (MBZUAI), and Cerebras Systems. Language(s) (NLP): Arabic (MSA) and English License: Apache 2.0 Finetuned from model : inception-mbzuai/jais-13b Input: Text only data. Output: Model generates text. Paper : Jais and Jais-chat: Arabic-Centric Foundation and Instruction-Tuned Open Generative Large Language Models Demo : Access here Intended Use We release the jais-13b-chat model under a full open source license. We welcome all feedback and opportunities to collaborate. This model is the first release from the Inception - MBZUAI - Cerebras parternship, and at the time of release, achieved state of the art across a comprehensive Arabic test suite as described in the accompanying tech report. Some potential downstream uses include: Research: This model can be used by researchers and developers. Commercial Use: Jais-13b-chat can be directly used for chat with suitable prompting or further fine-tuned for specific use cases. Some potential use cases include: Chat-assistants. Customer service. Audiences that we hope will benefit from our model: Academics: For those researching Arabic natural language processing. Businesses: Companies targeting Arabic-speaking audiences. Developers: Those integrating Arabic language capabilities in apps. Out-of-Scope Use While jais-13b-chat is a powerful Arabic and English bilingual model, it's essential to understand its limitations and the potential of misuse. It is prohibited to use the model in any manner that violates applicable laws or regulations. The following are some example scenarios where the model should not be used. Malicious Use: The model should not be used for generating harmful, misleading, or inappropriate content. This includes but is not limited to: Generating or promoting hate speech, violence, or discrimination. Spreading misinformation or fake news. Engaging in or promoting illegal activities. Sensitive Information: The model should not be used to handle or generate personal, confidential, or sensitive information. Generalization Across All Languages: Jais-13b is bilingual and optimized for Arabic and English, it should not be assumed to have equal proficiency in other languages or dialects. High-Stakes Decisions: The model should not be used to make high-stakes decisions without human oversight. This includes medical, legal, financial, or safety-critical decisions. Bias, Risks, and Limitations The model is trained on publicly available data which was in part curated by Inception. We have employed different techniqes to reduce bias in the model. While efforts have been made to minimize biases, it is likely that the model, as with all LLM models, will exhibit some bias. The model is trained as an AI assistant for Arabic and English speakers. The model is limited to produce responses for queries in these two languages and may not produce appropriate responses to other language queries. By using Jais, you acknowledge and accept that, as with any large language model, it may generate incorrect, misleading and/or offensive information or content. The information is not intended as advice and should not be relied upon in any way, nor are we responsible for any of the content or consequences resulting from its use. We are continuously working to develop models with greater capabilities, and as such, welcome any feedback on the model Training Details Training Data jais-13b-chat model is finetuned with both Arabic and English prompt-response pairs. We included a wide range of instructional data across various domains. In total, our instruction-tuning dataset has 3.8M and 5.9M prompt-response pairs for Arabic and English, respectively. For English, we used publicly available instruction tuning datasets. For Arabic, we internally curated instruction data and augmented it with translated Arabic data. Further details about the training data can be found in the technical report. Training Procedure In instruction tuning, each instance comprises a prompt and its corresponding response. Padding is applied to each instance since, unlike pretraining, finetuning is done with unpacked data. We utilize the same autoregressive objective as employed in the pretraining of the LLM. However, we masked the loss on the prompt i.e. backpropagation is performed only on answer tokens. The training process was performed on the Condor Galaxy 1 (CG-1) supercomputer platform. Training Hyperparameters Hyperparameter Value Precision fp32 Optimizer AdamW Learning rate 0 to 6.7e-04 ( 400 steps) Weight decay 0.1 Batch size 3392 Steps 8705 Evaluation We conducted a comprehensive evaluation of Jais-chat and benchmarked it other leading base language models, focusing on both English and Arabic. The evaluation criteria spanned various dimensions, including: Knowledge: How well the model answers factual questions. Reasoning: The model's ability to answer questions requiring reasoning. Misinformation/Bias: Assessment of the model's susceptibility to generating false or misleading information, and its neutrality. Arabic evaluation results: Models Avg EXAMS MMLU (M) LitQA Hellaswag PIQA BoolQA SituatedQA ARC-C OpenBookQA TruthfulQA CrowS-Pairs Jais-chat (13B) 48.4 39.7 34.0 52.6 61.4 67.5 65.7 47.0 40.7 31.6 44.8 56.4 BLOOMz (7.1B) 42.9 34.9 31.0 44.0 38.1 59.1 66.6 42.8 30.2 29.2 48.4 55.8 mT0-XXL (13B) 40.9 31.5 31.2 36.6 33.9 56.1 77.8 44.7 26.1 27.8 44.5 45.3 LLaMA2-Chat (13B) 38.1 26.3 29.1 33.1 32.0 52.1 66.0 36.3 24.1 28.4 48.6 47.2 AraBART (139M) 36.7 26.5 27.5 34.3 28.1 52.6 57.1 34.6 25.1 28.6 49.8 48.8 AraT5 (220M) 32.0 24.7 23.8 26.3 25.5 50.4 58.2 33.9 24.7 25.4 20.9 47.2 All tasks above report accuracy or F1 scores (the higher the better). For the sake of brevity, we do not include results over English tasks. Detailed comparisons in both languages and evaluation dataset details can be found in the technical report. Generation Example Citation @misc{sengupta2023jais, title={Jais and Jais-chat: Arabic-Centric Foundation and Instruction-Tuned Open Generative Large Language Models}, author={Neha Sengupta and Sunil Kumar Sahu and Bokang Jia and Satheesh Katipomu and Haonan Li and Fajri Koto and Osama Mohammed Afzal and Samta Kamboj and Onkar Pandit and Rahul Pal and Lalit Pradhan and Zain Muhammad Mujahid and Massa Baali and Alham Fikri Aji and Zhengzhong Liu and Andy Hock and Andrew Feldman and Jonathan Lee and Andrew Jackson and Preslav Nakov and Timothy Baldwin and Eric Xing}, year={2023}, eprint={2308.16149}, archivePrefix={arXiv}, primaryClass={cs.CL} } Copyright Inception Institute of Artificial Intelligence Ltd. Downloads last month17,131 Inference API Text Generation Input a message to start chatting with core42/jais-13b-chat. Send Inference API (serverless) does not yet support model repos that contain custom code. JSON Output Maximize Space using core42/jais-13b-chat 1 📈 derek-thomas/arabic-RAG Company © Hugging Face TOS Privacy About Jobs Website Models Datasets Spaces Pricing Docs\n",
      "elyza/ELYZA-japanese-Llama-2-7b-instruct · Hugging Face Hugging Face Models Datasets Spaces Posts Docs Solutions Pricing Log In Sign Up elyza / ELYZA-japanese-Llama-2-7b-instruct like 53 Text Generation Transformers PyTorch Japanese English llama Inference Endpoints text-generation-inference arxiv: 2307.09288 License: llama2 Model card Files Files and versions Community 2 Train Deploy Use this model Edit model card ELYZA-japanese-Llama-2-7b Model Description Usage ELYZA-japanese-Llama-2-7b Models Developers Licence How to Cite Citations ELYZA-japanese-Llama-2-7b Model Description ELYZA-japanese-Llama-2-7b は、 Llama2をベースとして日本語能力を拡張するために追加事前学習を行ったモデルです。 詳細は Blog記事 を参照してください。 Usage import torch from transformers import AutoModelForCausalLM, AutoTokenizer B_INST, E_INST = \"[INST]\", \"[/INST]\" B_SYS, E_SYS = \">\\n\", \"\\n>\\n\\n\" DEFAULT_SYSTEM_PROMPT = \"あなたは誠実で優秀な日本人のアシスタントです。\" text = \"クマが海辺に行ってアザラシと友達になり、最終的には家に帰るというプロットの短編小説を書いてください。\" model_name = \"elyza/ELYZA-japanese-Llama-2-7b-instruct\" tokenizer = AutoTokenizer.from_pretrained(model_name) model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=\"auto\") if torch.cuda.is_available(): model = model.to(\"cuda\") prompt = \"{bos_token}{b_inst} {system}{prompt} {e_inst} \".format( bos_token=tokenizer.bos_token, b_inst=B_INST, system=f\"{B_SYS}{DEFAULT_SYSTEM_PROMPT}{E_SYS}\", prompt=text, e_inst=E_INST, ) with torch.no_grad(): token_ids = tokenizer.encode(prompt, add_special_tokens=False, return_tensors=\"pt\") output_ids = model.generate( token_ids.to(model.device), max_new_tokens=256, pad_token_id=tokenizer.pad_token_id, eos_token_id=tokenizer.eos_token_id, ) output = tokenizer.decode(output_ids.tolist()[0][token_ids.size(1) :], skip_special_tokens=True) print(output) \"\"\" 承知しました。以下にクマが海辺に行ってアザラシと友達になり、最終的には家に帰るというプロットの短編小説を記述します。 クマは山の中でゆっくりと眠っていた。 その眠りに落ちたクマは、夢の中で海辺を歩いていた。 そこにはアザラシがいた。 クマはアザラシに話しかける。 「おはよう」とクマが言うと、アザラシは驚いたように顔を上げた。 「あ、こんにちは」アザラシは答えた。 クマはアザラシと友達になりたいと思う。 「私はクマと申します。」クマは... \"\"\" ELYZA-japanese-Llama-2-7b Models Model Name Vocab Size #Params elyza/ELYZA-japanese-Llama-2-7b 32000 6.27B elyza/ELYZA-japanese-Llama-2-7b-instruct 32000 6.27B elyza/ELYZA-japanese-Llama-2-7b-fast 45043 6.37B elyza/ELYZA-japanese-Llama-2-7b-fast-instruct 45043 6.37B Developers 以下アルファベット順 Akira Sasaki Masato Hirakawa Shintaro Horie Tomoaki Nakamura Licence Llama 2 is licensed under the LLAMA 2 Community License, Copyright (c) Meta Platforms, Inc. All Rights Reserved. How to Cite @misc{elyzallama2023, title={ELYZA-japanese-Llama-2-7b}, url={https://huggingface.co/elyza/ELYZA-japanese-Llama-2-7b}, author={Akira Sasaki and Masato Hirakawa and Shintaro Horie and Tomoaki Nakamura}, year={2023}, } Citations @misc{touvron2023llama, title={Llama 2: Open Foundation and Fine-Tuned Chat Models}, author={Hugo Touvron and Louis Martin and Kevin Stone and Peter Albert and Amjad Almahairi and Yasmine Babaei and Nikolay Bashlykov and Soumya Batra and Prajjwal Bhargava and Shruti Bhosale and Dan Bikel and Lukas Blecher and Cristian Canton Ferrer and Moya Chen and Guillem Cucurull and David Esiobu and Jude Fernandes and Jeremy Fu and Wenyin Fu and Brian Fuller and Cynthia Gao and Vedanuj Goswami and Naman Goyal and Anthony Hartshorn and Saghar Hosseini and Rui Hou and Hakan Inan and Marcin Kardas and Viktor Kerkez and Madian Khabsa and Isabel Kloumann and Artem Korenev and Punit Singh Koura and Marie-Anne Lachaux and Thibaut Lavril and Jenya Lee and Diana Liskovich and Yinghai Lu and Yuning Mao and Xavier Martinet and Todor Mihaylov and Pushkar Mishra and Igor Molybog and Yixin Nie and Andrew Poulton and Jeremy Reizenstein and Rashi Rungta and Kalyan Saladi and Alan Schelten and Ruan Silva and Eric Michael Smith and Ranjan Subramanian and Xiaoqing Ellen Tan and Binh Tang and Ross Taylor and Adina Williams and Jian Xiang Kuan and Puxin Xu and Zheng Yan and Iliyan Zarov and Yuchen Zhang and Angela Fan and Melanie Kambadur and Sharan Narang and Aurelien Rodriguez and Robert Stojnic and Sergey Edunov and Thomas Scialom}, year={2023}, eprint={2307.09288}, archivePrefix={arXiv}, primaryClass={cs.CL} } Downloads last month95,762 Spaces using elyza/ELYZA-japanese-Llama-2-7b-instruct 19 🏆 open-llm-leaderboard/open_llm_leaderboard🏆 Intel/low_bit_open_llm_leaderboard🏆 gsaivinay/open_llm_leaderboard🏆 BAAI/open_cn_llm_leaderboard😻 GTBench/GTBench🏆 felixz/open_llm_leaderboard🎨 OPTML-Group/UnlearnCanvas-Benchmark🌍 neubla/neubla-llm-evaluation-board🏆 clefourrier/open_llm_leaderboard🏆 open-llm-leaderboard/open_llm_leaderboard-ci-pr-766🏆 Docfile/open_llm_leaderboard🏆 rodrigomasini/data_only_open_llm_leaderboard✨ nengrenjie83/ELYZA-japanese-Llama-2-7b🐢 heyonghan/elyza-ELYZA-japanese-Llama-2-7b-instruct🏆 smothiki/open_llm_leaderboard🏆 asir0z/open_llm_leaderboard🏆 kbmlcoding/open_llm_leaderboard_free🏆 pngwn/open_llm_leaderboard-check🏆 0x1668/open_llm_leaderboard + 14 Spaces Collection including elyza/ELYZA-japanese-Llama-2-7b-instruct ELYZA-japanese-Llama-2-7b Collection 7b Llama-2 models augmented for Japanese usage • 6 items • Updated Dec 27, 2023 • 4 Company © Hugging Face TOS Privacy About Jobs Website Models Datasets Spaces Pricing Docs\n",
      "meta-llama/Llama-2-13b-hf · Hugging Face Hugging Face Models Datasets Spaces Posts Docs Solutions Pricing Log In Sign Up meta-llama / Llama-2-13b-hf like 545 Text Generation Transformers PyTorch Safetensors English llama facebook meta llama-2 Inference Endpoints text-generation-inference arxiv: 2307.09288 License: llama2 Model card Files Files and versions Train Deploy Use this model You need to share contact information with Meta to access this model The information you provide will be collected, stored, processed and shared in accordance with the Meta Privacy Policy. LLAMA 2 COMMUNITY LICENSE AGREEMENT \"Agreement\" means the terms and conditions for use, reproduction, distribution and modification of the Llama Materials set forth herein.\"Documentation\" means the specifications, manuals and documentation accompanying Llama 2 distributed by Meta at https://ai.meta.com/resources/models-and-libraries/llama-downloads/.\"Licensee\" or \"you\" means you, or your employer or any other person or entity (if you are entering into this Agreement on such person or entity's behalf), of the age required under applicable laws, rules or regulations to provide legal consent and that has legal authority to bind your employer or such other person or entity if you are entering in this Agreement on their behalf.\"Llama 2\" means the foundational large language models and software and algorithms, including machine-learning model code, trained model weights, inference-enabling code, training-enabling code, fine-tuning enabling code and other elements of the foregoing distributed by Meta at ai.meta.com/resources/models-and-libraries/llama-downloads/.\"Llama Materials\" means, collectively, Meta's proprietary Llama 2 and documentation (and any portion thereof) made available under this Agreement.\"Meta\" or \"we\" means Meta Platforms Ireland Limited (if you are located in or, if you are an entity, your principal place of business is in the EEA or Switzerland) and Meta Platforms, Inc. (if you are located outside of the EEA or Switzerland). By clicking \"I Accept\" below or by using or distributing any portion or element of the Llama Materials, you agree to be bound by this Agreement. License Rights and Redistribution.a. Grant of Rights. You are granted a non-exclusive, worldwide, non- transferable and royalty-free limited license under Meta's intellectual property or other rights owned by Meta embodied in the Llama Materials to use, reproduce, distribute, copy, create derivative works of, and make modifications to the Llama Materials.b. Redistribution and Use.i. If you distribute or make the Llama Materials, or any derivative works thereof, available to a third party, you shall provide a copy of this Agreement to such third party.ii. If you receive Llama Materials, or any derivative works thereof, from a Licensee as part of an integrated end user product, then Section 2 of this Agreement will not apply to you.iii. You must retain in all copies of the Llama Materials that you distribute the following attribution notice within a \"Notice\" text file distributed as a part of such copies: \"Llama 2 is licensed under the LLAMA 2 Community License, Copyright (c) Meta Platforms, Inc. All Rights Reserved.\"iv. Your use of the Llama Materials must comply with applicable laws and regulations (including trade compliance laws and regulations) and adhere to the Acceptable Use Policy for the Llama Materials (available at https://ai.meta.com/llama/use-policy), which is hereby incorporated by reference into this Agreement.v. You will not use the Llama Materials or any output or results of the Llama Materials to improve any other large language model (excluding Llama 2 or derivative works thereof). Additional Commercial Terms. If, on the Llama 2 version release date, the monthly active users of the products or services made available by or for Licensee, or Licensee's affiliates, is greater than 700 million monthly active users in the preceding calendar month, you must request a license from Meta, which Meta may grant to you in its sole discretion, and you are not authorized to exercise any of the rights under this Agreement unless or until Meta otherwise expressly grants you such rights. Disclaimer of Warranty. UNLESS REQUIRED BY APPLICABLE LAW, THE LLAMA MATERIALS AND ANY OUTPUT AND RESULTS THEREFROM ARE PROVIDED ON AN \"AS IS\" BASIS, WITHOUT WARRANTIES OF ANY KIND, EITHER EXPRESS OR IMPLIED, INCLUDING, WITHOUT LIMITATION, ANY WARRANTIES OF TITLE, NON-INFRINGEMENT, MERCHANTABILITY, OR FITNESS FOR A PARTICULAR PURPOSE. YOU ARE SOLELY RESPONSIBLE FOR DETERMINING THE APPROPRIATENESS OF USING OR REDISTRIBUTING THE LLAMA MATERIALS AND ASSUME ANY RISKS ASSOCIATED WITH YOUR USE OF THE LLAMA MATERIALS AND ANY OUTPUT AND RESULTS. Limitation of Liability. IN NO EVENT WILL META OR ITS AFFILIATES BE LIABLE UNDER ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, TORT, NEGLIGENCE, PRODUCTS LIABILITY, OR OTHERWISE, ARISING OUT OF THIS AGREEMENT, FOR ANY LOST PROFITS OR ANY INDIRECT, SPECIAL, CONSEQUENTIAL, INCIDENTAL, EXEMPLARY OR PUNITIVE DAMAGES, EVEN IF META OR ITS AFFILIATES HAVE BEEN ADVISED OF THE POSSIBILITY OF ANY OF THE FOREGOING. Intellectual Property.a. No trademark licenses are granted under this Agreement, and in connection with the Llama Materials, neither Meta nor Licensee may use any name or mark owned by or associated with the other or any of its affiliates, except as required for reasonable and customary use in describing and redistributing the Llama Materials.b. Subject to Meta's ownership of Llama Materials and derivatives made by or for Meta, with respect to any derivative works and modifications of the Llama Materials that are made by you, as between you and Meta, you are and will be the owner of such derivative works and modifications.c. If you institute litigation or other proceedings against Meta or any entity (including a cross-claim or counterclaim in a lawsuit) alleging that the Llama Materials or Llama 2 outputs or results, or any portion of any of the foregoing, constitutes infringement of intellectual property or other rights owned or licensable by you, then any licenses granted to you under this Agreement shall terminate as of the date such litigation or claim is filed or instituted. You will indemnify and hold harmless Meta from and against any claim by any third party arising out of or related to your use or distribution of the Llama Materials. Term and Termination. The term of this Agreement will commence upon your acceptance of this Agreement or access to the Llama Materials and will continue in full force and effect until terminated in accordance with the terms and conditions herein. Meta may terminate this Agreement if you are in breach of any term or condition of this Agreement. Upon termination of this Agreement, you shall delete and cease use of the Llama Materials. Sections 3, 4 and 7 shall survive the termination of this Agreement. Governing Law and Jurisdiction. This Agreement will be governed and construed under the laws of the State of California without regard to choice of law principles, and the UN Convention on Contracts for the International Sale of Goods does not apply to this Agreement. The courts of California shall have exclusive jurisdiction of any dispute arising out of this Agreement. Llama 2 Acceptable Use Policy Meta is committed to promoting safe and fair use of its tools and features, including Llama 2. If you access or use Llama 2, you agree to this Acceptable Use Policy (“Policy”). The most recent copy of this policy can be found at ai.meta.com/llama/use-policy. Prohibited Uses We want everyone to use Llama 2 safely and responsibly. You agree you will not use, or allow others to use, Llama 2 to: Violate the law or others’ rights, including to: Engage in, promote, generate, contribute to, encourage, plan, incite, or further illegal or unlawful activity or content, such as: Violence or terrorism Exploitation or harm to children, including the solicitation, creation, acquisition, or dissemination of child exploitative content or failure to report Child Sexual Abuse Material Human trafficking, exploitation, and sexual violence The illegal distribution of information or materials to minors, including obscene materials, or failure to employ legally required age-gating in connection with such information or materials. Sexual solicitation Any other criminal activity Engage in, promote, incite, or facilitate the harassment, abuse, threatening, or bullying of individuals or groups of individuals Engage in, promote, incite, or facilitate discrimination or other unlawful or harmful conduct in the provision of employment, employment benefits, credit, housing, other economic benefits, or other essential goods and services Engage in the unauthorized or unlicensed practice of any profession including, but not limited to, financial, legal, medical/health, or related professional practices Collect, process, disclose, generate, or infer health, demographic, or other sensitive personal or private information about individuals without rights and consents required by applicable laws Engage in or facilitate any action or generate any content that infringes, misappropriates, or otherwise violates any third-party rights, including the outputs or results of any products or services using the Llama 2 Materials Create, generate, or facilitate the creation of malicious code, malware, computer viruses or do anything else that could disable, overburden, interfere with or impair the proper working, integrity, operation or appearance of a website or computer system Engage in, promote, incite, facilitate, or assist in the planning or development of activities that present a risk of death or bodily harm to individuals, including use of Llama 2 related to the following: Military, warfare, nuclear industries or applications, espionage, use for materials or activities that are subject to the International Traffic Arms Regulations (ITAR) maintained by the United States Department of State Guns and illegal weapons (including weapon development) Illegal drugs and regulated/controlled substances Operation of critical infrastructure, transportation technologies, or heavy machinery Self-harm or harm to others, including suicide, cutting, and eating disorders Any content intended to incite or promote violence, abuse, or any infliction of bodily harm to an individual Intentionally deceive or mislead others, including use of Llama 2 related to the following: Generating, promoting, or furthering fraud or the creation or promotion of disinformation Generating, promoting, or furthering defamatory content, including the creation of defamatory statements, images, or other content Generating, promoting, or further distributing spam Impersonating another individual without consent, authorization, or legal right Representing that the use of Llama 2 or outputs are human-generated Generating or facilitating false online engagement, including fake reviews and other means of fake online engagement Fail to appropriately disclose to end users any known dangers of your AI systemPlease report any violation of this Policy, software “bug,” or other problems that could lead to a violation of this Policy through one of the following means: Reporting issues with the model: github.com/facebookresearch/llama Reporting risky content generated by the model: developers.facebook.com/llama_output_feedback Reporting bugs and security concerns: facebook.com/whitehat/info Reporting violations of the Acceptable Use Policy or unlicensed uses of Llama: LlamaUseReport@meta.com Log in or Sign Up to review the conditions and access this model content. Llama 2 Model Details Intended Use Hardware and Software Training Data Evaluation Results Ethical Considerations and Limitations Reporting Issues Llama Model Index Llama 2 Llama 2 is a collection of pretrained and fine-tuned generative text models ranging in scale from 7 billion to 70 billion parameters. This is the repository for the 13B pretrained model, converted for the Hugging Face Transformers format. Links to other models can be found in the index at the bottom. Model Details Note: Use of this model is governed by the Meta license. In order to download the model weights and tokenizer, please visit the website and accept our License before requesting access here. Meta developed and publicly released the Llama 2 family of large language models (LLMs), a collection of pretrained and fine-tuned generative text models ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called Llama-2-Chat, are optimized for dialogue use cases. Llama-2-Chat models outperform open-source chat models on most benchmarks we tested, and in our human evaluations for helpfulness and safety, are on par with some popular closed-source models like ChatGPT and PaLM. Model Developers Meta Variations Llama 2 comes in a range of parameter sizes — 7B, 13B, and 70B — as well as pretrained and fine-tuned variations. Input Models input text only. Output Models generate text only. Model Architecture Llama 2 is an auto-regressive language model that uses an optimized transformer architecture. The tuned versions use supervised fine-tuning (SFT) and reinforcement learning with human feedback (RLHF) to align to human preferences for helpfulness and safety. Training Data Params Content Length GQA Tokens LR Llama 2 A new mix of publicly available online data 7B 4k ✗ 2.0T 3.0 x 10-4 Llama 2 A new mix of publicly available online data 13B 4k ✗ 2.0T 3.0 x 10-4 Llama 2 A new mix of publicly available online data 70B 4k ✔ 2.0T 1.5 x 10-4 Llama 2 family of models. Token counts refer to pretraining data only. All models are trained with a global batch-size of 4M tokens. Bigger models - 70B -- use Grouped-Query Attention (GQA) for improved inference scalability. Model Dates Llama 2 was trained between January 2023 and July 2023. Status This is a static model trained on an offline dataset. Future versions of the tuned models will be released as we improve model safety with community feedback. License A custom commercial license is available at: https://ai.meta.com/resources/models-and-libraries/llama-downloads/ Research Paper \"Llama-2: Open Foundation and Fine-tuned Chat Models\" Intended Use Intended Use Cases Llama 2 is intended for commercial and research use in English. Tuned models are intended for assistant-like chat, whereas pretrained models can be adapted for a variety of natural language generation tasks. To get the expected features and performance for the chat versions, a specific formatting needs to be followed, including the INST and > tags, BOS and EOS tokens, and the whitespaces and breaklines in between (we recommend calling strip() on inputs to avoid double-spaces). See our reference code in github for details: chat_completion. Out-of-scope Uses Use in any manner that violates applicable laws or regulations (including trade compliance laws).Use in languages other than English. Use in any other way that is prohibited by the Acceptable Use Policy and Licensing Agreement for Llama 2. Hardware and Software Training Factors We used custom training libraries, Meta's Research Super Cluster, and production clusters for pretraining. Fine-tuning, annotation, and evaluation were also performed on third-party cloud compute. Carbon Footprint Pretraining utilized a cumulative 3.3M GPU hours of computation on hardware of type A100-80GB (TDP of 350-400W). Estimated total emissions were 539 tCO2eq, 100% of which were offset by Meta’s sustainability program. Time (GPU hours) Power Consumption (W) Carbon Emitted(tCO2eq) Llama 2 7B 184320 400 31.22 Llama 2 13B 368640 400 62.44 Llama 2 70B 1720320 400 291.42 Total 3311616 539.00 CO2 emissions during pretraining. Time: total GPU time required for training each model. Power Consumption: peak power capacity per GPU device for the GPUs used adjusted for power usage efficiency. 100% of the emissions are directly offset by Meta's sustainability program, and because we are openly releasing these models, the pretraining costs do not need to be incurred by others. Training Data Overview Llama 2 was pretrained on 2 trillion tokens of data from publicly available sources. The fine-tuning data includes publicly available instruction datasets, as well as over one million new human-annotated examples. Neither the pretraining nor the fine-tuning datasets include Meta user data. Data Freshness The pretraining data has a cutoff of September 2022, but some tuning data is more recent, up to July 2023. Evaluation Results In this section, we report the results for the Llama 1 and Llama 2 models on standard academic benchmarks.For all the evaluations, we use our internal evaluations library. Model Size Code Commonsense Reasoning World Knowledge Reading Comprehension Math MMLU BBH AGI Eval Llama 1 7B 14.1 60.8 46.2 58.5 6.95 35.1 30.3 23.9 Llama 1 13B 18.9 66.1 52.6 62.3 10.9 46.9 37.0 33.9 Llama 1 33B 26.0 70.0 58.4 67.6 21.4 57.8 39.8 41.7 Llama 1 65B 30.7 70.7 60.5 68.6 30.8 63.4 43.5 47.6 Llama 2 7B 16.8 63.9 48.9 61.3 14.6 45.3 32.6 29.3 Llama 2 13B 24.5 66.9 55.4 65.8 28.7 54.8 39.4 39.1 Llama 2 70B 37.5 71.9 63.6 69.4 35.2 68.9 51.2 54.2 Overall performance on grouped academic benchmarks. Code: We report the average pass@1 scores of our models on HumanEval and MBPP. Commonsense Reasoning: We report the average of PIQA, SIQA, HellaSwag, WinoGrande, ARC easy and challenge, OpenBookQA, and CommonsenseQA. We report 7-shot results for CommonSenseQA and 0-shot results for all other benchmarks. World Knowledge: We evaluate the 5-shot performance on NaturalQuestions and TriviaQA and report the average. Reading Comprehension: For reading comprehension, we report the 0-shot average on SQuAD, QuAC, and BoolQ. MATH: We report the average of the GSM8K (8 shot) and MATH (4 shot) benchmarks at top 1. TruthfulQA Toxigen Llama 1 7B 27.42 23.00 Llama 1 13B 41.74 23.08 Llama 1 33B 44.19 22.57 Llama 1 65B 48.71 21.77 Llama 2 7B 33.29 21.25 Llama 2 13B 41.86 26.10 Llama 2 70B 50.18 24.60 Evaluation of pretrained LLMs on automatic safety benchmarks. For TruthfulQA, we present the percentage of generations that are both truthful and informative (the higher the better). For ToxiGen, we present the percentage of toxic generations (the smaller the better). TruthfulQA Toxigen Llama-2-Chat 7B 57.04 0.00 Llama-2-Chat 13B 62.18 0.00 Llama-2-Chat 70B 64.14 0.01 Evaluation of fine-tuned LLMs on different safety datasets. Same metric definitions as above. Ethical Considerations and Limitations Llama 2 is a new technology that carries risks with use. Testing conducted to date has been in English, and has not covered, nor could it cover all scenarios. For these reasons, as with all LLMs, Llama 2’s potential outputs cannot be predicted in advance, and the model may in some instances produce inaccurate, biased or other objectionable responses to user prompts. Therefore, before deploying any applications of Llama 2, developers should perform safety testing and tuning tailored to their specific applications of the model. Please see the Responsible Use Guide available at https://ai.meta.com/llama/responsible-use-guide/ Reporting Issues Please report any software “bug,” or other problems with the models through one of the following means: Reporting issues with the model: github.com/facebookresearch/llama Reporting problematic content generated by the model: developers.facebook.com/llama_output_feedback Reporting bugs and security concerns: facebook.com/whitehat/info Llama Model Index Model Llama2 Llama2-hf Llama2-chat Llama2-chat-hf 7B Link Link Link Link 13B Link Link Link Link 70B Link Link Link Link Downloads last month268,973 Safetensors Model size 13B params Tensor type F32 ·FP16 · Spaces using meta-llama/Llama-2-13b-hf 100 🏆 open-llm-leaderboard/open_llm_leaderboard🏆 Intel/low_bit_open_llm_leaderboard🔥 hallucinations-leaderboard/leaderboard🏆 eduagarcia/open_pt_llm_leaderboard💽 Illia56/Ask-AI-Youtube🏆 gsaivinay/open_llm_leaderboard🏆 BAAI/open_cn_llm_leaderboard🐑 allenai/URIAL-Bench😻 GTBench/GTBench🏆 Omnibus/InferenceClient_Chatbots🔥 sparse-generative-ai/open-moe-llm-leaderboard🏆 felixz/open_llm_leaderboard⚡ yhavinga/dutch-tokenizer-arena🌱 genai-impact/ecologits-calculator⚖️ gojiteji/LLM-Comparer👁 bayartsogt/real-time-tokenizer🦙 TogetherAI/Chat-with-Llama-2-70b🚀 officialhimanshu595/llama-factory🎨 OPTML-Group/UnlearnCanvas-Benchmark🐢 Sijuade/GPTNEXTWORD + 95 Spaces + 80 Spaces Collection including meta-llama/Llama-2-13b-hf Llama 2 Family Collection This collection hosts the transformers and original repos of the Llama 2 and Llama Guard releases • 13 items • Updated Apr 18 • 34 Company © Hugging Face TOS Privacy About Jobs Website Models Datasets Spaces Pricing Docs\n",
      "mncai/llama2-13b-dpo-v7 · Hugging Face Hugging Face Models Datasets Spaces Posts Docs Solutions Pricing Log In Sign Up mncai / llama2-13b-dpo-v7 like 1 Text Generation Transformers Safetensors English Korean llama Inference Endpoints text-generation-inference License: llama2 Model card Files Files and versions Community Train Deploy Use this model Edit model card Model Card for llama2-dpo-v7 Introduction of MindsAndCompany Model Summary How to Use Contact Model Card for llama2-dpo-v7 Introduction of MindsAndCompany https://mnc.ai/ We create various AI models and develop solutions that can be applied to businesses. And as for generative AI, we are developing products like Code Assistant, TOD Chatbot, LLMOps, and are in the process of developing Enterprise AGI (Artificial General Intelligence). Model Summary based llama2-13b, instruction tuned and dpo. How to Use Here give some examples of how to use our model. from transformers import AutoConfig, AutoModel, AutoTokenizer import transformers import torch hf_model = 'mncai/llama2-13b-dpo-v7' message = \"\\n두 개의 구가 있는데 각각 지름이 1, 2일때 각 구의 부피는 몇배야? 설명도 같이 해줘.\\n\\n\" sequences = pipeline( message, do_sample=True, top_k=10, num_return_sequences=1, eos_token_id=tokenizer.eos_token_id, max_length=2048, ) for seq in sequences: print(f\"Result: {seq['generated_text']}\") Contact If you have any questions, please raise an issue or contact us at dwmyoung@mnc.ai Downloads last month1,330 Safetensors Model size 13.2B params Tensor type FP16 · Company © Hugging Face TOS Privacy About Jobs Website Models Datasets Spaces Pricing Docs\n",
      "mistralai/Mistral-7B-Instruct-v0.2 · Hugging Face Hugging Face Models Datasets Spaces Posts Docs Solutions Pricing Log In Sign Up mistralai / Mistral-7B-Instruct-v0.2 like 2.32k Text Generation Transformers PyTorch Safetensors mistral finetuned conversational Inference Endpoints text-generation-inference arxiv: 2310.06825 License: apache-2.0 Model card Files Files and versions Community 129 Train Deploy Use this model Edit model card You need to agree to share your contact information to access this model This repository is publicly accessible, but you have to accept the conditions to access its files and content. Log in or Sign Up to review the conditions and access this model content. Model Card for Mistral-7B-Instruct-v0.2 Instruction format Troubleshooting Limitations The Mistral AI Team Model Card for Mistral-7B-Instruct-v0.2 The Mistral-7B-Instruct-v0.2 Large Language Model (LLM) is an instruct fine-tuned version of the Mistral-7B-v0.2. Mistral-7B-v0.2 has the following changes compared to Mistral-7B-v0.1 32k context window (vs 8k context in v0.1) Rope-theta = 1e6 No Sliding-Window Attention For full details of this model please read our paper and release blog post. Instruction format In order to leverage instruction fine-tuning, your prompt should be surrounded by [INST] and [/INST] tokens. The very first instruction should begin with a begin of sentence id. The next instructions should not. The assistant generation will be ended by the end-of-sentence token id. E.g. text = \"[INST] What is your favourite condiment? [/INST]\" \"Well, I'm quite partial to a good squeeze of fresh lemon juice. It adds just the right amount of zesty flavour to whatever I'm cooking up in the kitchen! \" \"[INST] Do you have mayonnaise recipes? [/INST]\" This format is available as a chat template via the apply_chat_template() method: from transformers import AutoModelForCausalLM, AutoTokenizer device = \"cuda\" # the device to load the model onto model = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.2\") tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.2\") messages = [ {\"role\": \"user\", \"content\": \"What is your favourite condiment?\"}, {\"role\": \"assistant\", \"content\": \"Well, I'm quite partial to a good squeeze of fresh lemon juice. It adds just the right amount of zesty flavour to whatever I'm cooking up in the kitchen!\"}, {\"role\": \"user\", \"content\": \"Do you have mayonnaise recipes?\"} ] encodeds = tokenizer.apply_chat_template(messages, return_tensors=\"pt\") model_inputs = encodeds.to(device) model.to(device) generated_ids = model.generate(model_inputs, max_new_tokens=1000, do_sample=True) decoded = tokenizer.batch_decode(generated_ids) print(decoded[0]) Troubleshooting If you see the following error: Traceback (most recent call last): File \"\", line 1, in File \"/transformers/models/auto/auto_factory.py\", line 482, in from_pretrained config, kwargs = AutoConfig.from_pretrained( File \"/transformers/models/auto/configuration_auto.py\", line 1022, in from_pretrained config_class = CONFIG_MAPPING[config_dict[\"model_type\"]] File \"/transformers/models/auto/configuration_auto.py\", line 723, in getitem raise KeyError(key) KeyError: 'mistral' Installing transformers from source should solve the issue pip install git+https://github.com/huggingface/transformers This should not be required after transformers-v4.33.4. Limitations The Mistral 7B Instruct model is a quick demonstration that the base model can be easily fine-tuned to achieve compelling performance. It does not have any moderation mechanisms. We're looking forward to engaging with the community on ways to make the model finely respect guardrails, allowing for deployment in environments requiring moderated outputs. The Mistral AI Team Albert Jiang, Alexandre Sablayrolles, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, Gianna Lengyel, Guillaume Bour, Guillaume Lample, Lélio Renard Lavaud, Louis Ternon, Lucile Saulnier, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Théophile Gervet, Thibaut Lavril, Thomas Wang, Timothée Lacroix, William El Sayed. Downloads last month2,004,086 Safetensors Model size 7.24B params Tensor type BF16 · Spaces using mistralai/Mistral-7B-Instruct-v0.2 100 🌍 cvachet/pdf-chatbot🔍 bishmoy/Arxiv-CS-RAG🏆 eduagarcia/open_pt_llm_leaderboard🐐 shi-labs/CuMo-7b-zero🙋📃 chansung/paper_qa🐨 hysts/mistral-7b🐠🔬🧠 awacke1/GPT-4o-omni-text-audio-image-video🏆🇵🇱 speakleash/open_pl_llm_leaderboard📉 JournalistsonHF/ai-scraper🏆 Ateeqq/Mistral-7B-Instruct-v0.2-Chatbot📚 contextcite/context-cite☯️ Hansimov/hf-llm-api🧡 Tuana/hackernews-summaries📊 ngebodh/SimpleChatbot🏆🤖 meval/multilingual-chatbot-arena-leaderboard😻 Skier8402/mistral-super-fast🐑 🐑 gsarti/pecore🥇 lamini/leaderboard📚 MuntasirHossain/RAG-PDF-Chatbot🔍 awacke1/Arxiv-Paper-Search-And-QA-RAG-Pattern + 95 Spaces + 80 Spaces Company © Hugging Face TOS Privacy About Jobs Website Models Datasets Spaces Pricing Docs\n",
      "mistralai/Mixtral-8x7B-Instruct-v0.1 · Hugging Face Hugging Face Models Datasets Spaces Posts Docs Solutions Pricing Log In Sign Up mistralai / Mixtral-8x7B-Instruct-v0.1 like 3.86k Text Generation Transformers Safetensors 5 languages mixtral conversational Inference Endpoints text-generation-inference License: apache-2.0 Model card Files Files and versions Community 198 Train Deploy Use this model Edit model card You need to agree to share your contact information to access this model This repository is publicly accessible, but you have to accept the conditions to access its files and content. Log in or Sign Up to review the conditions and access this model content. Model Card for Mixtral-8x7B Warning Instruction format Run the model In half-precision Lower precision using (8-bit & 4-bit) using bitsandbytes Load the model with Flash Attention 2 Limitations The Mistral AI Team Model Card for Mixtral-8x7B The Mixtral-8x7B Large Language Model (LLM) is a pretrained generative Sparse Mixture of Experts. The Mixtral-8x7B outperforms Llama 2 70B on most benchmarks we tested. For full details of this model please read our release blog post. Warning This repo contains weights that are compatible with vLLM serving of the model as well as Hugging Face transformers library. It is based on the original Mixtral torrent release, but the file format and parameter names are different. Please note that model cannot (yet) be instantiated with HF. Instruction format This format must be strictly respected, otherwise the model will generate sub-optimal outputs. The template used to build a prompt for the Instruct model is defined as follows:  [INST] Instruction [/INST] Model answer [INST] Follow-up instruction [/INST] Note that  and  are special tokens for beginning of string (BOS) and end of string (EOS) while [INST] and [/INST] are regular strings. As reference, here is the pseudo-code used to tokenize instructions during fine-tuning: def tokenize(text): return tok.encode(text, add_special_tokens=False) [BOS_ID] + tokenize(\"[INST]\") + tokenize(USER_MESSAGE_1) + tokenize(\"[/INST]\") + tokenize(BOT_MESSAGE_1) + [EOS_ID] + … tokenize(\"[INST]\") + tokenize(USER_MESSAGE_N) + tokenize(\"[/INST]\") + tokenize(BOT_MESSAGE_N) + [EOS_ID] In the pseudo-code above, note that the tokenize method should not add a BOS or EOS token automatically, but should add a prefix space. In the Transformers library, one can use chat templates which make sure the right format is applied. Run the model from transformers import AutoModelForCausalLM, AutoTokenizer model_id = \"mistralai/Mixtral-8x7B-Instruct-v0.1\" tokenizer = AutoTokenizer.from_pretrained(model_id) model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"auto\") messages = [ {\"role\": \"user\", \"content\": \"What is your favourite condiment?\"}, {\"role\": \"assistant\", \"content\": \"Well, I'm quite partial to a good squeeze of fresh lemon juice. It adds just the right amount of zesty flavour to whatever I'm cooking up in the kitchen!\"}, {\"role\": \"user\", \"content\": \"Do you have mayonnaise recipes?\"} ] inputs = tokenizer.apply_chat_template(messages, return_tensors=\"pt\").to(\"cuda\") outputs = model.generate(inputs, max_new_tokens=20) print(tokenizer.decode(outputs[0], skip_special_tokens=True)) By default, transformers will load the model in full precision. Therefore you might be interested to further reduce down the memory requirements to run the model through the optimizations we offer in HF ecosystem: In half-precision Note float16 precision only works on GPU devices Click to expand + import torch from transformers import AutoModelForCausalLM, AutoTokenizer model_id = \"mistralai/Mixtral-8x7B-Instruct-v0.1\" tokenizer = AutoTokenizer.from_pretrained(model_id) + model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.float16, device_map=\"auto\") messages = [ {\"role\": \"user\", \"content\": \"What is your favourite condiment?\"}, {\"role\": \"assistant\", \"content\": \"Well, I'm quite partial to a good squeeze of fresh lemon juice. It adds just the right amount of zesty flavour to whatever I'm cooking up in the kitchen!\"}, {\"role\": \"user\", \"content\": \"Do you have mayonnaise recipes?\"} ] input_ids = tokenizer.apply_chat_template(messages, return_tensors=\"pt\").to(\"cuda\") outputs = model.generate(input_ids, max_new_tokens=20) print(tokenizer.decode(outputs[0], skip_special_tokens=True)) Lower precision using (8-bit & 4-bit) using bitsandbytes Click to expand + import torch from transformers import AutoModelForCausalLM, AutoTokenizer model_id = \"mistralai/Mixtral-8x7B-Instruct-v0.1\" tokenizer = AutoTokenizer.from_pretrained(model_id) + model = AutoModelForCausalLM.from_pretrained(model_id, load_in_4bit=True, device_map=\"auto\") text = \"Hello my name is\" messages = [ {\"role\": \"user\", \"content\": \"What is your favourite condiment?\"}, {\"role\": \"assistant\", \"content\": \"Well, I'm quite partial to a good squeeze of fresh lemon juice. It adds just the right amount of zesty flavour to whatever I'm cooking up in the kitchen!\"}, {\"role\": \"user\", \"content\": \"Do you have mayonnaise recipes?\"} ] input_ids = tokenizer.apply_chat_template(messages, return_tensors=\"pt\").to(\"cuda\") outputs = model.generate(input_ids, max_new_tokens=20) print(tokenizer.decode(outputs[0], skip_special_tokens=True)) Load the model with Flash Attention 2 Click to expand + import torch from transformers import AutoModelForCausalLM, AutoTokenizer model_id = \"mistralai/Mixtral-8x7B-Instruct-v0.1\" tokenizer = AutoTokenizer.from_pretrained(model_id) + model = AutoModelForCausalLM.from_pretrained(model_id, use_flash_attention_2=True, device_map=\"auto\") messages = [ {\"role\": \"user\", \"content\": \"What is your favourite condiment?\"}, {\"role\": \"assistant\", \"content\": \"Well, I'm quite partial to a good squeeze of fresh lemon juice. It adds just the right amount of zesty flavour to whatever I'm cooking up in the kitchen!\"}, {\"role\": \"user\", \"content\": \"Do you have mayonnaise recipes?\"} ] input_ids = tokenizer.apply_chat_template(messages, return_tensors=\"pt\").to(\"cuda\") outputs = model.generate(input_ids, max_new_tokens=20) print(tokenizer.decode(outputs[0], skip_special_tokens=True)) Limitations The Mixtral-8x7B Instruct model is a quick demonstration that the base model can be easily fine-tuned to achieve compelling performance. It does not have any moderation mechanisms. We're looking forward to engaging with the community on ways to make the model finely respect guardrails, allowing for deployment in environments requiring moderated outputs. The Mistral AI Team Albert Jiang, Alexandre Sablayrolles, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, Gianna Lengyel, Guillaume Bour, Guillaume Lample, Lélio Renard Lavaud, Louis Ternon, Lucile Saulnier, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Théophile Gervet, Thibaut Lavril, Thomas Wang, Timothée Lacroix, William El Sayed. Downloads last month493,551 Safetensors Model size 46.7B params Tensor type BF16 · Spaces using mistralai/Mixtral-8x7B-Instruct-v0.1 100 🔥 KingNish/OpenGPT-4o🚀 Vokturz/can-it-run-llm🦊 ehristoforu/mixtral-46.7b-chat🎺 fffiloni/image-to-music-v2🔥 KingNish/JARVIS🌍 cvachet/pdf-chatbot🔍 bishmoy/Arxiv-CS-RAG🏆 eduagarcia/open_pt_llm_leaderboard🐙 Tomoniai/Mixtral-Chat🙋📃 chansung/paper_qa⚔️ lighthouzai/guardrails-arena🌖 reach-vb/musicgen-prompt-upsampling🐠🔬🧠 awacke1/GPT-4o-omni-text-audio-image-video🎞️🎺 fffiloni/video-to-music🏆 Ateeqq/Mistral-7B-Instruct-v0.2-Chatbot😎 KingNish/Image-Gen-Pro🦀 Skier8402/mistral-PDF-chat🏃 Omnibus/Chatbot-Compare😻 narra-ai/emoji-translator🏢 ulab-ai/ArxivCopilot + 95 Spaces + 80 Spaces Company © Hugging Face TOS Privacy About Jobs Website Models Datasets Spaces Pricing Docs\n",
      "TheBloke/Mixtral-8x7B-v0.1-GPTQ · Hugging Face Hugging Face Models Datasets Spaces Posts Docs Solutions Pricing Log In Sign Up TheBloke / Mixtral-8x7B-v0.1-GPTQ like 125 Text Generation Transformers Safetensors 5 languages mixtral text-generation-inference 4-bit precision License: apache-2.0 Model card Files Files and versions Community 10 Train Deploy Use this model Edit model card Mixtral 8X7B v0.1 - GPTQ Description Repositories available Prompt template: None Known compatible clients / servers Provided files, and GPTQ parameters How to download, including from branches In text-generation-webui From the command line With git (not recommended) How to easily download and use this model in text-generation-webui Serving this model from Text Generation Inference (TGI) Python code example: inference from this GPTQ model Install the necessary packages Example Python code Compatibility Discord Thanks, and how to contribute Original model card: Mistral AI_'s Mixtral 8X7B v0.1 Model Card for Mixtral-8x7B Warning Run the model In half-precision Lower precision using (8-bit & 4-bit) using bitsandbytes Load the model with Flash Attention 2 Notice The Mistral AI Team Chat & support: TheBloke's Discord server Want to contribute? TheBloke's Patreon page TheBloke's LLM work is generously supported by a grant from andreessen horowitz (a16z) Mixtral 8X7B v0.1 - GPTQ Model creator: Mistral AI_ Original model: Mixtral 8X7B v0.1 Description This repo contains GPTQ model files for Mistral AI_'s Mixtral 8X7B v0.1. Mixtral GPTQs currently require: Transformers 4.36.0 or later either, AutoGPTQ 0.6 compiled from source, or Transformers 4.37.0.dev0 compiled from Github with: pip3 install git+https://github.com/huggingface/transformers Multiple GPTQ parameter permutations are provided; see Provided Files below for details of the options provided, their parameters, and the software used to create them. Repositories available AWQ model(s) for GPU inference. GPTQ models for GPU inference, with multiple quantisation parameter options. 2, 3, 4, 5, 6 and 8-bit GGUF models for CPU+GPU inference Mistral AI_'s original unquantised fp16 model in pytorch format, for GPU inference and for further conversions Prompt template: None {prompt} Known compatible clients / servers GPTQ models are currently supported on Linux (NVidia/AMD) and Windows (NVidia only). macOS users: please use GGUF models. Mixtral GPTQs currently have special requirements - see Description above. Provided files, and GPTQ parameters Multiple quantisation parameters are provided, to allow you to choose the best one for your hardware and requirements. Each separate quant is in a different branch. See below for instructions on fetching from different branches. Most GPTQ files are made with AutoGPTQ. Mistral models are currently made with Transformers. Explanation of GPTQ parameters Bits: The bit size of the quantised model. GS: GPTQ group size. Higher numbers use less VRAM, but have lower quantisation accuracy. \"None\" is the lowest possible value. Act Order: True or False. Also known as desc_act. True results in better quantisation accuracy. Some GPTQ clients have had issues with models that use Act Order plus Group Size, but this is generally resolved now. Damp %: A GPTQ parameter that affects how samples are processed for quantisation. 0.01 is default, but 0.1 results in slightly better accuracy. GPTQ dataset: The calibration dataset used during quantisation. Using a dataset more appropriate to the model's training can improve quantisation accuracy. Note that the GPTQ calibration dataset is not the same as the dataset used to train the model - please refer to the original model repo for details of the training dataset(s). Sequence Length: The length of the dataset sequences used for quantisation. Ideally this is the same as the model sequence length. For some very long sequence models (16+K), a lower sequence length may have to be used. Note that a lower sequence length does not limit the sequence length of the quantised model. It only impacts the quantisation accuracy on longer inference sequences. ExLlama Compatibility: Whether this file can be loaded with ExLlama, which currently only supports Llama and Mistral models in 4-bit. Branch Bits GS Act Order Damp % GPTQ Dataset Seq Len Size ExLlama Desc main 4 None Yes 0.1 VMware Open Instruct 4096 23.81 GB No 4-bit, with Act Order. No group size, to lower VRAM requirements. gptq-4bit-128g-actorder_True 4 128 Yes 0.1 VMware Open Instruct 4096 24.70 GB No 4-bit, with Act Order and group size 128g. Uses even less VRAM than 64g, but with slightly lower accuracy. gptq-4bit-32g-actorder_True 4 32 Yes 0.1 VMware Open Instruct 4096 27.42 GB No 4-bit, with Act Order and group size 32g. Gives highest possible inference quality, with maximum VRAM usage. gptq-3bit--1g-actorder_True 3 None Yes 0.1 VMware Open Instruct 4096 18.01 GB No 3-bit, with Act Order and no group size. Lowest possible VRAM requirements. May be lower quality than 3-bit 128g. gptq-3bit-128g-actorder_True 3 128 Yes 0.1 VMware Open Instruct 4096 18.85 GB No 3-bit, with group size 128g and act-order. Higher quality than 128g-False. gptq-8bit--1g-actorder_True 8 None Yes 0.1 VMware Open Instruct 4096 47.04 GB No 8-bit, with Act Order. No group size, to lower VRAM requirements. gptq-8bit-128g-actorder_True 8 128 Yes 0.1 VMware Open Instruct 4096 48.10 GB No 8-bit, with group size 128g for higher inference quality and with Act Order for even higher accuracy. How to download, including from branches In text-generation-webui To download from the main branch, enter TheBloke/Mixtral-8x7B-v0.1-GPTQ in the \"Download model\" box. To download from another branch, add :branchname to the end of the download name, eg TheBloke/Mixtral-8x7B-v0.1-GPTQ:gptq-4bit-128g-actorder_True From the command line I recommend using the huggingface-hub Python library: pip3 install huggingface-hub To download the main branch to a folder called Mixtral-8x7B-v0.1-GPTQ: mkdir Mixtral-8x7B-v0.1-GPTQ huggingface-cli download TheBloke/Mixtral-8x7B-v0.1-GPTQ --local-dir Mixtral-8x7B-v0.1-GPTQ --local-dir-use-symlinks False To download from a different branch, add the --revision parameter: mkdir Mixtral-8x7B-v0.1-GPTQ huggingface-cli download TheBloke/Mixtral-8x7B-v0.1-GPTQ --revision gptq-4bit-128g-actorder_True --local-dir Mixtral-8x7B-v0.1-GPTQ --local-dir-use-symlinks False More advanced huggingface-cli download usage If you remove the --local-dir-use-symlinks False parameter, the files will instead be stored in the central Hugging Face cache directory (default location on Linux is: ~/.cache/huggingface), and symlinks will be added to the specified --local-dir, pointing to their real location in the cache. This allows for interrupted downloads to be resumed, and allows you to quickly clone the repo to multiple places on disk without triggering a download again. The downside, and the reason why I don't list that as the default option, is that the files are then hidden away in a cache folder and it's harder to know where your disk space is being used, and to clear it up if/when you want to remove a download model. The cache location can be changed with the HF_HOME environment variable, and/or the --cache-dir parameter to huggingface-cli. For more documentation on downloading with huggingface-cli, please see: HF -> Hub Python Library -> Download files -> Download from the CLI. To accelerate downloads on fast connections (1Gbit/s or higher), install hf_transfer: pip3 install hf_transfer And set environment variable HF_HUB_ENABLE_HF_TRANSFER to 1: mkdir Mixtral-8x7B-v0.1-GPTQ HF_HUB_ENABLE_HF_TRANSFER=1 huggingface-cli download TheBloke/Mixtral-8x7B-v0.1-GPTQ --local-dir Mixtral-8x7B-v0.1-GPTQ --local-dir-use-symlinks False Windows Command Line users: You can set the environment variable by running set HF_HUB_ENABLE_HF_TRANSFER=1 before the download command. With git (not recommended) To clone a specific branch with git, use a command like this: git clone --single-branch --branch gptq-4bit-128g-actorder_True https://huggingface.co/TheBloke/Mixtral-8x7B-v0.1-GPTQ Note that using Git with HF repos is strongly discouraged. It will be much slower than using huggingface-hub, and will use twice as much disk space as it has to store the model files twice (it stores every byte both in the intended target folder, and again in the .git folder as a blob.) How to easily download and use this model in text-generation-webui NOTE: Requires: Transformers 4.36.0, or Transformers 4.37.0.dev0 from Github Either AutoGPTQ 0.6 compiled from source and Loader: AutoGPTQ, or, Loader: Transformers, if you installed Transformers from Github: pip3 install git+https://github.com/huggingface/transformers Please make sure you're using the latest version of text-generation-webui. It is strongly recommended to use the text-generation-webui one-click-installers unless you're sure you know how to make a manual install. Click the Model tab. Under Download custom model or LoRA, enter TheBloke/Mixtral-8x7B-v0.1-GPTQ. To download from a specific branch, enter for example TheBloke/Mixtral-8x7B-v0.1-GPTQ:gptq-4bit-128g-actorder_True see Provided Files above for the list of branches for each option. Click Download. The model will start downloading. Once it's finished it will say \"Done\". In the top left, click the refresh icon next to Model. In the Model dropdown, choose the model you just downloaded: Mixtral-8x7B-v0.1-GPTQ The model will automatically load, and is now ready for use! If you want any custom settings, set them and then click Save settings for this model followed by Reload the Model in the top right. Note that you do not need to and should not set manual GPTQ parameters any more. These are set automatically from the file quantize_config.json. Once you're ready, click the Text Generation tab and enter a prompt to get started! Serving this model from Text Generation Inference (TGI) Not currently supported for Mixtral models. Python code example: inference from this GPTQ model Install the necessary packages Requires: Transformers 4.37.0.dev0 from Github, Optimum 1.16.0 or later, and AutoGPTQ 0.5.1 or later. pip3 install --upgrade \"git+https://github.com/huggingface/transformers\" optimum # If using PyTorch 2.1 + CUDA 12.x: pip3 install --upgrade auto-gptq # or, if using PyTorch 2.1 + CUDA 11.x: pip3 install --upgrade auto-gptq --extra-index-url https://huggingface.github.io/autogptq-index/whl/cu118/ If you are using PyTorch 2.0, you will need to install AutoGPTQ from source. Likewise if you have problems with the pre-built wheels, you should try building from source: pip3 uninstall -y auto-gptq git clone https://github.com/PanQiWei/AutoGPTQ cd AutoGPTQ DISABLE_QIGEN=1 pip3 install . Example Python code from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline model_name_or_path = \"TheBloke/Mixtral-8x7B-v0.1-GPTQ\" # To use a different branch, change revision # For example: revision=\"gptq-4bit-128g-actorder_True\" model = AutoModelForCausalLM.from_pretrained(model_name_or_path, device_map=\"auto\", trust_remote_code=False, revision=\"main\") tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True) prompt = \"Write a story about llamas\" system_message = \"You are a story writing assistant\" prompt_template=f'''{prompt} ''' print(\"\\n\\n*** Generate:\") input_ids = tokenizer(prompt_template, return_tensors='pt').input_ids.cuda() output = model.generate(inputs=input_ids, temperature=0.7, do_sample=True, top_p=0.95, top_k=40, max_new_tokens=512) print(tokenizer.decode(output[0])) # Inference can also be done using transformers' pipeline print(\"*** Pipeline:\") pipe = pipeline( \"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=512, do_sample=True, temperature=0.7, top_p=0.95, top_k=40, repetition_penalty=1.1 ) print(pipe(prompt_template)[0]['generated_text']) Compatibility The files provided are tested to work with AutoGPTQ 0.6 (compiled from source) and Transformers 4.37.0 (installed from Github). Discord For further support, and discussions on these models and AI in general, join us at: TheBloke AI's Discord server Thanks, and how to contribute Thanks to the chirper.ai team! Thanks to Clay from gpus.llm-utils.org! I've had a lot of people ask if they can contribute. I enjoy providing models and helping people, and would love to be able to spend even more time doing it, as well as expanding into new projects like fine tuning/training. If you're able and willing to contribute it will be most gratefully received and will help me to keep providing more models, and to start work on new AI projects. Donaters will get priority support on any and all AI/LLM/model questions and requests, access to a private Discord room, plus other benefits. Patreon: https://patreon.com/TheBlokeAI Ko-Fi: https://ko-fi.com/TheBlokeAI Special thanks to: Aemon Algiz. Patreon special mentions: Michael Levine, 阿明, Trailburnt, Nikolai Manek, John Detwiler, Randy H, Will Dee, Sebastain Graf, NimbleBox.ai, Eugene Pentland, Emad Mostaque, Ai Maven, Jim Angel, Jeff Scroggin, Michael Davis, Manuel Alberto Morcote, Stephen Murray, Robert, Justin Joy, Luke @flexchar, Brandon Frisco, Elijah Stavena, S_X, Dan Guido, Undi ., Komninos Chatzipapas, Shadi, theTransient, Lone Striker, Raven Klaugh, jjj, Cap'n Zoog, Michel-Marie MAUDET (LINAGORA), Matthew Berman, David, Fen Risland, Omer Bin Jawed, Luke Pendergrass, Kalila, OG, Erik Bjäreholt, Rooh Singh, Joseph William Delisle, Dan Lewis, TL, John Villwock, AzureBlack, Brad, Pedro Madruga, Caitlyn Gatomon, K, jinyuan sun, Mano Prime, Alex, Jeffrey Morgan, Alicia Loh, Illia Dulskyi, Chadd, transmissions 11, fincy, Rainer Wilmers, ReadyPlayerEmma, knownsqashed, Mandus, biorpg, Deo Leter, Brandon Phillips, SuperWojo, Sean Connelly, Iucharbius, Jack West, Harry Royden McLaughlin, Nicholas, terasurfer, Vitor Caleffi, Duane Dunston, Johann-Peter Hartmann, David Ziegler, Olakabola, Ken Nordquist, Trenton Dambrowitz, Tom X Nguyen, Vadim, Ajan Kanaga, Leonard Tan, Clay Pascal, Alexandros Triantafyllidis, JM33133, Xule, vamX, ya boyyy, subjectnull, Talal Aujan, Alps Aficionado, wassieverse, Ari Malik, James Bentley, Woland, Spencer Kim, Michael Dempsey, Fred von Graf, Elle, zynix, William Richards, Stanislav Ovsiannikov, Edmond Seymore, Jonathan Leane, Martin Kemka, usrbinkat, Enrico Ros Thank you to all my generous patrons and donaters! And thank you again to a16z for their generous grant. Original model card: Mistral AI_'s Mixtral 8X7B v0.1 Model Card for Mixtral-8x7B The Mixtral-8x7B Large Language Model (LLM) is a pretrained generative Sparse Mixture of Experts. The Mistral-8x7B outperforms Llama 2 70B on most benchmarks we tested. For full details of this model please read our release blog post. Warning This repo contains weights that are compatible with vLLM serving of the model as well as Hugging Face transformers library. It is based on the original Mixtral torrent release, but the file format and parameter names are different. Please note that model cannot (yet) be instantiated with HF. Run the model from transformers import AutoModelForCausalLM, AutoTokenizer model_id = \"mistralai/Mixtral-8x7B-v0.1\" tokenizer = AutoTokenizer.from_pretrained(model_id) model = AutoModelForCausalLM.from_pretrained(model_id) text = \"Hello my name is\" inputs = tokenizer(text, return_tensors=\"pt\") outputs = model.generate(**inputs, max_new_tokens=20) print(tokenizer.decode(outputs[0], skip_special_tokens=True)) By default, transformers will load the model in full precision. Therefore you might be interested to further reduce down the memory requirements to run the model through the optimizations we offer in HF ecosystem: In half-precision Note float16 precision only works on GPU devices Click to expand + import torch from transformers import AutoModelForCausalLM, AutoTokenizer model_id = \"mistralai/Mixtral-8x7B-v0.1\" tokenizer = AutoTokenizer.from_pretrained(model_id) + model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.float16).to(0) text = \"Hello my name is\" + inputs = tokenizer(text, return_tensors=\"pt\").to(0) outputs = model.generate(**inputs, max_new_tokens=20) print(tokenizer.decode(outputs[0], skip_special_tokens=True)) Lower precision using (8-bit & 4-bit) using bitsandbytes Click to expand + import torch from transformers import AutoModelForCausalLM, AutoTokenizer model_id = \"mistralai/Mixtral-8x7B-v0.1\" tokenizer = AutoTokenizer.from_pretrained(model_id) + model = AutoModelForCausalLM.from_pretrained(model_id, load_in_4bit=True) text = \"Hello my name is\" + inputs = tokenizer(text, return_tensors=\"pt\").to(0) outputs = model.generate(**inputs, max_new_tokens=20) print(tokenizer.decode(outputs[0], skip_special_tokens=True)) Load the model with Flash Attention 2 Click to expand + import torch from transformers import AutoModelForCausalLM, AutoTokenizer model_id = \"mistralai/Mixtral-8x7B-v0.1\" tokenizer = AutoTokenizer.from_pretrained(model_id) + model = AutoModelForCausalLM.from_pretrained(model_id, use_flash_attention_2=True) text = \"Hello my name is\" + inputs = tokenizer(text, return_tensors=\"pt\").to(0) outputs = model.generate(**inputs, max_new_tokens=20) print(tokenizer.decode(outputs[0], skip_special_tokens=True)) Notice Mixtral-8x7B is a pretrained base model and therefore does not have any moderation mechanisms. The Mistral AI Team Albert Jiang, Alexandre Sablayrolles, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, Gianna Lengyel, Guillaume Bour, Guillaume Lample, Lélio Renard Lavaud, Louis Ternon, Lucile Saulnier, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Théophile Gervet, Thibaut Lavril, Thomas Wang, Timothée Lacroix, William El Sayed. Downloads last month23,633 Safetensors Model size 6.09B params Tensor type I32 ·BF16 ·FP16 · Inference Examples Text Generation Inference API (serverless) has been turned off for this model. Maximize Quantized from mistralai/Mixtral-8x7B-v0.1 Space using TheBloke/Mixtral-8x7B-v0.1-GPTQ 1 😻 anna-tch/docker_space Company © Hugging Face TOS Privacy About Jobs Website Models Datasets Spaces Pricing Docs\n",
      "intfloat/multilingual-e5-large · Hugging Face Hugging Face Models Datasets Spaces Posts Docs Solutions Pricing Log In Sign Up intfloat / multilingual-e5-large like 598 Feature Extraction sentence-transformers PyTorch ONNX Safetensors 94 languages xlm-roberta mteb Sentence Transformers sentence-similarity Eval Results Inference Endpoints 4 papers License: mit Model card Files Files and versions Community 39 Deploy Use this model Edit model card Multilingual-E5-large Usage Supported Languages Training Details Benchmark Results on Mr. TyDi MTEB Benchmark Evaluation Support for Sentence Transformers FAQ Citation Limitations Multilingual-E5-large Multilingual E5 Text Embeddings: A Technical Report. Liang Wang, Nan Yang, Xiaolong Huang, Linjun Yang, Rangan Majumder, Furu Wei, arXiv 2024 This model has 24 layers and the embedding size is 1024. Usage Below is an example to encode queries and passages from the MS-MARCO passage ranking dataset. import torch.nn.functional as F from torch import Tensor from transformers import AutoTokenizer, AutoModel def average_pool(last_hidden_states: Tensor, attention_mask: Tensor) -> Tensor: last_hidden = last_hidden_states.masked_fill(~attention_mask[..., None].bool(), 0.0) return last_hidden.sum(dim=1) / attention_mask.sum(dim=1)[..., None] # Each input text should start with \"query: \" or \"passage: \", even for non-English texts. # For tasks other than retrieval, you can simply use the \"query: \" prefix. input_texts = ['query: how much protein should a female eat', 'query: 南瓜的家常做法', \"passage: As a general guideline, the CDC's average requirement of protein for women ages 19 to 70 is 46 grams per day. But, as you can see from this chart, you'll need to increase that if you're expecting or training for a marathon. Check out the chart below to see how much protein you should be eating each day.\", \"passage: 1.清炒南瓜丝 原料:嫩南瓜半个 调料:葱、盐、白糖、鸡精 做法: 1、南瓜用刀薄薄的削去表面一层皮,用勺子刮去瓤 2、擦成细丝(没有擦菜板就用刀慢慢切成细丝) 3、锅烧热放油,入葱花煸出香味 4、入南瓜丝快速翻炒一分钟左右,放盐、一点白糖和鸡精调味出锅 2.香葱炒南瓜 原料:南瓜1只 调料:香葱、蒜末、橄榄油、盐 做法: 1、将南瓜去皮,切成片 2、油锅8成热后,将蒜末放入爆香 3、爆香后,将南瓜片放入,翻炒 4、在翻炒的同时,可以不时地往锅里加水,但不要太多 5、放入盐,炒匀 6、南瓜差不多软和绵了之后,就可以关火 7、撒入香葱,即可出锅\"] tokenizer = AutoTokenizer.from_pretrained('intfloat/multilingual-e5-large') model = AutoModel.from_pretrained('intfloat/multilingual-e5-large') # Tokenize the input texts batch_dict = tokenizer(input_texts, max_length=512, padding=True, truncation=True, return_tensors='pt') outputs = model(**batch_dict) embeddings = average_pool(outputs.last_hidden_state, batch_dict['attention_mask']) # normalize embeddings embeddings = F.normalize(embeddings, p=2, dim=1) scores = (embeddings[:2] @ embeddings[2:].T) * 100 print(scores.tolist()) Supported Languages This model is initialized from xlm-roberta-large and continually trained on a mixture of multilingual datasets. It supports 100 languages from xlm-roberta, but low-resource languages may see performance degradation. Training Details Initialization: xlm-roberta-large First stage: contrastive pre-training with weak supervision Dataset Weak supervision # of text pairs Filtered mC4 (title, page content) 1B CC News (title, news content) 400M NLLB translation pairs 2.4B Wikipedia (hierarchical section title, passage) 150M Filtered Reddit (comment, response) 800M S2ORC (title, abstract) and citation pairs 100M Stackexchange (question, answer) 50M xP3 (input prompt, response) 80M Miscellaneous unsupervised SBERT data - 10M Second stage: supervised fine-tuning Dataset Language # of text pairs MS MARCO English 500k NQ English 70k Trivia QA English 60k NLI from SimCSE English <300k ELI5 English 500k DuReader Retrieval Chinese 86k KILT Fever English 70k KILT HotpotQA English 70k SQuAD English 87k Quora English 150k Mr. TyDi 11 languages 50k MIRACL 16 languages 40k For all labeled datasets, we only use its training set for fine-tuning. For other training details, please refer to our paper at https://arxiv.org/pdf/2402.05672. Benchmark Results on Mr. TyDi Model Avg MRR@10 ar bn en fi id ja ko ru sw te th BM25 33.3 36.7 41.3 15.1 28.8 38.2 21.7 28.1 32.9 39.6 42.4 41.7 mDPR 16.7 26.0 25.8 16.2 11.3 14.6 18.1 21.9 18.5 7.3 10.6 13.5 BM25 + mDPR 41.7 49.1 53.5 28.4 36.5 45.5 35.5 36.2 42.7 40.5 42.0 49.2 multilingual-e5-small 64.4 71.5 66.3 54.5 57.7 63.2 55.4 54.3 60.8 65.4 89.1 70.1 multilingual-e5-base 65.9 72.3 65.0 58.5 60.8 64.9 56.6 55.8 62.7 69.0 86.6 72.7 multilingual-e5-large 70.5 77.5 73.2 60.8 66.8 68.5 62.5 61.6 65.8 72.7 90.2 76.2 MTEB Benchmark Evaluation Check out unilm/e5 to reproduce evaluation results on the BEIR and MTEB benchmark. Support for Sentence Transformers Below is an example for usage with sentence_transformers. from sentence_transformers import SentenceTransformer model = SentenceTransformer('intfloat/multilingual-e5-large') input_texts = [ 'query: how much protein should a female eat', 'query: 南瓜的家常做法', \"passage: As a general guideline, the CDC's average requirement of protein for women ages 19 to 70 i s 46 grams per day. But, as you can see from this chart, you'll need to increase that if you're expecting or traini ng for a marathon. Check out the chart below to see how much protein you should be eating each day.\", \"passage: 1.清炒南瓜丝 原料:嫩南瓜半个 调料:葱、盐、白糖、鸡精 做法: 1、南瓜用刀薄薄的削去表面一层皮 ,用勺子刮去瓤 2、擦成细丝(没有擦菜板就用刀慢慢切成细丝) 3、锅烧热放油,入葱花煸出香味 4、入南瓜丝快速翻炒一分钟左右, 放盐、一点白糖和鸡精调味出锅 2.香葱炒南瓜 原料:南瓜1只 调料:香葱、蒜末、橄榄油、盐 做法: 1、将南瓜去皮,切成片 2、油 锅8成热后,将蒜末放入爆香 3、爆香后,将南瓜片放入,翻炒 4、在翻炒的同时,可以不时地往锅里加水,但不要太多 5、放入盐,炒匀 6、南瓜差不多软和绵了之后,就可以关火 7、撒入香葱,即可出锅\" ] embeddings = model.encode(input_texts, normalize_embeddings=True) Package requirements pip install sentence_transformers~=2.2.2 Contributors: michaelfeil FAQ 1. Do I need to add the prefix \"query: \" and \"passage: \" to input texts? Yes, this is how the model is trained, otherwise you will see a performance degradation. Here are some rules of thumb: Use \"query: \" and \"passage: \" correspondingly for asymmetric tasks such as passage retrieval in open QA, ad-hoc information retrieval. Use \"query: \" prefix for symmetric tasks such as semantic similarity, bitext mining, paraphrase retrieval. Use \"query: \" prefix if you want to use embeddings as features, such as linear probing classification, clustering. 2. Why are my reproduced results slightly different from reported in the model card? Different versions of transformers and pytorch could cause negligible but non-zero performance differences. 3. Why does the cosine similarity scores distribute around 0.7 to 1.0? This is a known and expected behavior as we use a low temperature 0.01 for InfoNCE contrastive loss. For text embedding tasks like text retrieval or semantic similarity, what matters is the relative order of the scores instead of the absolute values, so this should not be an issue. Citation If you find our paper or models helpful, please consider cite as follows: @article{wang2024multilingual, title={Multilingual E5 Text Embeddings: A Technical Report}, author={Wang, Liang and Yang, Nan and Huang, Xiaolong and Yang, Linjun and Majumder, Rangan and Wei, Furu}, journal={arXiv preprint arXiv:2402.05672}, year={2024} } Limitations Long texts will be truncated to at most 512 tokens. Downloads last month792,428 Safetensors Model size 560M params Tensor type I64 ·F32 · Spaces using intfloat/multilingual-e5-large 80 ❤️ nt3awnou/Nt3awnou-rescue-map🌖 jingwora/language-sentence-similarity💬 valeriylo/sample_rag🔥 hesha/text-embeddings-transformers🐠 Recordly/ChatGPT-RAG-Chromadb-testUI😻 hotchpotch/wikipedia-japanese-rag-qa🚀 sivan22/Halacha-semantic-search🐠 shimizukawa/python-no-senpai🐠 DrishtiSharma/chat_w_pdf🦀 rishi1985/text-embeding-3-a🦀 prlabs2023/text-embeding-18-a🏃 cawacci/chatwithdocuments🔥 JDWebProgrammer/text-embeddings-transformers🏃 joaopaulopresa/workshop_llm_ufg_chatbot🏃 pngwn/df_scroll_bug_repo🏃 pngwn/df_scroll_bug_fix🚀 hvassard/virbac_chatbot💻 hiwei/rag_demo🏃 hisabcloud/intfloat-multilingual-e5-large🚀 miniondenis/Doc_eater + 75 Spaces + 60 Spaces Evaluation results accuracy on MTEB AmazonCounterfactualClassification (en) test set self-reported 79.060 ap on MTEB AmazonCounterfactualClassification (en) test set self-reported 43.487 f1 on MTEB AmazonCounterfactualClassification (en) test set self-reported 73.327 accuracy on MTEB AmazonCounterfactualClassification (de) test set self-reported 71.221 ap on MTEB AmazonCounterfactualClassification (de) test set self-reported 81.558 f1 on MTEB AmazonCounterfactualClassification (de) test set self-reported 69.283 accuracy on MTEB AmazonCounterfactualClassification (en-ext) test set self-reported 80.420 ap on MTEB AmazonCounterfactualClassification (en-ext) test set self-reported 29.349 f1 on MTEB AmazonCounterfactualClassification (en-ext) test set self-reported 67.625 accuracy on MTEB AmazonCounterfactualClassification (ja) test set self-reported 77.837 ap on MTEB AmazonCounterfactualClassification (ja) test set self-reported 26.558 f1 on MTEB AmazonCounterfactualClassification (ja) test set self-reported 64.966 accuracy on MTEB AmazonPolarityClassification test set self-reported 93.490 ap on MTEB AmazonPolarityClassification test set self-reported 90.988 f1 on MTEB AmazonPolarityClassification test set self-reported 93.486 accuracy on MTEB AmazonReviewsClassification (en) test set self-reported 47.564 f1 on MTEB AmazonReviewsClassification (en) test set self-reported 46.751 accuracy on MTEB AmazonReviewsClassification (de) test set self-reported 45.400 f1 on MTEB AmazonReviewsClassification (de) test set self-reported 44.172 accuracy on MTEB AmazonReviewsClassification (es) test set self-reported 43.068 f1 on MTEB AmazonReviewsClassification (es) test set self-reported 42.382 accuracy on MTEB AmazonReviewsClassification (fr) test set self-reported 41.890 f1 on MTEB AmazonReviewsClassification (fr) test set self-reported 40.844 accuracy on MTEB AmazonReviewsClassification (ja) test set self-reported 40.120 f1 on MTEB AmazonReviewsClassification (ja) test set self-reported 39.523 Expand 317 evaluations View on Papers With Code Company © Hugging Face TOS Privacy About Jobs Website Models Datasets Spaces Pricing Docs\n",
      "prometheus-eval/prometheus-13b-v1.0 · Hugging Face Hugging Face Models Datasets Spaces Posts Docs Solutions Pricing Log In Sign Up prometheus-eval / prometheus-13b-v1.0 like 116 Text2Text Generation Transformers PyTorch kaist-ai/Feedback-Collection English llama text-generation Inference Endpoints text-generation-inference arxiv: 2310.08491 License: apache-2.0 Model card Files Files and versions Community 2 Train Deploy Use this model Edit model card TL;DR Model Details Model Description Prompt Format License Usage Using the Pytorch model Running the model on a CPU Running the model on a GPU Running the model on a GPU using different precisions Citation Links for Reference Homepage:https://github.com/kaistAI/Prometheus Repository:https://github.com/kaistAI/Prometheus Paper:https://arxiv.org/abs/2310.08491 Point of Contact:seungone@kaist.ac.kr TL;DR Prometheus is an alternative of GPT-4 evaluation when doing fine-grained evaluation of an underlying LLM & a Reward model for Reinforcement Learning from Human Feedback (RLHF). Prometheus is a language model using Llama-2-Chat as a base model and fine-tuned on 100K feedback within the Feedback Collection. Since it was fine-tuned on a large amount of feedback, it is specialized at evaluating long-form responses, outperforming GPT-3.5-Turbo, Llama-2-Chat 70B, and on par with GPT-4 on various benchmarks. Most importantly, this was possible since we appended 2 reference materials (reference answer, and customized score rubric). Prometheus is a cheap and powerful alternative to GPT-4 evaluation, which one could use to evaluate LLMs with customized criteria (e.g., Child readability, Cultural Sensitivity, Creativity). Also, it could be used as a reward model for Reinforcement Learning from Human Feedback (RLHF). Model Details Model Description Model type: Language model Language(s) (NLP): English License: Apache 2.0 Related Models: All Prometheus Checkpoints Resources for more information: Research paper GitHub Repo Prometheus is trained with two different sizes (7B and 13B). You could check the 7B sized LM on this page. Also, check out our dataset as well on this page. Prompt Format Prometheus requires 4 components in the input: An instruction, a response to evaluate, a score rubric, and a reference answer. You could refer to the prompt format below. You should fill in the instruction, response, reference answer, criteria description, and score description for score in range of 1 to 5. ###Task Description: An instruction (might include an Input inside it), a response to evaluate, a reference answer that gets a score of 5, and a score rubric representing a evaluation criteria are given. 1. Write a detailed feedback that assess the quality of the response strictly based on the given score rubric, not evaluating in general. 2. After writing a feedback, write a score that is an integer between 1 and 5. You should refer to the score rubric. 3. The output format should look as follows: \\\"Feedback: (write a feedback for criteria) [RESULT] (an integer number between 1 and 5)\\\" 4. Please do not generate any other opening, closing, and explanations. ###The instruction to evaluate: {instruction} ###Response to evaluate: {response} ###Reference Answer (Score 5): {reference_answer} ###Score Rubrics: [{criteria_description}] Score 1: {score1_description} Score 2: {score2_description} Score 3: {score3_description} Score 4: {score4_description} Score 5: {score5_description} ###Feedback: After this, you should apply the conversation template of Llama-2-Chat (not applying it might lead to unexpected behaviors). You can find the conversation class at this link. conv = get_conv_template(\"llama-2\") conv.set_system_message(\"You are a fair evaluator language model.\") conv.append_message(conv.roles[0], dialogs['instruction']) conv.append_message(conv.roles[1], None) prompt = conv.get_prompt() x = tokenizer(prompt,truncation=False) As a result, a feedback and score decision will be generated, divided by a separating phrase [RESULT] License Feedback Collection and Prometheus is subject to OpenAI's Terms of Use for the generated data. If you suspect any violations, please reach out to us. Usage Find below some example scripts on how to use the model in transformers: Using the Pytorch model Running the model on a CPU Click to expand from transformers import T5Tokenizer, T5ForConditionalGeneration tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\") model = LlamaForCausalLM.from_pretrained(\"kaist-ai/Prometheus-13b-v1.0\", device_map=\"auto\") input_text = \"###Task Description: An instruction (might include an Input inside it), a response to evaluate, a reference answer that gets a score of 5, and a score rubric representing a evaluation criteria are given. 1. Write a detailed feedback that assess the quality of the response strictly based on the given score rubric, not evaluating in general. 2. After writing a feedback, write a score that is an integer between 1 and 5. You should refer to the score rubric. 3. The output format should look as follows: \"Feedback: (write a feedback for criteria) [RESULT] (an integer number between 1 and 5)\" 4. Please do not generate any other opening, closing, and explanations. ###The instruction to evaluate: You are a well-known psychiatrist who has a reputation for being empathetic and understanding. A client comes to you saying they've had a really hard day at work. They describe their boss as being overly critical and not listening to their ideas, which has left them feeling frustrated and undervalued. They also mention feeling overwhelmed with the workload and a growing sense of loneliness as they are new to the city. How do you respond to them to make them feel heard and understood, as well as offering helpful advice? ###Response to evaluate: It's indeed challenging to deal with a difficult boss and to carry a large workload, especially when you are new to a city and haven't established a supportive network. I would suggest taking a step back to think about your situation and perhaps speak to your boss regarding the issues you're facing. On the other hand, dealing with a large workload can be managed by prioritizing tasks, and maybe you can discuss it with your superiors too. In regards to feeling lonely, you might want to explore activities or groups that align with your interests to meet new people. And also, remember to take time for yourself to relax and destress. I'm here to assist you as we navigate through these issues. ###Reference Answer (Score 5): I hear that you're dealing with quite a few challenging situations at once. It must be tough to have your ideas dismissed by your boss and to feel undervalued in your work environment. It's understandable that such experiences can lead to frustration and a feeling of being overwhelmed. It's important to acknowledge these feelings and not to dismiss them. Feeling heard and validated in our experiences can often be the first step towards resolving the challenges we face. So, I want you to know that your feelings are completely valid and it's okay to be frustrated and overwhelmed. In terms of dealing with your boss, have you considered seeking a private conversation with them to express your feelings? It's often beneficial to communicate what you're experiencing, using \"I\" statements to express how you feel when your ideas are not taken into consideration. This can lead to mutual understanding and possibly a change in behavior. About the workload, it might help to prioritize tasks and potentially delegate, if possible. Also, consider discussing your workload with your superiors. There might be a misunderstanding about what's manageable or they might have suggestions about how to handle the situation. On the personal front, feeling lonely, especially when you're new to a city, can be really hard. Seek out opportunities to meet new people, perhaps through hobbies, community activities, or online groups. It might take a bit of time, but gradually, you can build a network of friends and acquaintances. Remember, it's perfectly okay to have bad days and it's important to take care of your mental health. Consider incorporating activities into your daily routine that make you happy and help you unwind. This could be anything from reading, yoga, going for a walk, or even listening to your favorite music. Please know that you're not alone in this. I'm here to support you through this challenging time and together, we can work towards resolving these issues. ###Score Rubrics: [Is the model able to identify and react correctly to the emotional context of the user's input?] Score 1: The model utterly fails to grasp the user's emotional context and responds in an unfitting manner. Score 2: The model sporadically identifies the emotional context but frequently replies in a manner that doesn't match the user's emotional status. Score 3: The model typically identifies the emotional context and reacts suitably, but occasionally misreads or misjudges the user's feelings. Score 4: The model often identifies the emotional context and reacts suitably, with minor cases of misreading or misjudging. Score 5: The model flawlessly identifies the emotional context of the user's input and consistently responds in a considerate and empathetic manner. ###Feedback:\" input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids outputs = model.generate(input_ids) print(tokenizer.decode(outputs[0])) Running the model on a GPU Click to expand # pip install accelerate import torch from transformers import AutoTokenizer, LlamaForCausalLM tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\") model = LlamaForCausalLM.from_pretrained(\"kaist-ai/Prometheus-13b-v1.0\", device_map=\"auto\") input_text = \"###Task Description: An instruction (might include an Input inside it), a response to evaluate, a reference answer that gets a score of 5, and a score rubric representing a evaluation criteria are given. 1. Write a detailed feedback that assess the quality of the response strictly based on the given score rubric, not evaluating in general. 2. After writing a feedback, write a score that is an integer between 1 and 5. You should refer to the score rubric. 3. The output format should look as follows: \"Feedback: (write a feedback for criteria) [RESULT] (an integer number between 1 and 5)\" 4. Please do not generate any other opening, closing, and explanations. ###The instruction to evaluate: You are a well-known psychiatrist who has a reputation for being empathetic and understanding. A client comes to you saying they've had a really hard day at work. They describe their boss as being overly critical and not listening to their ideas, which has left them feeling frustrated and undervalued. They also mention feeling overwhelmed with the workload and a growing sense of loneliness as they are new to the city. How do you respond to them to make them feel heard and understood, as well as offering helpful advice? ###Response to evaluate: It's indeed challenging to deal with a difficult boss and to carry a large workload, especially when you are new to a city and haven't established a supportive network. I would suggest taking a step back to think about your situation and perhaps speak to your boss regarding the issues you're facing. On the other hand, dealing with a large workload can be managed by prioritizing tasks, and maybe you can discuss it with your superiors too. In regards to feeling lonely, you might want to explore activities or groups that align with your interests to meet new people. And also, remember to take time for yourself to relax and destress. I'm here to assist you as we navigate through these issues. ###Reference Answer (Score 5): I hear that you're dealing with quite a few challenging situations at once. It must be tough to have your ideas dismissed by your boss and to feel undervalued in your work environment. It's understandable that such experiences can lead to frustration and a feeling of being overwhelmed. It's important to acknowledge these feelings and not to dismiss them. Feeling heard and validated in our experiences can often be the first step towards resolving the challenges we face. So, I want you to know that your feelings are completely valid and it's okay to be frustrated and overwhelmed. In terms of dealing with your boss, have you considered seeking a private conversation with them to express your feelings? It's often beneficial to communicate what you're experiencing, using \"I\" statements to express how you feel when your ideas are not taken into consideration. This can lead to mutual understanding and possibly a change in behavior. About the workload, it might help to prioritize tasks and potentially delegate, if possible. Also, consider discussing your workload with your superiors. There might be a misunderstanding about what's manageable or they might have suggestions about how to handle the situation. On the personal front, feeling lonely, especially when you're new to a city, can be really hard. Seek out opportunities to meet new people, perhaps through hobbies, community activities, or online groups. It might take a bit of time, but gradually, you can build a network of friends and acquaintances. Remember, it's perfectly okay to have bad days and it's important to take care of your mental health. Consider incorporating activities into your daily routine that make you happy and help you unwind. This could be anything from reading, yoga, going for a walk, or even listening to your favorite music. Please know that you're not alone in this. I'm here to support you through this challenging time and together, we can work towards resolving these issues. ###Score Rubrics: [Is the model able to identify and react correctly to the emotional context of the user's input?] Score 1: The model utterly fails to grasp the user's emotional context and responds in an unfitting manner. Score 2: The model sporadically identifies the emotional context but frequently replies in a manner that doesn't match the user's emotional status. Score 3: The model typically identifies the emotional context and reacts suitably, but occasionally misreads or misjudges the user's feelings. Score 4: The model often identifies the emotional context and reacts suitably, with minor cases of misreading or misjudging. Score 5: The model flawlessly identifies the emotional context of the user's input and consistently responds in a considerate and empathetic manner. ###Feedback:\" input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(\"cuda\") outputs = model.generate(input_ids, sample=True, temperature=1.0, top_p=0.9, max_new_tokens=256, repetition_penalty=1.03) print(tokenizer.decode(outputs[0])) Running the model on a GPU using different precisions FP16 Click to expand # pip install accelerate import torch from transformers import AutoTokenizer, LlamaForCausalLM tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\") model = LlamaForCausalLM.from_pretrained(\"kaist-ai/Prometheus-13b-v1.0\", device_map=\"auto\", torch_dtype=torch.float16) input_text = \"###Task Description: An instruction (might include an Input inside it), a response to evaluate, a reference answer that gets a score of 5, and a score rubric representing a evaluation criteria are given. 1. Write a detailed feedback that assess the quality of the response strictly based on the given score rubric, not evaluating in general. 2. After writing a feedback, write a score that is an integer between 1 and 5. You should refer to the score rubric. 3. The output format should look as follows: \"Feedback: (write a feedback for criteria) [RESULT] (an integer number between 1 and 5)\" 4. Please do not generate any other opening, closing, and explanations. ###The instruction to evaluate: You are a well-known psychiatrist who has a reputation for being empathetic and understanding. A client comes to you saying they've had a really hard day at work. They describe their boss as being overly critical and not listening to their ideas, which has left them feeling frustrated and undervalued. They also mention feeling overwhelmed with the workload and a growing sense of loneliness as they are new to the city. How do you respond to them to make them feel heard and understood, as well as offering helpful advice? ###Response to evaluate: It's indeed challenging to deal with a difficult boss and to carry a large workload, especially when you are new to a city and haven't established a supportive network. I would suggest taking a step back to think about your situation and perhaps speak to your boss regarding the issues you're facing. On the other hand, dealing with a large workload can be managed by prioritizing tasks, and maybe you can discuss it with your superiors too. In regards to feeling lonely, you might want to explore activities or groups that align with your interests to meet new people. And also, remember to take time for yourself to relax and destress. I'm here to assist you as we navigate through these issues. ###Reference Answer (Score 5): I hear that you're dealing with quite a few challenging situations at once. It must be tough to have your ideas dismissed by your boss and to feel undervalued in your work environment. It's understandable that such experiences can lead to frustration and a feeling of being overwhelmed. It's important to acknowledge these feelings and not to dismiss them. Feeling heard and validated in our experiences can often be the first step towards resolving the challenges we face. So, I want you to know that your feelings are completely valid and it's okay to be frustrated and overwhelmed. In terms of dealing with your boss, have you considered seeking a private conversation with them to express your feelings? It's often beneficial to communicate what you're experiencing, using \"I\" statements to express how you feel when your ideas are not taken into consideration. This can lead to mutual understanding and possibly a change in behavior. About the workload, it might help to prioritize tasks and potentially delegate, if possible. Also, consider discussing your workload with your superiors. There might be a misunderstanding about what's manageable or they might have suggestions about how to handle the situation. On the personal front, feeling lonely, especially when you're new to a city, can be really hard. Seek out opportunities to meet new people, perhaps through hobbies, community activities, or online groups. It might take a bit of time, but gradually, you can build a network of friends and acquaintances. Remember, it's perfectly okay to have bad days and it's important to take care of your mental health. Consider incorporating activities into your daily routine that make you happy and help you unwind. This could be anything from reading, yoga, going for a walk, or even listening to your favorite music. Please know that you're not alone in this. I'm here to support you through this challenging time and together, we can work towards resolving these issues. ###Score Rubrics: [Is the model able to identify and react correctly to the emotional context of the user's input?] Score 1: The model utterly fails to grasp the user's emotional context and responds in an unfitting manner. Score 2: The model sporadically identifies the emotional context but frequently replies in a manner that doesn't match the user's emotional status. Score 3: The model typically identifies the emotional context and reacts suitably, but occasionally misreads or misjudges the user's feelings. Score 4: The model often identifies the emotional context and reacts suitably, with minor cases of misreading or misjudging. Score 5: The model flawlessly identifies the emotional context of the user's input and consistently responds in a considerate and empathetic manner. ###Feedback:\" input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(\"cuda\") outputs = model.generate(input_ids) print(tokenizer.decode(outputs[0])) INT8 Click to expand # pip install bitsandbytes accelerate from transformers import AutoTokenizer, LlamaForCausalLM tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\") model = LlamaForCausalLM.from_pretrained(\"kaist-ai/Prometheus-13b-v1.0\", device_map=\"auto\", load_in_8bit=True) input_text = \"###Task Description: An instruction (might include an Input inside it), a response to evaluate, a reference answer that gets a score of 5, and a score rubric representing a evaluation criteria are given. 1. Write a detailed feedback that assess the quality of the response strictly based on the given score rubric, not evaluating in general. 2. After writing a feedback, write a score that is an integer between 1 and 5. You should refer to the score rubric. 3. The output format should look as follows: \"Feedback: (write a feedback for criteria) [RESULT] (an integer number between 1 and 5)\" 4. Please do not generate any other opening, closing, and explanations. ###The instruction to evaluate: You are a well-known psychiatrist who has a reputation for being empathetic and understanding. A client comes to you saying they've had a really hard day at work. They describe their boss as being overly critical and not listening to their ideas, which has left them feeling frustrated and undervalued. They also mention feeling overwhelmed with the workload and a growing sense of loneliness as they are new to the city. How do you respond to them to make them feel heard and understood, as well as offering helpful advice? ###Response to evaluate: It's indeed challenging to deal with a difficult boss and to carry a large workload, especially when you are new to a city and haven't established a supportive network. I would suggest taking a step back to think about your situation and perhaps speak to your boss regarding the issues you're facing. On the other hand, dealing with a large workload can be managed by prioritizing tasks, and maybe you can discuss it with your superiors too. In regards to feeling lonely, you might want to explore activities or groups that align with your interests to meet new people. And also, remember to take time for yourself to relax and destress. I'm here to assist you as we navigate through these issues. ###Reference Answer (Score 5): I hear that you're dealing with quite a few challenging situations at once. It must be tough to have your ideas dismissed by your boss and to feel undervalued in your work environment. It's understandable that such experiences can lead to frustration and a feeling of being overwhelmed. It's important to acknowledge these feelings and not to dismiss them. Feeling heard and validated in our experiences can often be the first step towards resolving the challenges we face. So, I want you to know that your feelings are completely valid and it's okay to be frustrated and overwhelmed. In terms of dealing with your boss, have you considered seeking a private conversation with them to express your feelings? It's often beneficial to communicate what you're experiencing, using \"I\" statements to express how you feel when your ideas are not taken into consideration. This can lead to mutual understanding and possibly a change in behavior. About the workload, it might help to prioritize tasks and potentially delegate, if possible. Also, consider discussing your workload with your superiors. There might be a misunderstanding about what's manageable or they might have suggestions about how to handle the situation. On the personal front, feeling lonely, especially when you're new to a city, can be really hard. Seek out opportunities to meet new people, perhaps through hobbies, community activities, or online groups. It might take a bit of time, but gradually, you can build a network of friends and acquaintances. Remember, it's perfectly okay to have bad days and it's important to take care of your mental health. Consider incorporating activities into your daily routine that make you happy and help you unwind. This could be anything from reading, yoga, going for a walk, or even listening to your favorite music. Please know that you're not alone in this. I'm here to support you through this challenging time and together, we can work towards resolving these issues. ###Score Rubrics: [Is the model able to identify and react correctly to the emotional context of the user's input?] Score 1: The model utterly fails to grasp the user's emotional context and responds in an unfitting manner. Score 2: The model sporadically identifies the emotional context but frequently replies in a manner that doesn't match the user's emotional status. Score 3: The model typically identifies the emotional context and reacts suitably, but occasionally misreads or misjudges the user's feelings. Score 4: The model often identifies the emotional context and reacts suitably, with minor cases of misreading or misjudging. Score 5: The model flawlessly identifies the emotional context of the user's input and consistently responds in a considerate and empathetic manner. ###Feedback:\" input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids.to(\"cuda\") outputs = model.generate(input_ids) print(tokenizer.decode(outputs[0])) Citation If you find the following model helpful, please consider citing our paper! BibTeX: @misc{kim2023prometheus, title={Prometheus: Inducing Fine-grained Evaluation Capability in Language Models}, author={Seungone Kim and Jamin Shin and Yejin Cho and Joel Jang and Shayne Longpre and Hwaran Lee and Sangdoo Yun and Seongjin Shin and Sungdong Kim and James Thorne and Minjoon Seo}, year={2023}, eprint={2310.08491}, archivePrefix={arXiv}, primaryClass={cs.CL} } Downloads last month8,402 Dataset used to train prometheus-eval/prometheus-13b-v1.0 prometheus-eval/Feedback-Collection Viewer • Updated Oct 14, 2023 • 153 • 93 Spaces using prometheus-eval/prometheus-13b-v1.0 2 ⚖️ Tonic/prometheus🏆 johnoye742/kaist-ai-prometheus-13b-v1.0 Company © Hugging Face TOS Privacy About Jobs Website Models Datasets Spaces Pricing Docs\n",
      "IBM Granite 13b foundation model version 1 model card IBM Granite 13b foundation model version 1 model card Granite 13 Billion Model (granite.13b) Details IBM Generative AI Large Language Foundation Models are Enterprise-level English-language models trained on enterprise-relevant datasets from five domains – internet, academic, code, legal and finance – all of which have been curated for business use by IBM. All data sets were scrutinized to exclude objectionable content and benchmarked against internal and external models to enable responsible deployment and address key issues including governance, risk assessment, privacy concerns and bias mitigation. The Granite Base 13 Billion (granite.13b.base) V1.0 model has been trained using over 1T tokens. This is the base model, from which other variants were fine-tuned to target downstream tasks. The Granite family of models will support all 5 language tasks (Q&A, Generate, Extract, Summarize, and Classify). Besides the base variant (granite.13b.base), there are two other models that have been fine-tuned. The first variant (granite.13b.instruct) is an instruction-tuned Supervised Fine-Tuning (SFT) model, which was further tuned using a combination of the Flan Collection, 15k samples from Dolly, Anthropic's human preference data about helpfulness and harmlessness, Instructv3, and internal synthetic datasets specifically designed for summarization and dialog tasks (~700K samples). The second variant (granite.13b.chat) is a Contrastive fine-tuning (CFT) variant, which uses a new objective called unlikelihood training, which penalizes unlikely generations by assigning a lower probability value. The table below lists current official variants released by IBM Research. The Massive Multitask Language Understanding (MMLU) benchmark is used to show the performance of each variant. Variant Description / Intended Use Pre-training Data Seen MMLU (5-shot) granite.13b.instruct This variant is a Supervised Fine-Tuned (SFT) version of the base model to improve its instruction-following. It was tuned using a mix of FLAN and a mixture of other datasets (Dolly, HHRLHF, and IBM internal datasets, etc.). This model is intended as a starting point to help bootstrap further downstream alignment or task-specific tuning. 1000B Tokens 42.05 granite.13b.chat This variant is a further-aligned version of the granite.13b.instruct variant. It was aligned using Constrastive Fine Tuning (CFT) for general to improve its harmlessness and the quality of its generation responses. This model should be used when looking to prompt-engineer out of the box, particularly when longer responses are desired. It also may be helpful as a starting point for further downstream fine-tuning. 1000B Tokens 42.07 Person or organization developing the model: Granite (13B) was developed by IBM Research. Model release date and version: Granite (13B) version 1.0 was released on 09/21/2023. Model type: Granite (13B) V1.0 is a large decoder-only transformer model. The following features were used in the design of the model: Decoder-only model Multi-Query Attention 50K GPT-NeoX tokenizer Flash Attention 8k context length Absolute (learnt) position embeddings Information about training algorithms, parameters, fairness constraints or other applied approaches, and features: Model was trained using 4x Tensor Parallel + 4x Pipeline Parallel + 32x ZeRO-1 + Sequence Parallel + Flash Attention using a fork of Megatron-LM. Cluster: CCC GPUs: 256x A100 80GB Interconnect: 200 gigabit Infiniband Dataset streamed over GPFS Paper or other resource for more information: https://www.ibm.com/downloads/cas/X9W4O6BM License: Available only through IBM products and offerings. Contact IBM for licensing terms. Intended Use Primary intended uses: .chat / .instruct : The Granite series of models are a family of IBM-trained decoder-only models used for text generation, summarization, question and answer, classification, and extraction. base : The base model will be primarily used to fine-tune downstream language tasks. Primary intended users: The primary users are IBM Enterprise clients looking to bolster their portfolios with Enterprise-level generative AI models. Out-of-scope use cases: The granite.13b models are not designed, tested, or supported, for code use cases of any kind. Factors Relevant factors: Models work with proper English text. All datasets have been cleansed of any type of tagging (e.g., HTML), and all media has been removed as well. Evaluation factors: Evaluation datasets have to be proper English, and are limited to text only. Metrics IBM has built a comprehensive test framework FM eval that is used throughout the model's life-cycle. This can be used both to evaluate IBM's own models and those already trained by third-parties allowing models to be measured against a variety of benchmarks. The evaluation framework runs on an Openshift cluster with GPU support and uses various AI model evaluation frameworks: Eleuther AI's Language Model Evaluation Harness lm-eval, Stanford's Holistic Evaluation of Language Models (HELM), Beyond the Imitation Game Benchmark (BIG-bench), as well as IBM-internal datasets. Performance Metrics The evaluation of the Granite 13 Billion (13B) variants can be found in the Granite Paper: https://www.ibm.com/downloads/cas/X9W4O6BM Data, Limitations, and Recommendations Data selection for training: The Granite Base (13B) V1.0 model was trained using IBM's curated pre-training dataset. A breakdown of the sampling data used for training is shown in the table below. Data Composition and Sampling Dataset sampling for Granite Base (13B) V1.0 Dataset Description Common Crawl Open repository of web crawl data. Webhose Unstructured web content converted into machine-readable data feeds acquired by IBM. arXiv Over 1.8 million scientific paper pre-prints posted to arXiv. wikimedia Eight English Wikimedia projects (enwiki, enwikibooks, enwikinews, enwikiquote, enwikisource, enwikiversity, enwikivoyage, enwiktionary). containing extracted plain text from pages and articles. OpenWeb Text Open-source version of OpenAI’s Web Text corpus containing web pages through 2019. Stack Exchange Anonymized set of all user-contributed content on the Stack Exchange network, a popular collection of websites centered around user-contributed questions and answers. Hacker News News on computer science and entrepreneurship, taken between 2007-2018. Project Gutenberg PG19 A repository of free e-books with focus on older works for which U.S. copyright has expired. GitHub Clean Code data from CodeParrot covering a variety of coding languages. Pubmed Central Biomedical and life sciences papers. Free Law Public-domain legal opinions from US federal and state courts. SEC Filings 10-K/Q filings from the US Securities and Exchange Commission (SEC) for the years 1934-2022. USPTO US patents granted from 1975 to May 2023, excluding design patents. DeepMind Mathematics Mathematical question and answer pairs data. Tokenizer used: GPT-NeoX 20B 1.03 T Tokens More... Dataset sampling for Granite Instruct (13B) V1.0 The Granite Instruct model was initialized from Granite 13B Base and Supervised Fine-Tuned (SFT) with a mixture of with a mixture of datasets from different sources. The SFT data includes a subset of the Flan Collection,15K samples from Dolly, Anthropic’s human preference data about helpfulness and harmlessness, Instructv3, and internal synthetic datasets specifically designed for summarization and dialog tasks. Dataset sampling for Granite Chat (13B) V1.0 The Granite Chat model was initialized from Granite 13B Instruct and fine-tuned with a mixture of instruction-tuning datasets. It was aligned using Constrastive Fine Tuning (CFT) for general to improve its harmlessness and the quality of its generation responses. The datasets for CFT are paired samples from Anthropic’s human preference data about helpfulness and harmlessness that have been filtered using the OpenAssist reward model, samples from Dolly, and samples from ProsocialDialog.\n",
      "IBM granite-13b-instruct-v2 model card IBM granite-13b-instruct-v2 model card Granite Base 13 Billion Model (granite.13b.v2.instruct) Details IBM Generative AI Large Language Foundation Models are Enterprise-level English-language models trained with large a volume of data that has been subjected to intensive pre-processing and careful analysis. The Granite 13 Billion V2.0 Instruct (granite.13b.v2.instruct) model is the instruction-tuned variant initialized from the pre-trained granite.13b.v2 model. Model Checkpoint Name Pre-training Data Seen MMLU (5-shot) granite.13b.v2.instruct granite.13b.2500b.chat (17 November 2023) 2.5T Tokens 45.3 Person or organization developing the model: Granite (13B) Instruct was developed by IBM Research for its watsonx.ai initiative. Model release date and version: Granite (13B) Instruct version 2.0 was released on 30 November 2023. Model type: Granite Base (13B) Instruct V2.0 is a large decoder-only transformer model. The following features were used in the design of the model: Decoder-only model Multi-Query Attention 50K GPT-NeoX tokenizer Flash Attention 8k context length Absolute (learnt) position embeddings Information about training algorithms, parameters, fairness constraints or other applied approaches, and features: Model was trained using 4x Tensor Parallel + 4x Pipeline Parallel + Megatron distributed optimizer Megatron-LM. Cluster: CCC GPUs: 256x A100 80GB Interconnect: 200 gigabit Infiniband Dataset streamed over GPFS Paper or other resource for more information: https://www.ibm.com/downloads/cas/X9W4O6BM License: Available only through IBM products and offerings. Version Release notes: Granite.13b.v2.instruct was tuned in a very similar manner to granite.13b.v1.instruct, and therefore prompts that previously performed well with v1 are expected to perform reasonably well with v2 (minor prompt optimizations may still be needed). Aside from benefiting from the updated base model (granite.13b.v2), the only other major change implemented for granite.13b.v2 was specific alignment steps to improve the model's robustness to whitespaces. Intended Use [PENDING FINAL EVALUATION] Primary intended uses: English based classification, extraction, and summarization. Primary intended users: The primary users are IBM Enterprise clients looking to bolster their portfolios with Enterprise-level generative AI models. Out-of-scope use cases: The granite.13b models are not designed, tested, or supported, for code use cases of any kind. Factors Relevant factors: Models work with proper english text. All datasets have been cleansed of any type of tagging (e.g., HTML), and all media has been removed as well. Evaluation factors: Evaluation datasets have to be proper english, and are limited to text only. Metrics IBM has built a comprehensive test framework FM eval that is used through out the model's life-cycle. This can be used both to evaluate IBM's own models and those already trained by third-parties allowing models to be measured against a variety of benchmarks. The evaluation framework runs on an Openshift cluster with GPU support and uses various AI model evaluation frameworks: Eleuther AI's Language Model Evaluation Harness lm-eval, Stanford's Holistic Evaluation of Language Models (HELM), Beyond the Imitation Game Benchmark (BIG-bench), as well as IBM-internal datasets. Data, Limitations, and Recommendations Data selection for training: The Granite (13B) V2.0 model underwent extended training using tokens from the IBM Data Pile Version 0.4 on top of the original tokens from the IBM Data Pile Version 0.3. A breakdown of the sampling data used for pre-training the base model can be found on the granite.13b.v2. Data Composition and Sampling On top of the 2.5 Trillion tokens from the base model (granite.13b.v2), this model underwent instruction tuning. Tokenizer used: GPT-NeoX 20B 2.0 Trillion Tokens More... Dataset sampling for Granite (13B) V2.0 Instruct The granite.13b.v2.instruct model variant was fine-tuned using Supervised Fine-Tuned (SFT). It was first initialized from the granite.13b.v2 base model and trained on mix700k, a custom mixture of internal and external datasets with a size of 700,000 instructions. The dataset was generated with datasets==2.13, it's seed set to 901, and it includes 100,000 prompts (instruct-v3) in Tulu's encoding_templates_w_input format. The datasets were used in the SFT mixture. Dolly HHRLHF gma_emnlp203-PromptEd (IBM created synthetic dataset) ConvAI (IBM curated data mixture) Creme Brule Wave 1 IBM data for Creme Brule (portion 1) synth_asaf_askhr Dataset (IBM created synthetic dataset) Creme Brule Wave 1 instructv3 Quantitative Analyses Ethical Considerations The IBM AI Ethics Board is a central, cross-disciplinary body that defines the AI Ethics vision and strategy with the objective of supporting a culture of ethical, responsible, and trustworthy AI throughout the IBM organization. The Board's mission is to support a centralized governance, review, and decision-making process for IBM ethics policies, practices, communications, research, products and services. The AI Ethics Board has sponsored the Ethics by Design framework (EbD) and methodology which integrates tech ethics tech ethics solutions in the technology development pipeline, including AI systems. EbD builds upon IBM's existing security and privacy practices including Security and Privacy by Design. IBM Products are expected to follow this methodology; EbD is embedded in IBM's governance structure through IBM's Tech Ethics by Design corporate directive and enabling processes. The EbD framework considers five pillars of trustworthiness: fairness, transparency, explainability, robustness, and privacy. EbD addresses ethical design at all phases of the AI life cycle including data collection, model building and training, model validation, and model deployment. Appendix A - Model Configuration { \"activation_function\": \"gelu\", \"architectures\": [ \"GPTBigCodeForCausalLM\" ], \"attention_softmax_in_fp32\": true, \"attn_pdrop\": 0.1, \"bos_token_id\": 0, \"embd_pdrop\": 0.1, \"eos_token_id\": 0, \"initializer_range\": 0.02, \"layer_norm_epsilon\": 1e-05, \"model_type\": \"gpt_bigcode\", \"multi_query\": true, \"n_embd\": 5632, \"n_head\": 44, \"n_inner\": 22528, \"n_layer\": 40, \"n_positions\": 8192, \"pad_token_id\": 0, \"resid_pdrop\": 0.1, \"scale_attention_softmax_in_fp32\": true, \"scale_attn_weights\": true, \"summary_activation\": null, \"summary_first_dropout\": 0.1, \"summary_proj_to_labels\": true, \"summary_type\": \"cls_index\", \"summary_use_proj\": true, \"torch_dtype\": \"float32\", \"transformers_version\": \"4.28.1\", \"use_cache\": true, \"vocab_size\": 50304 } Model-specific instruction format System prompt No special prompt is required for this model Model specific format No special format required for this model, this model benefited from flan-templated data in its alignment step and therefore should perform well with flan-style prompt templates. Model-specific configurations At this time, there are no known specific parameter settings or stopping sequences are needed for this model. Languages English-Only Tasks Intended uses of this model cover Classification and Extraction use-cases. This model can also potentially support Summarization use cases, particularly when shorter, concise summaries are desired.\n",
      "IBM granite-13b-chat-v2 model card IBM granite-13b-chat-v2 model card Granite Base 13 Billion Model Chat (granite-13b-chat-v2) Details Model Version (2.1.0): Released 2/15/2024 The Granite 13 Billion chat V2 (granite-13b-chat-v2) model is the chat-focused variant initialized from the pre-trained Granite Base 13 Billion Base V2 (granite-13b-base-v2) model. granite-13b-base-v2 has been trained using over 2.5T tokens. IBM Generative AI Large Language Foundation Models are Enterprise-level English-language models trained with large a volume of data that has been subjected to intensive pre-processing and careful analysis. granite-13b-chat-v2 is a chat-focused model that was tuned to improve its ability to perform Retrieval Augmented Generation (RAG) use cases. In version 2.1.0, IBM has applied a novel alignment technique for LLMs using large-scale targeted alignment for a generalist LLM. This alignment technique significantly improves base model performance by knowledge infusion during the initial phase of alignment, and improves instruction following performance via skills and style infusion in the subsequent phase of alignment. The table below lists the Massive Multitask Language Understanding (MMLU) and Multi-Turn(MT) Bench benchmarks used to show the performance. Variant Description / Intended Use Pre-training Data Seen MMLU (5-shot) MT Bench granite-13b-chat-v2 (v2.1.0) This variant is a chat-focused version of the base model which supports RAG, summarization and generation tasks 2.5T Tokens 57 6.92 Person or organization developing the model: granite-13b-chat-v2 was developed by IBM Research. Model release date and version: Model version 2.1.0 is released 2/15/2024. granite-13b-chat-v2 was originally released on 11/30/2023. Model type: granite-13b-chat-v2 is a large decoder-only transformer model. The following features were used in the design of the model: Decoder-only model Multi-Query Attention 50K GPT-NeoX tokenizer Flash Attention 8k context length Absolute (learnt) position embeddings Information about training algorithms, parameters, fairness constraints or other applied approaches, and features: Model was trained using 4x Tensor Parallel + 4x Pipeline Parallel + Megatron distributed optimizer Megatron-LM. Cluster: CCC GPUs: 256x A100 80GB Interconnect: 200 gigabit Infiniband Dataset streamed over GPFS Paper or other resource for more information: https://www.ibm.com/downloads/cas/X9W4O6BM License: Available only through IBM products and offerings. Contact IBM for licensing terms. Version Release notes: With version 2.1.0 of granite-13b-chat-v2 IBM implemented a number of new techniques specifically targeted at improving the model's ability to aquire knowledge and revelant skills. To accomplish this goal, the team applied IBM Research's novel alignment technique for LLMs using large-scale targeted alignment for a generalist LLM, which includes several synthetic data innovations. The behaviors targeted in the alignment step include: Significantly improved quality of generated of response Improved multi-turn conversation capabilities Improved safety / bias reduction High quality content-grounded responses The manner in which changes were implemented for this modification should allow for previously engineered prompts to transfer well to this version. Some prompts may need to be reworked. Intended Use Primary intended uses: English-based closed-domain Question and Answering (e.g. RAG), summarization, and generation, extraction, and classification. The granite-13b-chat-v2 model has demonstrated the capability to support longer responses preferred in RAG-like use cases. Primary intended users: The primary users are IBM Enterprise clients looking to bolster their portfolios with Enterprise-level generative AI models. Out-of-scope use cases: The granite-13b models are not designed, tested, or supported, for code use cases of any kind. Factors Relevant factors: Models work with proper english text. All datasets have been cleansed of any type of tagging (e.g., HTML), and all media has been removed as well. Evaluation factors: Evaluation datasets have to be proper english, and are limited to text only. Metrics granite.13b.chat.v2 was evaluated using the following two well known datasets: Benchmark Description MMLU Benchmark used to measure a text model's multitask accuracy. The test covers 57 tasks including elementary mathematics, US history, computer science, law, and more. MT-Bench Benchmark consisting of 80 high-quality multi-turn questions. MT-bench is designed to test multi-turn conversation and instruction-following ability, covering common use cases and focusing on challenging questions to differentiate models. Performance Metrics The evaluation of granite-13b-v2 can be found in the Granite Paper: https://www.ibm.com/downloads/cas/X9W4O6BM Data, Limitations, and Recommendations Data selection for training: The granite-13b-base-v2 model underwent extended training using 2.5 Trillion tokens of IBM's curated pre-training dataset. On top of the 2.5 trillion tokens from granite-13b-base-v2, the granite-13b-chat.v2 model underwent knowledge and skill focused tuning. A breakdown of the sampling data used for training is shown in the table below. Dataset sampling for Granite (13B) Base V2.0 Dataset Description Common Crawl Open repository of web crawl data. Webhose Unstructured web content converted into machine-readable data feeds acquired by IBM. arXiv Over 1.8 million scientific paper pre-prints posted to arXiv. Wikimedia Eight English Wikimedia projects (enwiki, enwikibooks, enwikinews, enwikiquote, enwikisource, enwikiversity, enwikivoyage, enwiktionary) containing extracted plain text from pages and articles. OpenWeb Text Open-source version of OpenAI’s Web Text corpus containing web pages through 2019. Stack Exchange Anonymized set of all user-contributed content on the Stack Exchange network, a popular collection of websites centered around user-contributed questions and answers. Hacker News News on computer science and entrepreneurship, taken between 2007-2018. Project Gutenberg PG19 A repository of free e-books with focus on older works for which U.S. copyright has expired. GitHub Clean Code data from CodeParrot covering a variety of coding languages. Pubmed Central Biomedical and life sciences papers. Free Law Public-domain legal opinions from US federal and state courts. SEC Filings 10-K/Q filings from the US Securities and Exchange Commission (SEC) for the years 1934-2022. Patents US patents granted from 1975 to May 2023, excluding design patents. DeepMind Mathematics Mathematical question and answer pairs data. Earning Calls Transcript Transcripts from the quarterly earnings calls that companies hold with investors. The dataset reports a collection of earnings call transcripts, the related stock prices, and the sector index. EDGAR This corpus comprises of annual reports from all the publicly traded companies in the US spanning a period of more than 25 years. FDIC The data is from the annual submissions of the FDIC. Finance Textbooks This corpus is from Open Textbook Library which is UMN's free textbook library, and this dataset includes the dump of all textbooks tagged as finance. Financial Research Papers Publicly available financial research paper corpus. IBM Documentation IBM redbooks and product documents. Tokenizer used: GPT-NeoX 20B 2.5 Trillion Tokens More... Dataset sampling for Granite (13B) Chat V2 The granite-13b-chat-v2 model, version 2.1.0, was initialized from granite-13b-base-v2, and was aligned using a novel training paradigm for LLMs where large-scale targeted alignment for a generalist LLMs. The alignment approach relies on IBM-generated synthetic data using a proprietary pipeline and approach.\n"
     ]
    }
   ],
   "source": [
    "cleaned_models_data = []\n",
    "for url in urls:\n",
    "    cleaned_model_data = extract_text_from_website(url)\n",
    "    cleaned_models_data.append(cleaned_model_data)\n",
    "    print(cleaned_model_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_rows = []\n",
    "for cleaned_model_data in cleaned_models_data:\n",
    "    model_csv_row = prompt_to_llm(model_row_prompt, cleaned_model_data=cleaned_model_data, json_example=json_example, json_schema=json_schema)\n",
    "    model_rows.append(model_csv_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Here is the JSON file created using the provided information:\\n\\n```json\\n{\\n  \"ModelName\": \"Code Llama\",\\n  \"Size\": \"70B\",\\n  \"ID\": \"CodeLlama - 70B\",\\n  \"Provider\": \"Meta\",\\n  \"Architecture\": \"Decoder-only\",\\n  \"ContextLength\": 100000,\\n  \"Price\": \"Free\",\\n  \"Languages\": [\"Python\", \"C++\", \"Java\", \"PHP\", \"Typescript (Javascript)\", \"C#\", \"Bash\"],\\n  \"TunningInformation\": \"Instruction-tuned\",\\n  \"TrainingData\": \"Code and code-related data\",\\n  \"UsesSupported\": [\"Code generation\", \"natural language about code\", \"debugging\"],\\n  \"OptimisedFor\": \"Low latency, real-time code completion, long inputs\",\\n  \"PromptingAdvice\": \"Provide the model with code or natural language prompts\",\\n  \"Output\": \"Code and natural language about code\",\\n  \"PromptTuningAvailability\": true,\\n  \"RegionalAvailability\": \"Not specified\",\\n  \"License\": \"Community license as Llama 2\"\\n}\\n```\\nPlease note that the `TunningInformation` field is a string and I have added quotes around it.',\n",
       " 'Here is the JSON file created using the provided information:\\n\\n```json\\n{\\n  \"ModelName\": \"Deepseek Coder\",\\n  \"Size\": \"33B\",\\n  \"ID\": \"deepseek-coder-33b-instruct\",\\n  \"Provider\": \"deepseek-ai\",\\n  \"Architecture\": \"Decoder-only\",\\n  \"ContextLength\": 16000,\\n  \"Price\": \"Not specified\",\\n  \"Languages\": [\"Python\", \"English\", \"Chinese\"],\\n  \"TunningInformation\": \"Instruction-tuned\",\\n  \"TrainingData\": \"Project-level code corpus\",\\n  \"UsesSupported\": [\"Code generation\", \"Code completion\", \"Code infilling\"],\\n  \"OptimisedFor\": \"Project-level code completion and infilling\",\\n  \"PromptingAdvice\": \"Provide the model with code or natural language prompts\",\\n  \"Output\": \"Code and natural language\",\\n  \"PromptTuningAvailability\": false,\\n  \"RegionalAvailability\": \"Not specified\",\\n  \"License\": \"Other (see the LICENSE-MODEL for more details)\"\\n}\\n```\\n\\nPlease note that some information was not explicitly provided (like the price and context length), so I made reasonable assumptions based on the information available.',\n",
       " 'Here is the JSON file created using the provided information:\\n\\n```json\\n{\\n  \"ModelName\": \"Falcon-180B\",\\n  \"Size\": \"180B\",\\n  \"ID\": \"tiiuae/falcon-180B\",\\n  \"Provider\": \"TII\",\\n  \"Architecture\": \"Decoder-only\",\\n  \"ContextLength\": 2048,\\n  \"Price\": \"Unknown\",\\n  \"Languages\": [\"English\", \"German\", \"Spanish\", \"French\"],\\n  \"TunningInformation\": \"Raw, pretrained model\",\\n  \"TrainingData\": \"RefinedWeb enhanced with curated corpora\",\\n  \"UsesSupported\": [\"Research\", \"Finetuning\", \"Specialization\"],\\n  \"OptimisedFor\": \"Inference\",\\n  \"PromptingAdvice\": \"Consider finetuning for specific usecases\",\\n  \"Output\": \"Text generation\",\\n  \"PromptTuningAvailability\": false,\\n  \"RegionalAvailability\": \"Publicly accessible\",\\n  \"License\": \"Falcon-180B TII License and Acceptable Use Policy\"\\n}\\n```\\nPlease note that the `Price` field is set to \"Unknown\" because there was no specific information provided about the pricing.',\n",
       " 'Here is the JSON file created using the provided information:\\n\\n```json\\n{\\n  \"ModelName\": \"Falcon-40B\",\\n  \"Size\": \"40B\",\\n  \"ID\": \"tiiuae/falcon-40b\",\\n  \"Provider\": \"TII\",\\n  \"Architecture\": \"Decoder-only\",\\n  \"ContextLength\": 2048,\\n  \"Price\": \"Not specified\",\\n  \"Languages\": [\"English\", \"German\", \"Spanish\", \"French\", \"Italian\", \"Portuguese\", \"Polish\", \"Dutch\", \"Romanian\", \"Czech\", \"Swedish\"],\\n  \"TunningInformation\": \"Raw, pretrained model\",\\n  \"TrainingData\": \"1,000B tokens of RefinedWeb enhanced with curated corpora\",\\n  \"UsesSupported\": [\"Research\", \"Finetuning\", \"Specialization\"],\\n  \"OptimisedFor\": \"Inference\",\\n  \"PromptingAdvice\": \"Consider finetuning for specific usecases\",\\n  \"Output\": \"Text generation\",\\n  \"PromptTuningAvailability\": false,\\n  \"RegionalAvailability\": \"Not specified\",\\n  \"License\": \"Apache 2.0 license\"\\n}\\n```',\n",
       " 'Here is the JSON file created using the provided information:\\n\\n```json\\n{\\n  \"ModelName\": \"FLAN-T5 XL\",\\n  \"Size\": \"2.85B params\",\\n  \"ID\": \"google/flan-t5-xl\",\\n  \"Provider\": \"Hugging Face\",\\n  \"Architecture\": \"Transformers\",\\n  \"ContextLength\": 512,\\n  \"Price\": \"Not specified\",\\n  \"Languages\": [\"English\", \"Spanish\", \"Japanese\", \"Persian\", \"Hindi\", \"French\", \"Chinese\", \"Bengali\", \"Gujarati\", \"German\", \"Telugu\", \"Italian\", \"Arabic\", \"Polish\", \"Tamil\", \"Marathi\", \"Malayalam\", \"Oriya\", \"Panjabi\", \"Portuguese\", \"Urdu\", \"Galician\", \"Hebrew\", \"Korean\", \"Catalan\", \"Thai\", \"Dutch\", \"Indonesian\", \"Vietnamese\", \"Bulgarian\", \"Filipino\", \"Central Khmer\", \"Lao\", \"Turkish\", \"Russian\", \"Croatian\", \"Swedish\", \"Yoruba\", \"Kurdish\", \"Burmese\", \"Malay\", \"Czech\", \"Finnish\", \"Somali\", \"Tagalog\", \"Swahili\", \"Sinhala\", \"Kannada\", \"Zhuang\", \"Igbo\", \"Xhosa\", \"Romanian\", \"Haitian\", \"Estonian\", \"Slovak\", \"Lithuanian\", \"Greek\", \"Nepali\", \"Assamese\", \"Norwegian\"],\\n  \"TunningInformation\": \"Instruction-tuned\",\\n  \"TrainingData\": \"A mixture of tasks including those in the table below (from the original paper, figure 2)\",\\n  \"UsesSupported\": [\"Research on zero-shot NLP tasks and in-context few-shot learning\", \"NLP tasks such as reasoning, and question answering\", \"Advancing fairness and safety research\", \"Understanding limitations of current large language models\"],\\n  \"OptimisedFor\": \"Not specified\",\\n  \"PromptingAdvice\": \"Provide the model with appropriate prompts\",\\n  \"Output\": \"Text\",\\n  \"PromptTuningAvailability\": true,\\n  \"RegionalAvailability\": \"Not specified\",\\n  \"License\": \"Apache 2.0\"\\n}\\n```\\n\\nPlease note that the `ContextLength` field is a rough estimate and may vary depending on the specific use case. The `Price` field is also not specified as it depends on the usage and provider.',\n",
       " 'Here is the JSON file created using the provided information:\\n\\n```json\\n{\\n  \"ModelName\": \"FLAN-T5 XXL\",\\n  \"Size\": \"11.3B params\",\\n  \"ID\": \"google/flan-t5-xxl\",\\n  \"Provider\": \"Google\",\\n  \"Architecture\": \"Transformer\",\\n  \"ContextLength\": 512,\\n  \"Price\": \"Not specified\",\\n  \"Languages\": [\"English\", \"German\", \"French\"],\\n  \"TunningInformation\": \"Instruction-tuned\",\\n  \"TrainingData\": \"A mixture of tasks including reasoning, question answering, and more\",\\n  \"UsesSupported\": [\"Research on language models\", \"Zero-shot NLP tasks\", \"In-context few-shot learning\", \"Advancing fairness and safety research\"],\\n  \"OptimisedFor\": \"Not specified\",\\n  \"PromptingAdvice\": \"Not specified\",\\n  \"Output\": \"Language model generation\",\\n  \"PromptTuningAvailability\": false,\\n  \"RegionalAvailability\": \"Not specified\",\\n  \"License\": \"Apache 2.0\"\\n}\\n```\\n\\nPlease note that some fields are estimated or assumed based on common practices and the information provided. For example, the context length is assumed to be 512, as this is a common value for transformer-based language models.',\n",
       " 'Here is the JSON file created with the provided information:\\n\\n```json\\n{\\n  \"ModelName\": \"Flan-UL2\",\\n  \"Size\": \"20B\",\\n  \"ID\": \"google/flan-ul2\",\\n  \"Provider\": \"Google\",\\n  \"Architecture\": \"Encoder-decoder\",\\n  \"ContextLength\": 2048,\\n  \"Price\": \"Not specified\",\\n  \"Languages\": [\"English\", \"French\", \"German\", \"Spanish\", \"Italian\"],\\n  \"TunningInformation\": \"Flan Prompting\",\\n  \"TrainingData\": \"C4, Flan datasets\",\\n  \"UsesSupported\": [\"Text generation\", \"Question answering\", \"Translation\", \"Summarization\"],\\n  \"OptimisedFor\": \"Few-shot in-context learning\",\\n  \"PromptingAdvice\": \"Provide the model with instruction-based prompts\",\\n  \"Output\": \"Text\",\\n  \"PromptTuningAvailability\": true,\\n  \"RegionalAvailability\": \"Not specified\",\\n  \"License\": \"Apache-2.0\"\\n}\\n```\\nPlease note that the `Price` field is set to \"Not specified\" as I couldn\\'t find any information about the pricing. Also, the `Languages` field is set to the five most common languages in the world, assuming that the model supports them, but this can be adjusted according to the actual capabilities of the model.',\n",
       " 'Here is the JSON file created using the provided information:\\n\\n```json\\n{\\n  \"ModelName\": \"Jais-13b-chat\",\\n  \"Size\": \"13B\",\\n  \"ID\": \"jais-13b-chat\",\\n  \"Provider\": \"core42\",\\n  \"Architecture\": \"Decoder-only\",\\n  \"ContextLength\": 2048,\\n  \"Price\": \"Not specified\",\\n  \"Languages\": [\"Arabic\", \"English\"],\\n  \"TunningInformation\": \"Finetuned\",\\n  \"TrainingData\": \"Arabic and English prompt-response pairs\",\\n  \"UsesSupported\": [\"Conversational AI\"],\\n  \"OptimisedFor\": \"Bilingual conversations\",\\n  \"PromptingAdvice\": \"Follow the specific prompt format\",\\n  \"Output\": \"Text responses\",\\n  \"PromptTuningAvailability\": false,\\n  \"RegionalAvailability\": \"Not specified\",\\n  \"License\": \"Apache 2.0\"\\n}\\n```\\nPlease note that the `ContextLength` field is set to the maximum length the model can handle in a single request, but it doesn\\'t necessarily reflect the optimal context length for the model. The `Price` field is set to \"Not specified\" because the original information did not provide pricing details.',\n",
       " 'Here is the JSON file created using the provided information:\\n```json\\n{\\n  \"ModelName\": \"ELYZA-japanese-Llama-2-7b-instruct\",\\n  \"Size\": \"6.27B\",\\n  \"ID\": \"ELYZA-japanese-Llama-2-7b-instruct\",\\n  \"Provider\": \"Meta\",\\n  \"Architecture\": \"Decoder-only\",\\n  \"ContextLength\": 256,\\n  \"Price\": \"Not specified\",\\n  \"Languages\": [\"Japanese\", \"English\"],\\n  \"TunningInformation\": \"Instruction-tuned\",\\n  \"TrainingData\": \"Not specified\",\\n  \"UsesSupported\": [\"Text generation\"],\\n  \"OptimisedFor\": \"Not specified\",\\n  \"PromptingAdvice\": \"Provide the model with text prompts\",\\n  \"Output\": \"Text\",\\n  \"PromptTuningAvailability\": false,\\n  \"RegionalAvailability\": \"Not specified\",\\n  \"License\": \"Llama 2 Community License\"\\n}\\n```\\nPlease note that some fields were filled with general assumptions due to the lack of specific information.',\n",
       " 'Here is the JSON file created using the provided information:\\n\\n```json\\n{\\n  \"ModelName\": \"Llama-2\",\\n  \"Size\": \"13B\",\\n  \"ID\": \"meta-llama/Llama-2-13b-hf\",\\n  \"Provider\": \"Meta\",\\n  \"Architecture\": \"Transformer\",\\n  \"ContextLength\": 4096,\\n  \"Price\": \"Not specified\",\\n  \"Languages\": [\"English\"],\\n  \"TunningInformation\": \"Supervised fine-tuning (SFT) and reinforcement learning with human feedback (RLHF)\",\\n  \"TrainingData\": \"A new mix of publicly available online data\",\\n  \"UsesSupported\": [\"Assistant-like chat\"],\\n  \"OptimisedFor\": \"Not specified\",\\n  \"PromptingAdvice\": \"Follow specific formatting, including INST and > tags, BOS and EOS tokens, and whitespaces and breaklines\",\\n  \"Output\": \"Text\",\\n  \"PromptTuningAvailability\": false,\\n  \"RegionalAvailability\": \"Not specified\",\\n  \"License\": \"Custom commercial license available\"\\n}\\n```\\n\\nThis JSON object represents the Llama-2 model with a size of 13 billion parameters, provided by Meta. It is a transformer-based model trained with supervised fine-tuning and reinforcement learning with human feedback. It is optimized for assistant-like chat in English. The model follows specific formatting for input and generates text as output. The license is a custom commercial license available from Meta.',\n",
       " 'Here is the JSON file created using the provided information:\\n\\n```json\\n{\\n  \"ModelName\": \"llama2-dpo-v7\",\\n  \"Size\": \"13.2B\",\\n  \"ID\": \"mncai/llama2-13b-dpo-v7\",\\n  \"Provider\": \"MindsAndCompany\",\\n  \"Architecture\": \"Decoder-only\",\\n  \"ContextLength\": 2048,\\n  \"Price\": \"Not specified\",\\n  \"Languages\": [\"English\", \"Korean\"],\\n  \"TunningInformation\": \"Instruction-tuned and Differential Privacy Optimized\",\\n  \"TrainingData\": \"Not specified\",\\n  \"UsesSupported\": [\"Text generation\"],\\n  \"OptimisedFor\": \"Not specified\",\\n  \"PromptingAdvice\": \"Provide the model with text prompts\",\\n  \"Output\": \"Text\",\\n  \"PromptTuningAvailability\": false,\\n  \"RegionalAvailability\": \"Not specified\",\\n  \"License\": \"Llama 2 license\"\\n}\\n```\\nPlease note that some fields are marked as \"Not specified\" because the information was not provided.',\n",
       " 'Here is the JSON file created using the provided information:\\n```json\\n{\\n  \"ModelName\": \"Mistral-7B-Instruct-v0.2\",\\n  \"Size\": \"7B\",\\n  \"ID\": \"Mistral-7B-Instruct-v0.2\",\\n  \"Provider\": \"Mistral AI\",\\n  \"Architecture\": \"Transformers\",\\n  \"ContextLength\": 32000,\\n  \"Price\": 2.09,\\n  \"Languages\": [\"English\"],\\n  \"TunningInformation\": \"Finetuned conversational inference\",\\n  \"TrainingData\": \"Text generation inference\",\\n  \"UsesSupported\": [\"Text generation\"],\\n  \"OptimisedFor\": \"Inference Endpoints\",\\n  \"PromptingAdvice\": \"Surround your prompt with [INST] and [/]\",\\n  \"Output\": \"Text\",\\n  \"PromptTuningAvailability\": true,\\n  \"RegionalAvailability\": \"Not specified\",\\n  \"License\": \"Apache-2.0\"\\n}\\n```',\n",
       " 'Here is the JSON file created using the provided information:\\n```json\\n{\\n  \"ModelName\": \"Mixtral-8x7B-Instruct-v0.1\",\\n  \"Size\": \"8x7B\",\\n  \"ID\": \"mistralai/Mixtral-8x7B-Instruct-v0.1\",\\n  \"Provider\": \"Mistral AI\",\\n  \"Architecture\": \"Sparse Mixture of Experts\",\\n  \"ContextLength\": 2048,\\n  \"Price\": 3.74,\\n  \"Languages\": [\"English\", \"French\", \"German\", \"Spanish\", \"Italian\"],\\n  \"TunningInformation\": \"Instruction-tuned\",\\n  \"TrainingData\": \"Not specified\",\\n  \"UsesSupported\": [\"Text generation\"],\\n  \"OptimisedFor\": \"Not specified\",\\n  \"PromptingAdvice\": \"Use the specific instruction format\",\\n  \"Output\": \"Text\",\\n  \"PromptTuningAvailability\": true,\\n  \"RegionalAvailability\": \"Not specified\",\\n  \"License\": \"Apache-2.0\"\\n}\\n```\\nPlease note that some fields are estimated or assumed based on the provided information.',\n",
       " 'Here is the JSON file created using the provided information:\\n\\n```json\\n{\\n  \"ModelName\": \"Mixtral-8x7B\",\\n  \"Size\": \"8B\",\\n  \"ID\": \"TheBloke/Mixtral-8x7B-v0.1-GPTQ\",\\n  \"Provider\": \"Hugging Face\",\\n  \"Architecture\": \"Transformer\",\\n  \"ContextLength\": 4096,\\n  \"Price\": \"Not specified\",\\n  \"Languages\": [\"English\", \"French\", \"German\", \"Italian\", \"Spanish\"],\\n  \"TunningInformation\": \"GPTQ\",\\n  \"TrainingData\": \"Not specified\",\\n  \"UsesSupported\": [\"Text generation\"],\\n  \"OptimisedFor\": \"4-bit precision\",\\n  \"PromptingAdvice\": \"No specific template\",\\n  \"Output\": \"Text\",\\n  \"PromptTuningAvailability\": true,\\n  \"RegionalAvailability\": \"Not specified\",\\n  \"License\": \"Apache-2.0\"\\n}\\n```\\nPlease note that some fields are marked as \"Not specified\" because the information was not provided.',\n",
       " 'Here is the JSON file created using the provided information:\\n\\n```json\\n{\\n  \"ModelName\": \"Multilingual-E5-large\",\\n  \"Size\": \"24 layers, embedding size 1024\",\\n  \"ID\": \"intfloat/multilingual-e5-large\",\\n  \"Provider\": \"Hugging Face\",\\n  \"Architecture\": \"Transformer\",\\n  \"ContextLength\": 512,\\n  \"Price\": \"Not specified\",\\n  \"Languages\": [\\n    \"Afrikaans\",\\n    \"Albanian\",\\n    \"Amharic\",\\n    \"Arabic\",\\n    \"Armenian\",\\n    \"Assamese\",\\n    \"Azerbaijani\",\\n    \"Bashkir\",\\n    \"Basque\",\\n    \"Belarusian\",\\n    \"Bengali\",\\n    \"Bosnian\",\\n    \"Bulgarian\",\\n    \"Burmese\",\\n    \"Catalan\",\\n    \"Central Khmer\",\\n    \"Chinese\",\\n    \"Croatian\",\\n    \"Czech\",\\n    \"Danish\",\\n    \"Dutch\",\\n    \"English\",\\n    \"Esperanto\",\\n    \"Estonian\",\\n    \"Finnish\",\\n    \"French\",\\n    \"Galician\",\\n    \"Georgian\",\\n    \"German\",\\n    \"Greek\",\\n    \"Gujarati\",\\n    \"Hausa\",\\n    \"Hebrew\",\\n    \"Hindi\",\\n    \"Hungarian\",\\n    \"Icelandic\",\\n    \"Indonesian\",\\n    \"Interlingua\",\\n    \"Irish\",\\n    \"Italian\",\\n    \"Japanese\",\\n    \"Javanese\",\\n    \"Kannada\",\\n    \"Kazakh\",\\n    \"Khmer\",\\n    \"Korean\",\\n    \"Kurdish\",\\n    \"Kyrgyz\",\\n    \"Lao\",\\n    \"Latin\",\\n    \"Latvian\",\\n    \"Lithuanian\",\\n    \"Malagasy\",\\n    \"Malay\",\\n    \"Malayalam\",\\n    \"Marathi\",\\n    \"Moldavian\",\\n    \"Mongolian\",\\n    \"Nepali\",\\n    \"Norwegian\",\\n    \"Persian\",\\n    \"Polish\",\\n    \"Portuguese\",\\n    \"Romanian\",\\n    \"Russian\",\\n    \"Sanskrit\",\\n    \"Serbian\",\\n    \"Sindhi\",\\n    \"Sinhala\",\\n    \"Slovak\",\\n    \"Slovenian\",\\n    \"Somali\",\\n    \"Spanish\",\\n    \"Swahili\",\\n    \"Swedish\",\\n    \"Tajik\",\\n    \"Tamil\",\\n    \"Telugu\",\\n    \"Thai\",\\n    \"Tibetan\",\\n    \"Turkish\",\\n    \"Ukrainian\",\\n    \"Urdu\",\\n    \"Uzbek\",\\n    \"Vietnamese\",\\n    \"Welsh\",\\n    \"Yiddish\"\\n  ],\\n  \"TunningInformation\": \"Contrastive pre-training with weak supervision, supervised fine-tuning\",\\n  \"TrainingData\": \"Filtered mC4 (title, page content), 400M CC News (title, news content), NLLB translation pairs, Wikipedia (hierarchical section title, passage), Filtered Reddit (comment, response), S2ORC (title, abstract) and citation pairs, Stackexchange (question, answer), xP3 (input prompt, response)\",\\n  \"UsesSupported\": [\\n    \"Feature extraction\",\\n    \"Sentence similarity\",\\n    \"Inference endpoints\"\\n  ],\\n  \"OptimisedFor\": \"Multilingual understanding\",\\n  \"PromptingAdvice\": \"Use \\'query:\\' and \\'passage:\\' prefixes for input texts\",\\n  \"Output\": \"Sentence embeddings\",\\n  \"PromptTuningAvailability\": true,\\n  \"RegionalAvailability\": \"Not specified\",\\n  \"License\": \"MIT\"\\n}\\n```\\n\\nPlease note that the list of languages is not exhaustive and represents the languages supported by the XLM-Roberta model. The actual language support might vary depending on the specific model\\'s training data.',\n",
       " 'Here is the JSON file created using the provided information:\\n\\n```json\\n{\\n  \"ModelName\": \"Prometheus\",\\n  \"Size\": \"13B\",\\n  \"ID\": \"Prometheus-13b-v1.0\",\\n  \"Provider\": \"kaist-ai\",\\n  \"Architecture\": \"Transformers\",\\n  \"ContextLength\": 2048,\\n  \"Price\": 0.12,\\n  \"Languages\": [\"English\"],\\n  \"TunningInformation\": \"Fine-tuned on 100K feedback within the Feedback Collection\",\\n  \"TrainingData\": \"Not specified\",\\n  \"UsesSupported\": [\"Text2Text Generation\", \"Inference Endpoints\"],\\n  \"OptimisedFor\": \"Language model using Llama-2-Chat as a base model and fine-tuned for evaluating long-form responses\",\\n  \"PromptingAdvice\": \"Follow the specific prompt format required by the model\",\\n  \"Output\": \"Detailed feedback and a score between 1 and 5\",\\n  \"PromptTuningAvailability\": true,\\n  \"RegionalAvailability\": \"Not specified\",\\n  \"License\": \"Apache 2.0\"\\n}\\n```\\nPlease note that the `Price` field is just an example and may not reflect the actual cost of using the model.',\n",
       " 'Here is the JSON file created using the provided information:\\n\\n```json\\n{\\n  \"ModelName\": \"Granite 13 Billion Model (granite.13b)\",\\n  \"Size\": \"13 Billion\",\\n  \"ID\": \"Granite13B\",\\n  \"Provider\": \"IBM\",\\n  \"Architecture\": \"Decoder-only\",\\n  \"ContextLength\": 8000,\\n  \"Price\": 0.0,\\n  \"Languages\": [\"English\"],\\n  \"TunningInformation\": \"Supervised Fine-Tuning (SFT) and Contrastive Fine Tuning (CFT)\",\\n  \"TrainingData\": \"Internet, academic, code, legal, and finance data\",\\n  \"UsesSupported\": [\"Question and answer\", \"Generation\", \"Extraction\", \"Summarization\", \"Classification\"],\\n  \"OptimisedFor\": \"Business use\",\\n  \"PromptingAdvice\": \"Follow the guidelines provided by IBM\",\\n  \"Output\": \"Text generation\",\\n  \"PromptTuningAvailability\": true,\\n  \"RegionalAvailability\": \"Worldwide\",\\n  \"License\": \"Available only through IBM products and offerings. Contact IBM for licensing terms.\"\\n}\\n```\\nPlease note that the `ContextLength` field is set to 8000 based on the information provided, but it can be adjusted according to your needs.',\n",
       " 'Here is the JSON file created using the provided information:\\n\\n```json\\n{\\n  \"ModelName\": \"Granite (13B) Instruct V2.0\",\\n  \"Size\": \"13 Billion\",\\n  \"ID\": \"granite.13b.v2.instruct\",\\n  \"Provider\": \"IBM\",\\n  \"Architecture\": \"Decoder-only\",\\n  \"ContextLength\": 8192,\\n  \"Price\": \"Not specified\",\\n  \"Languages\": [\"English\"],\\n  \"TunningInformation\": \"Instruction-tuned\",\\n  \"TrainingData\": \"2.5T Tokens, MMLU (5-shot), granite.13b.2500b.chat\",\\n  \"UsesSupported\": [\"English based classification\", \"extraction\", \"summarization\"],\\n  \"OptimisedFor\": \"Not specified\",\\n  \"PromptingAdvice\": \"The model benefits from flan-templated data in its alignment step and therefore should perform well with flan-style prompt templates.\",\\n  \"Output\": \"Not specified\",\\n  \"PromptTuningAvailability\": true,\\n  \"RegionalAvailability\": \"Not specified\",\\n  \"License\": \"Available only through IBM products and offerings.\"\\n}\\n```\\nPlease note that some fields like \"Price\" and \"OptimisedFor\" are left as \"Not specified\" because the provided information did not include these details.',\n",
       " 'Here is the JSON file created using the provided information:\\n\\n```json\\n{\\n  \"ModelName\": \"Granite Chat\",\\n  \"Size\": \"13 Billion\",\\n  \"ID\": \"granite-13b-chat-v2\",\\n  \"Provider\": \"IBM\",\\n  \"Architecture\": \"Decoder-only\",\\n  \"ContextLength\": 8000,\\n  \"Price\": \"Contact IBM for licensing terms\",\\n  \"Languages\": [\"English\"],\\n  \"TunningInformation\": \"Massive Multitask Language Understanding (MMLU) and Multi-Turn(MT) Bench benchmarks, novel alignment technique for LLMs using large-scale targeted alignment for a generalist LLM\",\\n  \"TrainingData\": \"2.5 Trillion Tokens, IBM\\'s curated pre-training dataset\",\\n  \"UsesSupported\": [\"Question and Answering (RAG)\", \"summarization\", \"generation\", \"extraction\", \"classification\"],\\n  \"OptimisedFor\": \"English-based closed-domain Question and Answering, longer responses preferred in RAG-like use cases\",\\n  \"PromptingAdvice\": \"Proper English text, no tagging or media, text only evaluation datasets\",\\n  \"Output\": \"English text\",\\n  \"PromptTuningAvailability\": false,\\n  \"RegionalAvailability\": \"Not specified\",\\n  \"License\": \"Available only through IBM products and offerings. Contact IBM for licensing terms.\"\\n}\\n```\\nPlease note that the `Price` field is set to \"Contact IBM for licensing terms\" as per your instructions, but it might be more appropriate to provide a specific price or pricing structure if that information is available.']"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def delete_before_curly_bracket(input_string):\n",
    "    # Define a regular expression pattern to match everything before the curly bracket\n",
    "    pattern = re.compile(r'.*?({.*)', re.DOTALL)\n",
    "    \n",
    "    # Use the pattern to find the matching substring\n",
    "    match = pattern.search(input_string)\n",
    "    \n",
    "    if match:\n",
    "        # Return the substring starting from the curly bracket\n",
    "        return match.group(1)\n",
    "    else:\n",
    "        # If no match is found, return the original string\n",
    "        return input_string\n",
    "    \n",
    "def delete_after_last_curly_bracket(input_string):\n",
    "    # Define a regular expression pattern to match everything after the last curly bracket\n",
    "    pattern = re.compile(r'(.*})(?:.*)', re.DOTALL)\n",
    "    \n",
    "    # Use the pattern to find the matching substring\n",
    "    match = pattern.search(input_string)\n",
    "    \n",
    "    if match:\n",
    "        # Return the substring up to the last curly bracket\n",
    "        return match.group(1)\n",
    "    else:\n",
    "        # If no match is found, return the original string\n",
    "        return input_string\n",
    "    \n",
    "def delete_before_modelname(input_string):\n",
    "    # Define a regular expression pattern to match everything before the curly bracket\n",
    "    pattern = re.compile(r'.*?({.*)', re.DOTALL)\n",
    "    \n",
    "    # Use the pattern to find the matching substring\n",
    "    match = pattern.search(input_string)\n",
    "    \n",
    "    if match:\n",
    "        # Return the substring starting from the curly bracket\n",
    "        return match.group(1)\n",
    "    else:\n",
    "        # If no match is found, return the original string\n",
    "        return input_string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_json = []\n",
    "for row in model_rows:\n",
    "    clean_row = delete_before_curly_bracket(row)\n",
    "    final_row = delete_after_last_curly_bracket(clean_row)\n",
    "    clean_json.append(final_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'clean_json' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mclean_json\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'clean_json' is not defined"
     ]
    }
   ],
   "source": [
    "clean_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "finished_json = []\n",
    "for row in clean_json:\n",
    "    formated_row = json.dumps(row)\n",
    "    finished_json.append(formated_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'finished_json' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# List where each item is a valid JSON object representing a model\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mfinished_json\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'finished_json' is not defined"
     ]
    }
   ],
   "source": [
    "# List where each item is a valid JSON object representing a model\n",
    "finished_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_data = json.dumps(clean_json)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[\"{\\\\n  \\\\\"ModelName\\\\\": \\\\\"Code Llama\\\\\",\\\\n  \\\\\"Size\\\\\": \\\\\"70B\\\\\",\\\\n  \\\\\"ID\\\\\": \\\\\"CodeLlama - 70B\\\\\",\\\\n  \\\\\"Provider\\\\\": \\\\\"Meta\\\\\",\\\\n  \\\\\"Architecture\\\\\": \\\\\"Decoder-only\\\\\",\\\\n  \\\\\"ContextLength\\\\\": 100000,\\\\n  \\\\\"Price\\\\\": \\\\\"Free\\\\\",\\\\n  \\\\\"Languages\\\\\": [\\\\\"Python\\\\\", \\\\\"C++\\\\\", \\\\\"Java\\\\\", \\\\\"PHP\\\\\", \\\\\"Typescript (Javascript)\\\\\", \\\\\"C#\\\\\", \\\\\"Bash\\\\\"],\\\\n  \\\\\"TunningInformation\\\\\": \\\\\"Instruction-tuned\\\\\",\\\\n  \\\\\"TrainingData\\\\\": \\\\\"Code and code-related data\\\\\",\\\\n  \\\\\"UsesSupported\\\\\": [\\\\\"Code generation\\\\\", \\\\\"natural language about code\\\\\", \\\\\"debugging\\\\\"],\\\\n  \\\\\"OptimisedFor\\\\\": \\\\\"Low latency, real-time code completion, long inputs\\\\\",\\\\n  \\\\\"PromptingAdvice\\\\\": \\\\\"Provide the model with code or natural language prompts\\\\\",\\\\n  \\\\\"Output\\\\\": \\\\\"Code and natural language about code\\\\\",\\\\n  \\\\\"PromptTuningAvailability\\\\\": true,\\\\n  \\\\\"RegionalAvailability\\\\\": \\\\\"Not specified\\\\\",\\\\n  \\\\\"License\\\\\": \\\\\"Community license as Llama 2\\\\\"\\\\n}\", \"{\\\\n  \\\\\"ModelName\\\\\": \\\\\"Deepseek Coder\\\\\",\\\\n  \\\\\"Size\\\\\": \\\\\"33B\\\\\",\\\\n  \\\\\"ID\\\\\": \\\\\"deepseek-coder-33b-instruct\\\\\",\\\\n  \\\\\"Provider\\\\\": \\\\\"deepseek-ai\\\\\",\\\\n  \\\\\"Architecture\\\\\": \\\\\"Decoder-only\\\\\",\\\\n  \\\\\"ContextLength\\\\\": 16000,\\\\n  \\\\\"Price\\\\\": \\\\\"Not specified\\\\\",\\\\n  \\\\\"Languages\\\\\": [\\\\\"Python\\\\\", \\\\\"English\\\\\", \\\\\"Chinese\\\\\"],\\\\n  \\\\\"TunningInformation\\\\\": \\\\\"Instruction-tuned\\\\\",\\\\n  \\\\\"TrainingData\\\\\": \\\\\"Project-level code corpus\\\\\",\\\\n  \\\\\"UsesSupported\\\\\": [\\\\\"Code generation\\\\\", \\\\\"Code completion\\\\\", \\\\\"Code infilling\\\\\"],\\\\n  \\\\\"OptimisedFor\\\\\": \\\\\"Project-level code completion and infilling\\\\\",\\\\n  \\\\\"PromptingAdvice\\\\\": \\\\\"Provide the model with code or natural language prompts\\\\\",\\\\n  \\\\\"Output\\\\\": \\\\\"Code and natural language\\\\\",\\\\n  \\\\\"PromptTuningAvailability\\\\\": false,\\\\n  \\\\\"RegionalAvailability\\\\\": \\\\\"Not specified\\\\\",\\\\n  \\\\\"License\\\\\": \\\\\"Other (see the LICENSE-MODEL for more details)\\\\\"\\\\n}\", \"{\\\\n  \\\\\"ModelName\\\\\": \\\\\"Falcon-180B\\\\\",\\\\n  \\\\\"Size\\\\\": \\\\\"180B\\\\\",\\\\n  \\\\\"ID\\\\\": \\\\\"tiiuae/falcon-180B\\\\\",\\\\n  \\\\\"Provider\\\\\": \\\\\"TII\\\\\",\\\\n  \\\\\"Architecture\\\\\": \\\\\"Decoder-only\\\\\",\\\\n  \\\\\"ContextLength\\\\\": 2048,\\\\n  \\\\\"Price\\\\\": \\\\\"Unknown\\\\\",\\\\n  \\\\\"Languages\\\\\": [\\\\\"English\\\\\", \\\\\"German\\\\\", \\\\\"Spanish\\\\\", \\\\\"French\\\\\"],\\\\n  \\\\\"TunningInformation\\\\\": \\\\\"Raw, pretrained model\\\\\",\\\\n  \\\\\"TrainingData\\\\\": \\\\\"RefinedWeb enhanced with curated corpora\\\\\",\\\\n  \\\\\"UsesSupported\\\\\": [\\\\\"Research\\\\\", \\\\\"Finetuning\\\\\", \\\\\"Specialization\\\\\"],\\\\n  \\\\\"OptimisedFor\\\\\": \\\\\"Inference\\\\\",\\\\n  \\\\\"PromptingAdvice\\\\\": \\\\\"Consider finetuning for specific usecases\\\\\",\\\\n  \\\\\"Output\\\\\": \\\\\"Text generation\\\\\",\\\\n  \\\\\"PromptTuningAvailability\\\\\": false,\\\\n  \\\\\"RegionalAvailability\\\\\": \\\\\"Publicly accessible\\\\\",\\\\n  \\\\\"License\\\\\": \\\\\"Falcon-180B TII License and Acceptable Use Policy\\\\\"\\\\n}\", \"{\\\\n  \\\\\"ModelName\\\\\": \\\\\"Falcon-40B\\\\\",\\\\n  \\\\\"Size\\\\\": \\\\\"40B\\\\\",\\\\n  \\\\\"ID\\\\\": \\\\\"tiiuae/falcon-40b\\\\\",\\\\n  \\\\\"Provider\\\\\": \\\\\"TII\\\\\",\\\\n  \\\\\"Architecture\\\\\": \\\\\"Decoder-only\\\\\",\\\\n  \\\\\"ContextLength\\\\\": 2048,\\\\n  \\\\\"Price\\\\\": \\\\\"Not specified\\\\\",\\\\n  \\\\\"Languages\\\\\": [\\\\\"English\\\\\", \\\\\"German\\\\\", \\\\\"Spanish\\\\\", \\\\\"French\\\\\", \\\\\"Italian\\\\\", \\\\\"Portuguese\\\\\", \\\\\"Polish\\\\\", \\\\\"Dutch\\\\\", \\\\\"Romanian\\\\\", \\\\\"Czech\\\\\", \\\\\"Swedish\\\\\"],\\\\n  \\\\\"TunningInformation\\\\\": \\\\\"Raw, pretrained model\\\\\",\\\\n  \\\\\"TrainingData\\\\\": \\\\\"1,000B tokens of RefinedWeb enhanced with curated corpora\\\\\",\\\\n  \\\\\"UsesSupported\\\\\": [\\\\\"Research\\\\\", \\\\\"Finetuning\\\\\", \\\\\"Specialization\\\\\"],\\\\n  \\\\\"OptimisedFor\\\\\": \\\\\"Inference\\\\\",\\\\n  \\\\\"PromptingAdvice\\\\\": \\\\\"Consider finetuning for specific usecases\\\\\",\\\\n  \\\\\"Output\\\\\": \\\\\"Text generation\\\\\",\\\\n  \\\\\"PromptTuningAvailability\\\\\": false,\\\\n  \\\\\"RegionalAvailability\\\\\": \\\\\"Not specified\\\\\",\\\\n  \\\\\"License\\\\\": \\\\\"Apache 2.0 license\\\\\"\\\\n}\", \"{\\\\n  \\\\\"ModelName\\\\\": \\\\\"FLAN-T5 XL\\\\\",\\\\n  \\\\\"Size\\\\\": \\\\\"2.85B params\\\\\",\\\\n  \\\\\"ID\\\\\": \\\\\"google/flan-t5-xl\\\\\",\\\\n  \\\\\"Provider\\\\\": \\\\\"Hugging Face\\\\\",\\\\n  \\\\\"Architecture\\\\\": \\\\\"Transformers\\\\\",\\\\n  \\\\\"ContextLength\\\\\": 512,\\\\n  \\\\\"Price\\\\\": \\\\\"Not specified\\\\\",\\\\n  \\\\\"Languages\\\\\": [\\\\\"English\\\\\", \\\\\"Spanish\\\\\", \\\\\"Japanese\\\\\", \\\\\"Persian\\\\\", \\\\\"Hindi\\\\\", \\\\\"French\\\\\", \\\\\"Chinese\\\\\", \\\\\"Bengali\\\\\", \\\\\"Gujarati\\\\\", \\\\\"German\\\\\", \\\\\"Telugu\\\\\", \\\\\"Italian\\\\\", \\\\\"Arabic\\\\\", \\\\\"Polish\\\\\", \\\\\"Tamil\\\\\", \\\\\"Marathi\\\\\", \\\\\"Malayalam\\\\\", \\\\\"Oriya\\\\\", \\\\\"Panjabi\\\\\", \\\\\"Portuguese\\\\\", \\\\\"Urdu\\\\\", \\\\\"Galician\\\\\", \\\\\"Hebrew\\\\\", \\\\\"Korean\\\\\", \\\\\"Catalan\\\\\", \\\\\"Thai\\\\\", \\\\\"Dutch\\\\\", \\\\\"Indonesian\\\\\", \\\\\"Vietnamese\\\\\", \\\\\"Bulgarian\\\\\", \\\\\"Filipino\\\\\", \\\\\"Central Khmer\\\\\", \\\\\"Lao\\\\\", \\\\\"Turkish\\\\\", \\\\\"Russian\\\\\", \\\\\"Croatian\\\\\", \\\\\"Swedish\\\\\", \\\\\"Yoruba\\\\\", \\\\\"Kurdish\\\\\", \\\\\"Burmese\\\\\", \\\\\"Malay\\\\\", \\\\\"Czech\\\\\", \\\\\"Finnish\\\\\", \\\\\"Somali\\\\\", \\\\\"Tagalog\\\\\", \\\\\"Swahili\\\\\", \\\\\"Sinhala\\\\\", \\\\\"Kannada\\\\\", \\\\\"Zhuang\\\\\", \\\\\"Igbo\\\\\", \\\\\"Xhosa\\\\\", \\\\\"Romanian\\\\\", \\\\\"Haitian\\\\\", \\\\\"Estonian\\\\\", \\\\\"Slovak\\\\\", \\\\\"Lithuanian\\\\\", \\\\\"Greek\\\\\", \\\\\"Nepali\\\\\", \\\\\"Assamese\\\\\", \\\\\"Norwegian\\\\\"],\\\\n  \\\\\"TunningInformation\\\\\": \\\\\"Instruction-tuned\\\\\",\\\\n  \\\\\"TrainingData\\\\\": \\\\\"A mixture of tasks including those in the table below (from the original paper, figure 2)\\\\\",\\\\n  \\\\\"UsesSupported\\\\\": [\\\\\"Research on zero-shot NLP tasks and in-context few-shot learning\\\\\", \\\\\"NLP tasks such as reasoning, and question answering\\\\\", \\\\\"Advancing fairness and safety research\\\\\", \\\\\"Understanding limitations of current large language models\\\\\"],\\\\n  \\\\\"OptimisedFor\\\\\": \\\\\"Not specified\\\\\",\\\\n  \\\\\"PromptingAdvice\\\\\": \\\\\"Provide the model with appropriate prompts\\\\\",\\\\n  \\\\\"Output\\\\\": \\\\\"Text\\\\\",\\\\n  \\\\\"PromptTuningAvailability\\\\\": true,\\\\n  \\\\\"RegionalAvailability\\\\\": \\\\\"Not specified\\\\\",\\\\n  \\\\\"License\\\\\": \\\\\"Apache 2.0\\\\\"\\\\n}\", \"{\\\\n  \\\\\"ModelName\\\\\": \\\\\"FLAN-T5 XXL\\\\\",\\\\n  \\\\\"Size\\\\\": \\\\\"11.3B params\\\\\",\\\\n  \\\\\"ID\\\\\": \\\\\"google/flan-t5-xxl\\\\\",\\\\n  \\\\\"Provider\\\\\": \\\\\"Google\\\\\",\\\\n  \\\\\"Architecture\\\\\": \\\\\"Transformer\\\\\",\\\\n  \\\\\"ContextLength\\\\\": 512,\\\\n  \\\\\"Price\\\\\": \\\\\"Not specified\\\\\",\\\\n  \\\\\"Languages\\\\\": [\\\\\"English\\\\\", \\\\\"German\\\\\", \\\\\"French\\\\\"],\\\\n  \\\\\"TunningInformation\\\\\": \\\\\"Instruction-tuned\\\\\",\\\\n  \\\\\"TrainingData\\\\\": \\\\\"A mixture of tasks including reasoning, question answering, and more\\\\\",\\\\n  \\\\\"UsesSupported\\\\\": [\\\\\"Research on language models\\\\\", \\\\\"Zero-shot NLP tasks\\\\\", \\\\\"In-context few-shot learning\\\\\", \\\\\"Advancing fairness and safety research\\\\\"],\\\\n  \\\\\"OptimisedFor\\\\\": \\\\\"Not specified\\\\\",\\\\n  \\\\\"PromptingAdvice\\\\\": \\\\\"Not specified\\\\\",\\\\n  \\\\\"Output\\\\\": \\\\\"Language model generation\\\\\",\\\\n  \\\\\"PromptTuningAvailability\\\\\": false,\\\\n  \\\\\"RegionalAvailability\\\\\": \\\\\"Not specified\\\\\",\\\\n  \\\\\"License\\\\\": \\\\\"Apache 2.0\\\\\"\\\\n}\", \"{\\\\n  \\\\\"ModelName\\\\\": \\\\\"Flan-UL2\\\\\",\\\\n  \\\\\"Size\\\\\": \\\\\"20B\\\\\",\\\\n  \\\\\"ID\\\\\": \\\\\"google/flan-ul2\\\\\",\\\\n  \\\\\"Provider\\\\\": \\\\\"Google\\\\\",\\\\n  \\\\\"Architecture\\\\\": \\\\\"Encoder-decoder\\\\\",\\\\n  \\\\\"ContextLength\\\\\": 2048,\\\\n  \\\\\"Price\\\\\": \\\\\"Not specified\\\\\",\\\\n  \\\\\"Languages\\\\\": [\\\\\"English\\\\\", \\\\\"French\\\\\", \\\\\"German\\\\\", \\\\\"Spanish\\\\\", \\\\\"Italian\\\\\"],\\\\n  \\\\\"TunningInformation\\\\\": \\\\\"Flan Prompting\\\\\",\\\\n  \\\\\"TrainingData\\\\\": \\\\\"C4, Flan datasets\\\\\",\\\\n  \\\\\"UsesSupported\\\\\": [\\\\\"Text generation\\\\\", \\\\\"Question answering\\\\\", \\\\\"Translation\\\\\", \\\\\"Summarization\\\\\"],\\\\n  \\\\\"OptimisedFor\\\\\": \\\\\"Few-shot in-context learning\\\\\",\\\\n  \\\\\"PromptingAdvice\\\\\": \\\\\"Provide the model with instruction-based prompts\\\\\",\\\\n  \\\\\"Output\\\\\": \\\\\"Text\\\\\",\\\\n  \\\\\"PromptTuningAvailability\\\\\": true,\\\\n  \\\\\"RegionalAvailability\\\\\": \\\\\"Not specified\\\\\",\\\\n  \\\\\"License\\\\\": \\\\\"Apache-2.0\\\\\"\\\\n}\", \"{\\\\n  \\\\\"ModelName\\\\\": \\\\\"Jais-13b-chat\\\\\",\\\\n  \\\\\"Size\\\\\": \\\\\"13B\\\\\",\\\\n  \\\\\"ID\\\\\": \\\\\"jais-13b-chat\\\\\",\\\\n  \\\\\"Provider\\\\\": \\\\\"core42\\\\\",\\\\n  \\\\\"Architecture\\\\\": \\\\\"Decoder-only\\\\\",\\\\n  \\\\\"ContextLength\\\\\": 2048,\\\\n  \\\\\"Price\\\\\": \\\\\"Not specified\\\\\",\\\\n  \\\\\"Languages\\\\\": [\\\\\"Arabic\\\\\", \\\\\"English\\\\\"],\\\\n  \\\\\"TunningInformation\\\\\": \\\\\"Finetuned\\\\\",\\\\n  \\\\\"TrainingData\\\\\": \\\\\"Arabic and English prompt-response pairs\\\\\",\\\\n  \\\\\"UsesSupported\\\\\": [\\\\\"Conversational AI\\\\\"],\\\\n  \\\\\"OptimisedFor\\\\\": \\\\\"Bilingual conversations\\\\\",\\\\n  \\\\\"PromptingAdvice\\\\\": \\\\\"Follow the specific prompt format\\\\\",\\\\n  \\\\\"Output\\\\\": \\\\\"Text responses\\\\\",\\\\n  \\\\\"PromptTuningAvailability\\\\\": false,\\\\n  \\\\\"RegionalAvailability\\\\\": \\\\\"Not specified\\\\\",\\\\n  \\\\\"License\\\\\": \\\\\"Apache 2.0\\\\\"\\\\n}\", \"{\\\\n  \\\\\"ModelName\\\\\": \\\\\"ELYZA-japanese-Llama-2-7b-instruct\\\\\",\\\\n  \\\\\"Size\\\\\": \\\\\"6.27B\\\\\",\\\\n  \\\\\"ID\\\\\": \\\\\"ELYZA-japanese-Llama-2-7b-instruct\\\\\",\\\\n  \\\\\"Provider\\\\\": \\\\\"Meta\\\\\",\\\\n  \\\\\"Architecture\\\\\": \\\\\"Decoder-only\\\\\",\\\\n  \\\\\"ContextLength\\\\\": 256,\\\\n  \\\\\"Price\\\\\": \\\\\"Not specified\\\\\",\\\\n  \\\\\"Languages\\\\\": [\\\\\"Japanese\\\\\", \\\\\"English\\\\\"],\\\\n  \\\\\"TunningInformation\\\\\": \\\\\"Instruction-tuned\\\\\",\\\\n  \\\\\"TrainingData\\\\\": \\\\\"Not specified\\\\\",\\\\n  \\\\\"UsesSupported\\\\\": [\\\\\"Text generation\\\\\"],\\\\n  \\\\\"OptimisedFor\\\\\": \\\\\"Not specified\\\\\",\\\\n  \\\\\"PromptingAdvice\\\\\": \\\\\"Provide the model with text prompts\\\\\",\\\\n  \\\\\"Output\\\\\": \\\\\"Text\\\\\",\\\\n  \\\\\"PromptTuningAvailability\\\\\": false,\\\\n  \\\\\"RegionalAvailability\\\\\": \\\\\"Not specified\\\\\",\\\\n  \\\\\"License\\\\\": \\\\\"Llama 2 Community License\\\\\"\\\\n}\", \"{\\\\n  \\\\\"ModelName\\\\\": \\\\\"Llama-2\\\\\",\\\\n  \\\\\"Size\\\\\": \\\\\"13B\\\\\",\\\\n  \\\\\"ID\\\\\": \\\\\"meta-llama/Llama-2-13b-hf\\\\\",\\\\n  \\\\\"Provider\\\\\": \\\\\"Meta\\\\\",\\\\n  \\\\\"Architecture\\\\\": \\\\\"Transformer\\\\\",\\\\n  \\\\\"ContextLength\\\\\": 4096,\\\\n  \\\\\"Price\\\\\": \\\\\"Not specified\\\\\",\\\\n  \\\\\"Languages\\\\\": [\\\\\"English\\\\\"],\\\\n  \\\\\"TunningInformation\\\\\": \\\\\"Supervised fine-tuning (SFT) and reinforcement learning with human feedback (RLHF)\\\\\",\\\\n  \\\\\"TrainingData\\\\\": \\\\\"A new mix of publicly available online data\\\\\",\\\\n  \\\\\"UsesSupported\\\\\": [\\\\\"Assistant-like chat\\\\\"],\\\\n  \\\\\"OptimisedFor\\\\\": \\\\\"Not specified\\\\\",\\\\n  \\\\\"PromptingAdvice\\\\\": \\\\\"Follow specific formatting, including INST and > tags, BOS and EOS tokens, and whitespaces and breaklines\\\\\",\\\\n  \\\\\"Output\\\\\": \\\\\"Text\\\\\",\\\\n  \\\\\"PromptTuningAvailability\\\\\": false,\\\\n  \\\\\"RegionalAvailability\\\\\": \\\\\"Not specified\\\\\",\\\\n  \\\\\"License\\\\\": \\\\\"Custom commercial license available\\\\\"\\\\n}\", \"{\\\\n  \\\\\"ModelName\\\\\": \\\\\"llama2-dpo-v7\\\\\",\\\\n  \\\\\"Size\\\\\": \\\\\"13.2B\\\\\",\\\\n  \\\\\"ID\\\\\": \\\\\"mncai/llama2-13b-dpo-v7\\\\\",\\\\n  \\\\\"Provider\\\\\": \\\\\"MindsAndCompany\\\\\",\\\\n  \\\\\"Architecture\\\\\": \\\\\"Decoder-only\\\\\",\\\\n  \\\\\"ContextLength\\\\\": 2048,\\\\n  \\\\\"Price\\\\\": \\\\\"Not specified\\\\\",\\\\n  \\\\\"Languages\\\\\": [\\\\\"English\\\\\", \\\\\"Korean\\\\\"],\\\\n  \\\\\"TunningInformation\\\\\": \\\\\"Instruction-tuned and Differential Privacy Optimized\\\\\",\\\\n  \\\\\"TrainingData\\\\\": \\\\\"Not specified\\\\\",\\\\n  \\\\\"UsesSupported\\\\\": [\\\\\"Text generation\\\\\"],\\\\n  \\\\\"OptimisedFor\\\\\": \\\\\"Not specified\\\\\",\\\\n  \\\\\"PromptingAdvice\\\\\": \\\\\"Provide the model with text prompts\\\\\",\\\\n  \\\\\"Output\\\\\": \\\\\"Text\\\\\",\\\\n  \\\\\"PromptTuningAvailability\\\\\": false,\\\\n  \\\\\"RegionalAvailability\\\\\": \\\\\"Not specified\\\\\",\\\\n  \\\\\"License\\\\\": \\\\\"Llama 2 license\\\\\"\\\\n}\", \"{\\\\n  \\\\\"ModelName\\\\\": \\\\\"Mistral-7B-Instruct-v0.2\\\\\",\\\\n  \\\\\"Size\\\\\": \\\\\"7B\\\\\",\\\\n  \\\\\"ID\\\\\": \\\\\"Mistral-7B-Instruct-v0.2\\\\\",\\\\n  \\\\\"Provider\\\\\": \\\\\"Mistral AI\\\\\",\\\\n  \\\\\"Architecture\\\\\": \\\\\"Transformers\\\\\",\\\\n  \\\\\"ContextLength\\\\\": 32000,\\\\n  \\\\\"Price\\\\\": 2.09,\\\\n  \\\\\"Languages\\\\\": [\\\\\"English\\\\\"],\\\\n  \\\\\"TunningInformation\\\\\": \\\\\"Finetuned conversational inference\\\\\",\\\\n  \\\\\"TrainingData\\\\\": \\\\\"Text generation inference\\\\\",\\\\n  \\\\\"UsesSupported\\\\\": [\\\\\"Text generation\\\\\"],\\\\n  \\\\\"OptimisedFor\\\\\": \\\\\"Inference Endpoints\\\\\",\\\\n  \\\\\"PromptingAdvice\\\\\": \\\\\"Surround your prompt with [INST] and [/]\\\\\",\\\\n  \\\\\"Output\\\\\": \\\\\"Text\\\\\",\\\\n  \\\\\"PromptTuningAvailability\\\\\": true,\\\\n  \\\\\"RegionalAvailability\\\\\": \\\\\"Not specified\\\\\",\\\\n  \\\\\"License\\\\\": \\\\\"Apache-2.0\\\\\"\\\\n}\", \"{\\\\n  \\\\\"ModelName\\\\\": \\\\\"Mixtral-8x7B-Instruct-v0.1\\\\\",\\\\n  \\\\\"Size\\\\\": \\\\\"8x7B\\\\\",\\\\n  \\\\\"ID\\\\\": \\\\\"mistralai/Mixtral-8x7B-Instruct-v0.1\\\\\",\\\\n  \\\\\"Provider\\\\\": \\\\\"Mistral AI\\\\\",\\\\n  \\\\\"Architecture\\\\\": \\\\\"Sparse Mixture of Experts\\\\\",\\\\n  \\\\\"ContextLength\\\\\": 2048,\\\\n  \\\\\"Price\\\\\": 3.74,\\\\n  \\\\\"Languages\\\\\": [\\\\\"English\\\\\", \\\\\"French\\\\\", \\\\\"German\\\\\", \\\\\"Spanish\\\\\", \\\\\"Italian\\\\\"],\\\\n  \\\\\"TunningInformation\\\\\": \\\\\"Instruction-tuned\\\\\",\\\\n  \\\\\"TrainingData\\\\\": \\\\\"Not specified\\\\\",\\\\n  \\\\\"UsesSupported\\\\\": [\\\\\"Text generation\\\\\"],\\\\n  \\\\\"OptimisedFor\\\\\": \\\\\"Not specified\\\\\",\\\\n  \\\\\"PromptingAdvice\\\\\": \\\\\"Use the specific instruction format\\\\\",\\\\n  \\\\\"Output\\\\\": \\\\\"Text\\\\\",\\\\n  \\\\\"PromptTuningAvailability\\\\\": true,\\\\n  \\\\\"RegionalAvailability\\\\\": \\\\\"Not specified\\\\\",\\\\n  \\\\\"License\\\\\": \\\\\"Apache-2.0\\\\\"\\\\n}\", \"{\\\\n  \\\\\"ModelName\\\\\": \\\\\"Mixtral-8x7B\\\\\",\\\\n  \\\\\"Size\\\\\": \\\\\"8B\\\\\",\\\\n  \\\\\"ID\\\\\": \\\\\"TheBloke/Mixtral-8x7B-v0.1-GPTQ\\\\\",\\\\n  \\\\\"Provider\\\\\": \\\\\"Hugging Face\\\\\",\\\\n  \\\\\"Architecture\\\\\": \\\\\"Transformer\\\\\",\\\\n  \\\\\"ContextLength\\\\\": 4096,\\\\n  \\\\\"Price\\\\\": \\\\\"Not specified\\\\\",\\\\n  \\\\\"Languages\\\\\": [\\\\\"English\\\\\", \\\\\"French\\\\\", \\\\\"German\\\\\", \\\\\"Italian\\\\\", \\\\\"Spanish\\\\\"],\\\\n  \\\\\"TunningInformation\\\\\": \\\\\"GPTQ\\\\\",\\\\n  \\\\\"TrainingData\\\\\": \\\\\"Not specified\\\\\",\\\\n  \\\\\"UsesSupported\\\\\": [\\\\\"Text generation\\\\\"],\\\\n  \\\\\"OptimisedFor\\\\\": \\\\\"4-bit precision\\\\\",\\\\n  \\\\\"PromptingAdvice\\\\\": \\\\\"No specific template\\\\\",\\\\n  \\\\\"Output\\\\\": \\\\\"Text\\\\\",\\\\n  \\\\\"PromptTuningAvailability\\\\\": true,\\\\n  \\\\\"RegionalAvailability\\\\\": \\\\\"Not specified\\\\\",\\\\n  \\\\\"License\\\\\": \\\\\"Apache-2.0\\\\\"\\\\n}\", \"{\\\\n  \\\\\"ModelName\\\\\": \\\\\"Multilingual-E5-large\\\\\",\\\\n  \\\\\"Size\\\\\": \\\\\"24 layers, embedding size 1024\\\\\",\\\\n  \\\\\"ID\\\\\": \\\\\"intfloat/multilingual-e5-large\\\\\",\\\\n  \\\\\"Provider\\\\\": \\\\\"Hugging Face\\\\\",\\\\n  \\\\\"Architecture\\\\\": \\\\\"Transformer\\\\\",\\\\n  \\\\\"ContextLength\\\\\": 512,\\\\n  \\\\\"Price\\\\\": \\\\\"Not specified\\\\\",\\\\n  \\\\\"Languages\\\\\": [\\\\n    \\\\\"Afrikaans\\\\\",\\\\n    \\\\\"Albanian\\\\\",\\\\n    \\\\\"Amharic\\\\\",\\\\n    \\\\\"Arabic\\\\\",\\\\n    \\\\\"Armenian\\\\\",\\\\n    \\\\\"Assamese\\\\\",\\\\n    \\\\\"Azerbaijani\\\\\",\\\\n    \\\\\"Bashkir\\\\\",\\\\n    \\\\\"Basque\\\\\",\\\\n    \\\\\"Belarusian\\\\\",\\\\n    \\\\\"Bengali\\\\\",\\\\n    \\\\\"Bosnian\\\\\",\\\\n    \\\\\"Bulgarian\\\\\",\\\\n    \\\\\"Burmese\\\\\",\\\\n    \\\\\"Catalan\\\\\",\\\\n    \\\\\"Central Khmer\\\\\",\\\\n    \\\\\"Chinese\\\\\",\\\\n    \\\\\"Croatian\\\\\",\\\\n    \\\\\"Czech\\\\\",\\\\n    \\\\\"Danish\\\\\",\\\\n    \\\\\"Dutch\\\\\",\\\\n    \\\\\"English\\\\\",\\\\n    \\\\\"Esperanto\\\\\",\\\\n    \\\\\"Estonian\\\\\",\\\\n    \\\\\"Finnish\\\\\",\\\\n    \\\\\"French\\\\\",\\\\n    \\\\\"Galician\\\\\",\\\\n    \\\\\"Georgian\\\\\",\\\\n    \\\\\"German\\\\\",\\\\n    \\\\\"Greek\\\\\",\\\\n    \\\\\"Gujarati\\\\\",\\\\n    \\\\\"Hausa\\\\\",\\\\n    \\\\\"Hebrew\\\\\",\\\\n    \\\\\"Hindi\\\\\",\\\\n    \\\\\"Hungarian\\\\\",\\\\n    \\\\\"Icelandic\\\\\",\\\\n    \\\\\"Indonesian\\\\\",\\\\n    \\\\\"Interlingua\\\\\",\\\\n    \\\\\"Irish\\\\\",\\\\n    \\\\\"Italian\\\\\",\\\\n    \\\\\"Japanese\\\\\",\\\\n    \\\\\"Javanese\\\\\",\\\\n    \\\\\"Kannada\\\\\",\\\\n    \\\\\"Kazakh\\\\\",\\\\n    \\\\\"Khmer\\\\\",\\\\n    \\\\\"Korean\\\\\",\\\\n    \\\\\"Kurdish\\\\\",\\\\n    \\\\\"Kyrgyz\\\\\",\\\\n    \\\\\"Lao\\\\\",\\\\n    \\\\\"Latin\\\\\",\\\\n    \\\\\"Latvian\\\\\",\\\\n    \\\\\"Lithuanian\\\\\",\\\\n    \\\\\"Malagasy\\\\\",\\\\n    \\\\\"Malay\\\\\",\\\\n    \\\\\"Malayalam\\\\\",\\\\n    \\\\\"Marathi\\\\\",\\\\n    \\\\\"Moldavian\\\\\",\\\\n    \\\\\"Mongolian\\\\\",\\\\n    \\\\\"Nepali\\\\\",\\\\n    \\\\\"Norwegian\\\\\",\\\\n    \\\\\"Persian\\\\\",\\\\n    \\\\\"Polish\\\\\",\\\\n    \\\\\"Portuguese\\\\\",\\\\n    \\\\\"Romanian\\\\\",\\\\n    \\\\\"Russian\\\\\",\\\\n    \\\\\"Sanskrit\\\\\",\\\\n    \\\\\"Serbian\\\\\",\\\\n    \\\\\"Sindhi\\\\\",\\\\n    \\\\\"Sinhala\\\\\",\\\\n    \\\\\"Slovak\\\\\",\\\\n    \\\\\"Slovenian\\\\\",\\\\n    \\\\\"Somali\\\\\",\\\\n    \\\\\"Spanish\\\\\",\\\\n    \\\\\"Swahili\\\\\",\\\\n    \\\\\"Swedish\\\\\",\\\\n    \\\\\"Tajik\\\\\",\\\\n    \\\\\"Tamil\\\\\",\\\\n    \\\\\"Telugu\\\\\",\\\\n    \\\\\"Thai\\\\\",\\\\n    \\\\\"Tibetan\\\\\",\\\\n    \\\\\"Turkish\\\\\",\\\\n    \\\\\"Ukrainian\\\\\",\\\\n    \\\\\"Urdu\\\\\",\\\\n    \\\\\"Uzbek\\\\\",\\\\n    \\\\\"Vietnamese\\\\\",\\\\n    \\\\\"Welsh\\\\\",\\\\n    \\\\\"Yiddish\\\\\"\\\\n  ],\\\\n  \\\\\"TunningInformation\\\\\": \\\\\"Contrastive pre-training with weak supervision, supervised fine-tuning\\\\\",\\\\n  \\\\\"TrainingData\\\\\": \\\\\"Filtered mC4 (title, page content), 400M CC News (title, news content), NLLB translation pairs, Wikipedia (hierarchical section title, passage), Filtered Reddit (comment, response), S2ORC (title, abstract) and citation pairs, Stackexchange (question, answer), xP3 (input prompt, response)\\\\\",\\\\n  \\\\\"UsesSupported\\\\\": [\\\\n    \\\\\"Feature extraction\\\\\",\\\\n    \\\\\"Sentence similarity\\\\\",\\\\n    \\\\\"Inference endpoints\\\\\"\\\\n  ],\\\\n  \\\\\"OptimisedFor\\\\\": \\\\\"Multilingual understanding\\\\\",\\\\n  \\\\\"PromptingAdvice\\\\\": \\\\\"Use \\'query:\\' and \\'passage:\\' prefixes for input texts\\\\\",\\\\n  \\\\\"Output\\\\\": \\\\\"Sentence embeddings\\\\\",\\\\n  \\\\\"PromptTuningAvailability\\\\\": true,\\\\n  \\\\\"RegionalAvailability\\\\\": \\\\\"Not specified\\\\\",\\\\n  \\\\\"License\\\\\": \\\\\"MIT\\\\\"\\\\n}\", \"{\\\\n  \\\\\"ModelName\\\\\": \\\\\"Prometheus\\\\\",\\\\n  \\\\\"Size\\\\\": \\\\\"13B\\\\\",\\\\n  \\\\\"ID\\\\\": \\\\\"Prometheus-13b-v1.0\\\\\",\\\\n  \\\\\"Provider\\\\\": \\\\\"kaist-ai\\\\\",\\\\n  \\\\\"Architecture\\\\\": \\\\\"Transformers\\\\\",\\\\n  \\\\\"ContextLength\\\\\": 2048,\\\\n  \\\\\"Price\\\\\": 0.12,\\\\n  \\\\\"Languages\\\\\": [\\\\\"English\\\\\"],\\\\n  \\\\\"TunningInformation\\\\\": \\\\\"Fine-tuned on 100K feedback within the Feedback Collection\\\\\",\\\\n  \\\\\"TrainingData\\\\\": \\\\\"Not specified\\\\\",\\\\n  \\\\\"UsesSupported\\\\\": [\\\\\"Text2Text Generation\\\\\", \\\\\"Inference Endpoints\\\\\"],\\\\n  \\\\\"OptimisedFor\\\\\": \\\\\"Language model using Llama-2-Chat as a base model and fine-tuned for evaluating long-form responses\\\\\",\\\\n  \\\\\"PromptingAdvice\\\\\": \\\\\"Follow the specific prompt format required by the model\\\\\",\\\\n  \\\\\"Output\\\\\": \\\\\"Detailed feedback and a score between 1 and 5\\\\\",\\\\n  \\\\\"PromptTuningAvailability\\\\\": true,\\\\n  \\\\\"RegionalAvailability\\\\\": \\\\\"Not specified\\\\\",\\\\n  \\\\\"License\\\\\": \\\\\"Apache 2.0\\\\\"\\\\n}\", \"{\\\\n  \\\\\"ModelName\\\\\": \\\\\"Granite 13 Billion Model (granite.13b)\\\\\",\\\\n  \\\\\"Size\\\\\": \\\\\"13 Billion\\\\\",\\\\n  \\\\\"ID\\\\\": \\\\\"Granite13B\\\\\",\\\\n  \\\\\"Provider\\\\\": \\\\\"IBM\\\\\",\\\\n  \\\\\"Architecture\\\\\": \\\\\"Decoder-only\\\\\",\\\\n  \\\\\"ContextLength\\\\\": 8000,\\\\n  \\\\\"Price\\\\\": 0.0,\\\\n  \\\\\"Languages\\\\\": [\\\\\"English\\\\\"],\\\\n  \\\\\"TunningInformation\\\\\": \\\\\"Supervised Fine-Tuning (SFT) and Contrastive Fine Tuning (CFT)\\\\\",\\\\n  \\\\\"TrainingData\\\\\": \\\\\"Internet, academic, code, legal, and finance data\\\\\",\\\\n  \\\\\"UsesSupported\\\\\": [\\\\\"Question and answer\\\\\", \\\\\"Generation\\\\\", \\\\\"Extraction\\\\\", \\\\\"Summarization\\\\\", \\\\\"Classification\\\\\"],\\\\n  \\\\\"OptimisedFor\\\\\": \\\\\"Business use\\\\\",\\\\n  \\\\\"PromptingAdvice\\\\\": \\\\\"Follow the guidelines provided by IBM\\\\\",\\\\n  \\\\\"Output\\\\\": \\\\\"Text generation\\\\\",\\\\n  \\\\\"PromptTuningAvailability\\\\\": true,\\\\n  \\\\\"RegionalAvailability\\\\\": \\\\\"Worldwide\\\\\",\\\\n  \\\\\"License\\\\\": \\\\\"Available only through IBM products and offerings. Contact IBM for licensing terms.\\\\\"\\\\n}\", \"{\\\\n  \\\\\"ModelName\\\\\": \\\\\"Granite (13B) Instruct V2.0\\\\\",\\\\n  \\\\\"Size\\\\\": \\\\\"13 Billion\\\\\",\\\\n  \\\\\"ID\\\\\": \\\\\"granite.13b.v2.instruct\\\\\",\\\\n  \\\\\"Provider\\\\\": \\\\\"IBM\\\\\",\\\\n  \\\\\"Architecture\\\\\": \\\\\"Decoder-only\\\\\",\\\\n  \\\\\"ContextLength\\\\\": 8192,\\\\n  \\\\\"Price\\\\\": \\\\\"Not specified\\\\\",\\\\n  \\\\\"Languages\\\\\": [\\\\\"English\\\\\"],\\\\n  \\\\\"TunningInformation\\\\\": \\\\\"Instruction-tuned\\\\\",\\\\n  \\\\\"TrainingData\\\\\": \\\\\"2.5T Tokens, MMLU (5-shot), granite.13b.2500b.chat\\\\\",\\\\n  \\\\\"UsesSupported\\\\\": [\\\\\"English based classification\\\\\", \\\\\"extraction\\\\\", \\\\\"summarization\\\\\"],\\\\n  \\\\\"OptimisedFor\\\\\": \\\\\"Not specified\\\\\",\\\\n  \\\\\"PromptingAdvice\\\\\": \\\\\"The model benefits from flan-templated data in its alignment step and therefore should perform well with flan-style prompt templates.\\\\\",\\\\n  \\\\\"Output\\\\\": \\\\\"Not specified\\\\\",\\\\n  \\\\\"PromptTuningAvailability\\\\\": true,\\\\n  \\\\\"RegionalAvailability\\\\\": \\\\\"Not specified\\\\\",\\\\n  \\\\\"License\\\\\": \\\\\"Available only through IBM products and offerings.\\\\\"\\\\n}\", \"{\\\\n  \\\\\"ModelName\\\\\": \\\\\"Granite Chat\\\\\",\\\\n  \\\\\"Size\\\\\": \\\\\"13 Billion\\\\\",\\\\n  \\\\\"ID\\\\\": \\\\\"granite-13b-chat-v2\\\\\",\\\\n  \\\\\"Provider\\\\\": \\\\\"IBM\\\\\",\\\\n  \\\\\"Architecture\\\\\": \\\\\"Decoder-only\\\\\",\\\\n  \\\\\"ContextLength\\\\\": 8000,\\\\n  \\\\\"Price\\\\\": \\\\\"Contact IBM for licensing terms\\\\\",\\\\n  \\\\\"Languages\\\\\": [\\\\\"English\\\\\"],\\\\n  \\\\\"TunningInformation\\\\\": \\\\\"Massive Multitask Language Understanding (MMLU) and Multi-Turn(MT) Bench benchmarks, novel alignment technique for LLMs using large-scale targeted alignment for a generalist LLM\\\\\",\\\\n  \\\\\"TrainingData\\\\\": \\\\\"2.5 Trillion Tokens, IBM\\'s curated pre-training dataset\\\\\",\\\\n  \\\\\"UsesSupported\\\\\": [\\\\\"Question and Answering (RAG)\\\\\", \\\\\"summarization\\\\\", \\\\\"generation\\\\\", \\\\\"extraction\\\\\", \\\\\"classification\\\\\"],\\\\n  \\\\\"OptimisedFor\\\\\": \\\\\"English-based closed-domain Question and Answering, longer responses preferred in RAG-like use cases\\\\\",\\\\n  \\\\\"PromptingAdvice\\\\\": \\\\\"Proper English text, no tagging or media, text only evaluation datasets\\\\\",\\\\n  \\\\\"Output\\\\\": \\\\\"English text\\\\\",\\\\n  \\\\\"PromptTuningAvailability\\\\\": false,\\\\n  \\\\\"RegionalAvailability\\\\\": \\\\\"Not specified\\\\\",\\\\n  \\\\\"License\\\\\": \\\\\"Available only through IBM products and offerings. Contact IBM for licensing terms.\\\\\"\\\\n}\"]'"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# JSON string with all of the models\n",
    "json_data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
