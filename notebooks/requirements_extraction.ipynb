{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for OpenAI\n__root__\n  Did not find openai_api_key, please add an environment variable `OPENAI_API_KEY` which contains it, or pass `openai_api_key` as a named parameter. (type=value_error)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mibm_watson_machine_learning\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfoundation_models\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Model\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mibm_watson_machine_learning\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfoundation_models\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mextensions\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlangchain\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m WatsonxLLM\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mllms_initialisation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m credentials, project_id, greedy_params\n\u001b[1;32m      6\u001b[0m llama_70b_model \u001b[38;5;241m=\u001b[39m Model(\n\u001b[1;32m      7\u001b[0m     model_id \u001b[38;5;241m=\u001b[39m ModelTypes\u001b[38;5;241m.\u001b[39mLLAMA_2_70B_CHAT,\n\u001b[1;32m      8\u001b[0m     params \u001b[38;5;241m=\u001b[39m greedy_params,\n\u001b[1;32m      9\u001b[0m     credentials \u001b[38;5;241m=\u001b[39m credentials,\n\u001b[1;32m     10\u001b[0m     project_id \u001b[38;5;241m=\u001b[39m project_id\n\u001b[1;32m     11\u001b[0m )\n\u001b[1;32m     12\u001b[0m llama_70b \u001b[38;5;241m=\u001b[39m WatsonxLLM(llama_70b_model)\n",
      "File \u001b[0;32m~/Documents/GitHub/ModelRecommender/llms_initialisation.py:14\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mibm_watson_machine_learning\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetanames\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GenTextParamsMetaNames \u001b[38;5;28;01mas\u001b[39;00m GenParams\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m#Initialise GPT\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m llm \u001b[38;5;241m=\u001b[39m \u001b[43mOpenAI\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgpt-3.5-turbo-instruct\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;66;43;03m#  handle_parsing_errors=True\u001b[39;49;00m\n\u001b[1;32m     18\u001b[0m \u001b[43m                 \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m#Initialise WatsonX\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m#Unlike GPT, requires explicit retrieval of .env variables\u001b[39;00m\n\u001b[1;32m     22\u001b[0m credentials\u001b[38;5;241m=\u001b[39m{\n\u001b[1;32m     23\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapikey\u001b[39m\u001b[38;5;124m\"\u001b[39m: os\u001b[38;5;241m.\u001b[39mgetenv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWATSONX_API_KEY\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m     24\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124murl\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://us-south.ml.cloud.ibm.com\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     25\u001b[0m }\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/langchain_core/load/serializable.py:107\u001b[0m, in \u001b[0;36mSerializable.__init__\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 107\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    108\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lc_kwargs \u001b[38;5;241m=\u001b[39m kwargs\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pydantic/v1/main.py:341\u001b[0m, in \u001b[0;36mBaseModel.__init__\u001b[0;34m(__pydantic_self__, **data)\u001b[0m\n\u001b[1;32m    339\u001b[0m values, fields_set, validation_error \u001b[38;5;241m=\u001b[39m validate_model(__pydantic_self__\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m, data)\n\u001b[1;32m    340\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m validation_error:\n\u001b[0;32m--> 341\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m validation_error\n\u001b[1;32m    342\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    343\u001b[0m     object_setattr(__pydantic_self__, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__dict__\u001b[39m\u001b[38;5;124m'\u001b[39m, values)\n",
      "\u001b[0;31mValidationError\u001b[0m: 1 validation error for OpenAI\n__root__\n  Did not find openai_api_key, please add an environment variable `OPENAI_API_KEY` which contains it, or pass `openai_api_key` as a named parameter. (type=value_error)"
     ]
    }
   ],
   "source": [
    "from ibm_watson_machine_learning.foundation_models.utils.enums import ModelTypes\n",
    "from ibm_watson_machine_learning.foundation_models import Model\n",
    "from ibm_watson_machine_learning.foundation_models.extensions.langchain import WatsonxLLM\n",
    "from llms_initialisation import credentials, project_id, greedy_params\n",
    "\n",
    "llama_70b_model = Model(\n",
    "    model_id = ModelTypes.LLAMA_2_70B_CHAT,\n",
    "    params = greedy_params,\n",
    "    credentials = credentials,\n",
    "    project_id = project_id\n",
    ")\n",
    "llama_70b = WatsonxLLM(llama_70b_model)\n",
    "\n",
    "flan_ul2_model = Model(\n",
    "    model_id = ModelTypes.FLAN_UL2,\n",
    "    params = greedy_params,\n",
    "    credentials = credentials,\n",
    "    project_id = project_id\n",
    ")\n",
    "flan_ul2 = WatsonxLLM(flan_ul2_model)\n",
    "\n",
    "granite_chat_model = Model(\n",
    "    model_id = ModelTypes.GRANITE_13B_CHAT_V2,\n",
    "    params = greedy_params,\n",
    "    credentials = credentials,\n",
    "    project_id = project_id\n",
    ")\n",
    "granite_chat = WatsonxLLM(granite_chat_model)\n",
    "\n",
    "mixtral_model = Model(\n",
    "    model_id = 'ibm-mistralai/mixtral-8x7b-instruct-v01-q',\n",
    "    params = greedy_params,\n",
    "    credentials = credentials,\n",
    "    project_id = project_id\n",
    ")\n",
    "mixtral = WatsonxLLM(mixtral_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [flan_ul2, granite_chat, mixtral]\n",
    "model_names = [\"flan_ul2\", \"granite_chat\", \"mixtral\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# user_request = \"\"\"\n",
    "# A bank is receiving 100s of complaints a day.\n",
    "# The data is in CSV format, stored in the AW3 cloud environment.\n",
    "# They want to inject data into either an algorithm or an LLM, which can categorise the complaints into a list of 50 complaint types.\n",
    "# Model to categories the root cause issues.\n",
    "# \"\"\"\n",
    "user_request = \"\"\"\n",
    "I need a model that will help me summarise customer calls and then categorise them based on the type of complaint. I need multilingual support, especially German and English fluency.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "requirements_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Language: English'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "language_prompt_template = \"\"\" \n",
    "From the user input provided, state the language requirements of the user:\n",
    "Here is the user input: {user_request}.\n",
    "Only use the information given.\n",
    "Don't make anything up.\n",
    "\"\"\"\n",
    "\n",
    "language = \"Language: \" + flan_ul2.invoke(language_prompt_template.format(user_request=user_request))\n",
    "requirements_list.append(language)\n",
    "language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary: \n",
      "1. Data Retrieval:\n",
      "   - Retrieve the CSV data from the AW3 cloud environment.\n",
      "\n",
      "2. Data Pre-processing:\n",
      "   - Clean the data as required (remove any irrelevant information, handle missing values, etc.)\n",
      "   - Format the data into a suitable structure for input into the LLM.\n",
      "\n",
      "3. Model Training:\n",
      "   - Train an LLM or select an appropriate algorithm to categorise the complaints into 50 complaint types.\n",
      "\n",
      "4. Model Testing:\n",
      "   - Test the model with a subset of the data to ensure it is accurately categorising the complaints.\n",
      "\n",
      "5. Model Deployment:\n",
      "   - Deploy the model in a production environment where it can categorise incoming complaints.\n",
      "\n",
      "6. Monitoring and Maintenance:\n",
      "   - Monitor the model's performance over time and retrain as necessary to maintain high accuracy.\n",
      "\n",
      "7. Reporting:\n",
      "   - Provide regular reports on the model's performance, including accuracy metrics and any necessary improvements.\n"
     ]
    }
   ],
   "source": [
    "#### CAN'T GET THIS ONE WORKING GREAT YET, IGNORE\n",
    "\n",
    "# subtasks_template = \"\"\"\n",
    "# As a solution architect specialising in Large Language Model (LLM) workflows, your expertise is invaluable in dissecting user requirements into LLM tasks. \n",
    "# Please break down the user's request into a list of key requirements.\n",
    "\n",
    "# User Needs: {needs}\n",
    "# Please ensure your response is strictly based on the provided information without any assumptions or additions.\n",
    "\n",
    "# Tasks to be accomplished:\n",
    "# \"\"\"\n",
    "# summary = mixtral.invoke(subtasks_template.format(needs=user_request, \n",
    "#                                                             #  requirements=requirements_list\n",
    "#                                                              ))\n",
    "# print(\"Summary: \" + summary)\n",
    "\n",
    "# for model,name in zip(models, model_names):\n",
    "#     print(name + \": \" + model.invoke(match_template.format(needs=user_request, requirements=requirements_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Task(s): text classification'"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tasks_involved_prompt_template = \"\"\" \n",
    "Using the user input provided, state which of the following tasks are needed to achieve the user's goal:\n",
    "- text generation\n",
    "- text summarisation\n",
    "- text classification\n",
    "- question answering\n",
    "- retrieval augmented generation\n",
    "- translation\n",
    "- code\n",
    "\n",
    "Use step-by-step reasoning to determine your answer.\n",
    "\n",
    "Here is the user input: {user_request}.\n",
    "Only use the information given.\n",
    "Don't make anything up.\n",
    "\"\"\"\n",
    "\n",
    "tasks = \"Task(s): \" + flan_ul2.invoke(tasks_involved_prompt_template.format(user_request=user_request)) \n",
    "# requirements_list.append(tasks)\n",
    "tasks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Architecture: \n",
      "Encoder-only\n"
     ]
    }
   ],
   "source": [
    "#These recommendations were generated by GPT; they look right\n",
    "model_architecture_prompt_template = \"\"\" \n",
    "You are an expert in matching LLM model architectures to specific tasks.\n",
    "Use the following matches to output which model to use for this task: {task}.\n",
    "\n",
    "Text Generation: Decoder-only\n",
    "Text Summarization: Encoder-Decoder\n",
    "Text Classification: Encoder-only\n",
    "Question Answering: Encoder-only\n",
    "Retrieval Augmented Generation: Encoder-Decoder with External Retrieval\n",
    "Translation: Encoder-Decoder\n",
    "Code Generation: Encoder-Decoder\n",
    "\n",
    "Do not make anything up, only use the information given.\n",
    "If you don't know, say you don't know\n",
    "Only state the answer, don't explain your reasoning.\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "#Better than a look up table because string alterations (e.g. Q&A/RAG/Summarization) won't be an issue\n",
    "architecture = \"Model Architecture: \" + mixtral.invoke(model_architecture_prompt_template.format(task=tasks))\n",
    "print(architecture)\n",
    "\n",
    "requirements_list.append(architecture)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Industry: Banking'"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "industry_prompt_template = \"\"\" \n",
    "From the user input provided, state the industry of the use case:\n",
    "Here is the user input: {user_request}.\n",
    "Only use the information given.\n",
    "Don't make anything up.\n",
    "\"\"\"\n",
    "\n",
    "industry = \"Industry: \" + flan_ul2.invoke(industry_prompt_template.format(user_request=user_request))\n",
    "requirements_list.append(industry)\n",
    "industry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latency: \n",
      "Query to user:\n",
      "\"Is there a requirement for real-time interaction with the categorization model, or can it process data in batches?\"\n"
     ]
    }
   ],
   "source": [
    "latency_prompt_template = \"\"\" \n",
    "Deduce from the user input whether latency is a critical factor, such as in applications requiring real-time interaction versus batch processing tasks.\n",
    "\n",
    "Here is the user input: {user_request}.\n",
    "\n",
    "If you are unsure and need more information, write a query to the user requesting it.\n",
    "Don't make anything up.\n",
    "\"\"\"\n",
    "\n",
    "latency = \"Latency: \" + mixtral.invoke(latency_prompt_template.format(user_request=user_request))\n",
    "print(latency)\n",
    "\n",
    "if \"?\" not in latency:\n",
    "    requirements_list.append(latency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function Calling:\n",
      "The user requirements do not imply a need for the production of structured outputs. The user wants to inject data into an algorithm or a large language model (LLM) to categorize complaints into a list of 50 complaint types. The output of this process would likely be a list or a set of categories, not a structured output like JSON or YAML.\n"
     ]
    }
   ],
   "source": [
    "function_calling_prompt_template = \"\"\" \n",
    "Deduce whether or not user requirements provided imply a need for the production of structured outputs, such as JSON or YAML.\n",
    "Here is the user input: {user_request}.\n",
    "Only use the information given.\n",
    "Don't make anything up.\n",
    "\"\"\"\n",
    "\n",
    "funcCalling = \"Function Calling:\" + mixtral.invoke(function_calling_prompt_template.format(user_request=user_request))\n",
    "print(funcCalling)\n",
    "\n",
    "requirements_list.append(funcCalling)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context Length: not a priority\n"
     ]
    }
   ],
   "source": [
    "context_length_prompt_template = \"\"\" \n",
    "You are an expert on data processing.\n",
    "You must decide from the user input whether the task requires ingesting/ producing lots of text at once or whether it can be done in batches.\n",
    "\n",
    "Here is the user input: {user_request}.\n",
    "\n",
    "Your answer will decide whether the LLM used for data processing will require a large context window or not.\n",
    "Your answer should either state 'not a priority' if it can be done in batches, or 'large context window required' if this is the case.\n",
    "\n",
    "If it is not clear, you can ask the user a question to clarify.\n",
    "\n",
    "Only use the information given.\n",
    "Don't make anything up.\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "contextLength = \"Context Length: \" + flan_ul2.invoke(context_length_prompt_template.format(user_request=user_request))\n",
    "print(contextLength)\n",
    "\n",
    "requirements_list.append(contextLength)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Language: English', 'Task(s): text classification', 'Function Calling:\\nThe user requirements do not imply a need for the production of structured outputs. The user wants to inject data into an algorithm or a large language model (LLM) to categorize complaints into a list of 50 complaint types. The output of this process would likely be a list or a set of categories, not a structured output like JSON or YAML.', 'Context Length: not a priority']\n"
     ]
    }
   ],
   "source": [
    "print(requirements_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key Words: Complaints\n",
      "CSV\n",
      "AW3\n",
      "Algorithm\n",
      "LLM\n",
      "Categorize\n",
      "Root Cause Issues\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "keyWords_template = \"\"\"\n",
    "Identify key-words or phrases that characterise this set of user needs and requirements:\n",
    "User Needs: {needs}\n",
    "\n",
    "Please ensure your response is strictly based on the provided information without any assumptions or additions.\n",
    "\n",
    "Key Words:\n",
    "\"\"\"\n",
    "\n",
    "# Requirements: {requirements}\n",
    "\n",
    "key_words = llama_70b.invoke(keyWords_template.format(needs=user_request, \n",
    "                                                    # requirements=requirements_list\n",
    "                                                             ))\n",
    "\n",
    "print(\"Key Words: \" + key_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'original_request': '\\nA bank is receiving 100s of complaints a day.\\nThe data is in CSV format, stored in the AW3 cloud environment.\\nThey want to inject data into either an algorithm or an LLM, which can categorise the complaints into a list of 50 complaint types.\\nModel to categories the root cause issues.\\n',\n",
       " 'requirements': ['Language: English',\n",
       "  'Task(s): text classification',\n",
       "  'Function Calling:\\nThe user requirements do not imply a need for the production of structured outputs. The user wants to inject data into an algorithm or a large language model (LLM) to categorize complaints into a list of 50 complaint types. The output of this process would likely be a list or a set of categories, not a structured output like JSON or YAML.',\n",
       "  'Context Length: not a priority',\n",
       "  'Model Architecture: \\nEncoder-only',\n",
       "  'Industry: Banking'],\n",
       " 'key_words': 'Complaints\\nCSV\\nAW3\\nAlgorithm\\nLLM\\nCategorize\\nRoot Cause Issues\\n\\n\\n\\n\\n\\n'}"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_info_dict = {\n",
    "    \"original_request\": user_request,\n",
    "    \"requirements\": requirements_list,\n",
    "    \"key_words\": key_words\n",
    "}\n",
    "\n",
    "user_info_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Order Bias Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The response produced by Llama is better because it is more definitive in its conclusion that handling extensive amounts of text is not necessary. It provides specific reasons for its conclusion, such as the fact that the data is already in a structured format and the task is to categorize the complaints into 50 types, which does not require processing large amounts of unstructured text. The response by mixtral is more speculative and does not provide as much certainty in its conclusion. It suggests that there may be a need for handling extensive amounts of text, but does not provide specific reasons for this conclusion. Therefore, the response by Llama is more helpful in determining whether a user query requires handling large amounts of text.\n",
      "The best response is the one produced by Llama. The user input does not suggest a need for handling extensive amounts of text to be ingested or produced. The data is already in a structured format (CSV) and the task is to categorize the complaints into 50 types, which does not require processing large amounts of unstructured text. Additionally, the input does not mention any requirements for text generation or output, which further suggests that handling extensive amounts of text is not necessary.\n",
      "\n",
      "The response produced by mixtral is less clear and more speculative. While it is true that the complaints will need to be categorized into a list of 50 complaint types, it is not necessarily true that this will require handling extensive amounts of text. The fact that the data is in CSV format suggests that it is already structured and does not require text analysis. Therefore, the response produced by mixtral is less accurate and more speculative than the response produced by Llama.\n"
     ]
    }
   ],
   "source": [
    "context_length_prompt_template_A = \"\"\" \n",
    "Two LLMs were each tasked with evaluation whether a user query requires handling large amounts of text.\n",
    "Determine which response is better and explain your reasoning.\n",
    "Here is the response produced by Llama: {llama_response}.\n",
    "Here is the response produced by mixtral: {mixtral_response}\n",
    "Here are the original user requirements: {user_requirements}\n",
    "\"\"\"\n",
    "context_length_prompt_template_B = \"\"\" \n",
    "Two LLMs were each tasked with evaluation whether a user query requires handling large amounts of text.\n",
    "Determine which response is better and explain your reasoning.\n",
    "Here is the response produced by mixtral: {mixtral_response}\n",
    "Here is the response produced by Llama: {llama_response}.\n",
    "Here are the original user requirements: {user_requirements}\n",
    "\"\"\"\n",
    "\n",
    "response_from_orderA = mixtral.invoke(context_length_prompt_template_A.format(llama_response=llama_response, mixtral_response=mixtral_response, user_requirements=user_request))\n",
    "response_from_orderB = mixtral.invoke(context_length_prompt_template_B.format(llama_response=llama_response, mixtral_response=mixtral_response, user_requirements=user_request))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The response produced by Llama is better because it is more definitive in its conclusion that handling extensive amounts of text is not necessary. It provides specific reasons for its conclusion, such as the fact that the data is already in a structured format and the task is to categorize the complaints into 50 types, which does not require processing large amounts of unstructured text. The response by mixtral is more speculative and does not provide as much certainty in its conclusion. It suggests that there may be a need for handling extensive amounts of text, but does not provide specific reasons for this conclusion. Therefore, the response by Llama is more helpful in determining whether a user query requires handling large amounts of text.\n"
     ]
    }
   ],
   "source": [
    "print(response_from_orderA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best response is the one produced by Llama. The user input does not suggest a need for handling extensive amounts of text to be ingested or produced. The data is already in a structured format (CSV) and the task is to categorize the complaints into 50 types, which does not require processing large amounts of unstructured text. Additionally, the input does not mention any requirements for text generation or output, which further suggests that handling extensive amounts of text is not necessary.\n",
      "\n",
      "The response produced by mixtral is less clear and more speculative. While it is true that the complaints will need to be categorized into a list of 50 complaint types, it is not necessarily true that this will require handling extensive amounts of text. The fact that the data is in CSV format suggests that it is already structured and does not require text analysis. Therefore, the response produced by mixtral is less accurate and more speculative than the response produced by Llama.\n"
     ]
    }
   ],
   "source": [
    "print(response_from_orderB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama_response_from_orderA = llama_70b.invoke(context_length_prompt_template_A.format(llama_response=llama_response, mixtral_response=mixtral_response, user_requirements=user_request))\n",
    "llama_response_from_orderB = llama_70b.invoke(context_length_prompt_template_B.format(llama_response=llama_response, mixtral_response=mixtral_response, user_requirements=user_request))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: The response produced by Llama is better.\n",
      "\n",
      "Reasoning:\n",
      "\n",
      "Llama's response accurately assesses the user's requirements and correctly determines that handling large amounts of text is not necessary. The user input clearly states that the data is already in a structured format (CSV) and the task is to categorize the complaints into 50 types, which does not require processing large amounts of unstructured text. Additionally, Llama notes that the input does not mention any requirements for text generation or output, further supporting the conclusion that handling extensive amounts of text is not necessary.\n",
      "\n",
      "On the other hand, mixtral's response is less accurate. While it acknowledges that the user input does not explicitly mention the need for handling extensive amounts of text, it suggests that there may be a need for it based on the fact that the bank is receiving a large number of complaints and the data is in CSV format. However, this is not sufficient reason to assume that handling large amounts of text is necessary. mixtral also mentions that the complaints will need to be categorized into a list of 50 complaint types, but this does not necessarily imply that extensive text analysis\n"
     ]
    }
   ],
   "source": [
    "print(llama_response_from_orderA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer:\n",
      "The response produced by Llama is better.\n",
      "\n",
      "Reasoning:\n",
      "Llama's response accurately assesses the user's requirements and correctly determines that handling large amounts of text is not necessary. The data is already in a structured format (CSV), and the task is to categorize the complaints into 50 types, which can be done without processing large amounts of unstructured text. Additionally, the user requirements do not mention any requirements for text generation or output, further supporting Llama's conclusion.\n",
      "\n",
      "In contrast, Mixtral's response is less accurate. While it acknowledges that the user input does not explicitly mention the need for handling extensive amounts of text, it suggests that there may be a need for it based on the fact that the bank is receiving a large number of complaints and the data is in CSV format. However, this is not sufficient reason to assume that handling large amounts of text is necessary. Furthermore, Mixtral's response does not provide a clear conclusion or recommendation, whereas Llama's response clearly states that handling extensive amounts of text is not necessary.\n"
     ]
    }
   ],
   "source": [
    "print(llama_response_from_orderB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_preferences_prompt_template = \"\"\" \n",
    "From the user input, determine their position on how they prioritise cost vs performance.\n",
    "Here is the user input: {user_request}. The solution needs to be cost effective.\n",
    "Explain your reasoning concisely.\n",
    "Only use the information given.\n",
    "\"\"\"\n",
    "\n",
    "mixtral_response = mixtral.invoke(user_preferences_prompt_template.format(user_request=user_request))\n",
    "llama_response = llama_70b.invoke(user_preferences_prompt_template.format(user_request=user_request))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nThe user prioritises cost over performance.\\n\\nThe user wants to categorise complaints into a list of 50 complaint types.\\nThey want to use a model to categories the root cause issues.\\nThe data is in CSV format, stored in the AW3 cloud environment.\\nThe solution needs to be cost effective.\\n\\nThe user can use a pre-trained model, such as BERT, to categorise the complaints.\\nThis would be a cost-effective solution, as they would not need to train their own model.\\nAdditionally, using a pre-trained model would likely be faster than training their own model, as the pre-trained model has already been trained on a large dataset.\\n\\nHowever, the user may not get the same level of performance as they would with a custom model.\\nThe pre-trained model may not be as accurate in categorising the complaints as a custom model.\\nAdditionally, the pre-trained model may not be able to handle the specific format of the user's data, as it is in CSV format and stored in the AW3 cloud environment.\\n\\nTherefore, the user needs to weigh the cost savings of using a pre-trained model against the\""
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mixtral_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nMy reasoning is as follows:\\nThe user has specified that the solution needs to be cost-effective, which suggests that they prioritize cost over performance. They also mention that the data is in CSV format and stored in the AW3 cloud environment, which suggests that they are looking for a solution that is easy to implement and does not require a lot of additional resources.\\n\\nTherefore, I would recommend a solution that uses a simple machine learning algorithm, such as a decision tree or random forest, to categorize the complaints. These algorithms are relatively easy to implement and can be trained on the CSV data without requiring a lot of additional resources. Additionally, they are relatively inexpensive to train and deploy compared to more complex algorithms or LLMs.\\n\\nIn summary, based on the user input, I would prioritize cost over performance and recommend a simple machine learning algorithm to categorize the complaints in a cost-effective manner.'"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llama_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "- Application to a Specific Request: Apply the above instructions to analyse and convert the given user request into structured requirements. Ensure to adapt the analysis based on the specific details and needs presented in the user's request.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "savenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
