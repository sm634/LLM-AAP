{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from ibm_watson_machine_learning.foundation_models import Model\n",
    "from ibm_watson_machine_learning.metanames import GenTextParamsMetaNames as GenParams\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "api_key = os.getenv(\"API_KEY\", None)\n",
    "ibm_cloud_url = os.getenv(\"IBM_CLOUD_URL\", None)\n",
    "project_id = os.getenv(\"PROJECT_ID\", None)\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\", None)\n",
    "if api_key is None or ibm_cloud_url is None or project_id is None:\n",
    "    print(\"Ensure you copied the .env file that you created earlier into the same directory as this notebook\")\n",
    "else:\n",
    "    creds = {\n",
    "        \"url\": ibm_cloud_url,\n",
    "        \"apikey\": api_key \n",
    "    }\n",
    "\n",
    "model_params = {\n",
    "        GenParams.DECODING_METHOD: \"greedy\",\n",
    "        GenParams.MIN_NEW_TOKENS: 30,\n",
    "        GenParams.MAX_NEW_TOKENS: 2000,\n",
    "        GenParams.RANDOM_SEED: None,\n",
    "        GenParams.TEMPERATURE: 0,\n",
    "        GenParams.REPETITION_PENALTY: 1.05\n",
    "    }\n",
    "\n",
    "\n",
    "    # Instantiate a model proxy object to send your requests\n",
    "llm = Model(\n",
    "    model_id=\"ibm-mistralai/mixtral-8x7b-instruct-v01-q\",\n",
    "    params=model_params,\n",
    "    credentials=creds,\n",
    "    project_id=project_id)\n",
    "\n",
    "llm_to_langchain=llm.to_langchain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt_to_llm(prompt_template, **variables):\n",
    "    return llm_to_langchain.invoke(prompt_template.format(**variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommendation_prompt = '''\n",
    "[INST] \n",
    "You are a helpful assistant. Your task is to generate a recommendation on which large languege model would be a better fit based on the following information: {top_n_models}.  \n",
    "Just generate the recommendation based on the mentioned models:\n",
    "[/INST] \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_n_models = \"\"\"\n",
    "Searching for the models that best match the following description:\n",
    "{'original_request': '\\nA bank is receiving 100s of complaints a day.\\nThe data is in CSV format, stored in the AW3 cloud environment.\\nThey want to inject data into either an algorithm or an LLM, which can categorise the complaints into a list of 50 complaint types.\\nModel to categories the root cause issues.\\n',  'requirements': ['Language: English',    'Task(s): text classification',    'Function Calling:\\nThe user requirements do not imply a need for the production of structured outputs. The user wants to inject data into an algorithm or a large language model (LLM) to categorize complaints into a list of 50 complaint types. The output of this process would likely be a list or a set of categories, not a structured output like JSON or YAML.',    'Context Length: not a priority',    'Model Architecture: \\nEncoder-only',    'Industry: Banking'],  'key_words': 'Complaints\\nCSV\\nAW3\\nAlgorithm\\nLLM\\nCategorize\\nRoot Cause Issues\\n\\n\\n\\n\\n\\n'}  \n",
    "\n",
    "1: \"{\\n  \\\"ModelName\\\": \\\"Granite (13B) Instruct V2.0\\\",\\n  \\\"Size\\\": \\\"13 Billion\\\",\\n  \\\"ID\\\": \\\"granite.13b.v2.instruct\\\",\\n  \\\"Provider\\\": \\\"IBM\\\",\\n  \\\"Architecture\\\": \\\"Decoder-only\\\",\\n  \\\"ContextLength\\\": 8192,\\n  \\\"Price\\\": \\\"Not specified\\\",\\n  \\\"Languages\\\": [\\\"English\\\"],\\n  \\\"TunningInformation\\\": \\\"Instruction-tuned\\\",\\n  \\\"TrainingData\\\": \\\"2.5T Tokens, MMLU (5-shot), granite.13b.2500b.chat\\\",\\n  \\\"UsesSupported\\\": [\\\"English based classification\\\", \\\"extraction\\\", \\\"summarization\\\"],\\n  \\\"OptimisedFor\\\": \\\"Not specified\\\",\\n  \\\"PromptingAdvice\\\": \\\"The model benefits from flan-templated data in its alignment step and therefore should perform well with flan-style prompt templates.\\\",\\n  \\\"Output\\\": \\\"Not specified\\\",\\n  \\\"PromptTuningAvailability\\\": true,\\n  \\\"RegionalAvailability\\\": \\\"Not specified\\\",\\n  \\\"License\\\": \\\"Available only through IBM products and offerings.\\\"\\n}\"  \n",
    "\n",
    "Score: 0.591354250907898\n",
    "\n",
    "2: \"{\\n  \\\"ModelName\\\": \\\"Granite 13 Billion Model (granite.13b)\\\",\\n  \\\"Size\\\": \\\"13 Billion\\\",\\n  \\\"ID\\\": \\\"Granite13B\\\",\\n  \\\"Provider\\\": \\\"IBM\\\",\\n  \\\"Architecture\\\": \\\"Decoder-only\\\",\\n  \\\"ContextLength\\\": 8000,\\n  \\\"Price\\\": 0.0,\\n  \\\"Languages\\\": [\\\"English\\\"],\\n  \\\"TunningInformation\\\": \\\"Supervised Fine-Tuning (SFT) and Contrastive Fine Tuning (CFT)\\\",\\n  \\\"TrainingData\\\": \\\"Internet, academic, code, legal, and finance data\\\",\\n  \\\"UsesSupported\\\": [\\\"Question and answer\\\", \\\"Generation\\\", \\\"Extraction\\\", \\\"Summarization\\\", \\\"Classification\\\"],\\n  \\\"OptimisedFor\\\": \\\"Business use\\\",\\n  \\\"PromptingAdvice\\\": \\\"Follow the guidelines provided by IBM\\\",\\n  \\\"Output\\\": \\\"Text generation\\\",\\n  \\\"PromptTuningAvailability\\\": true,\\n  \\\"RegionalAvailability\\\": \\\"Worldwide\\\",\\n  \\\"License\\\": \\\"Available only through IBM products and offerings. Contact IBM for licensing terms.\\\"\\n}\"  \n",
    "\n",
    "Score: 0.5875870585441589\n",
    "\n",
    "3: \"{\\n  \\\"ModelName\\\": \\\"Prometheus\\\",\\n  \\\"Size\\\": \\\"13B\\\",\\n  \\\"ID\\\": \\\"Prometheus-13b-v1.0\\\",\\n  \\\"Provider\\\": \\\"kaist-ai\\\",\\n  \\\"Architecture\\\": \\\"Transformers\\\",\\n  \\\"ContextLength\\\": 2048,\\n  \\\"Price\\\": 0.12,\\n  \\\"Languages\\\": [\\\"English\\\"],\\n  \\\"TunningInformation\\\": \\\"Fine-tuned on 100K feedback within the Feedback Collection\\\",\\n  \\\"TrainingData\\\": \\\"Not specified\\\",\\n  \\\"UsesSupported\\\": [\\\"Text2Text Generation\\\", \\\"Inference Endpoints\\\"],\\n  \\\"OptimisedFor\\\": \\\"Language model using Llama-2-Chat as a base model and fine-tuned for evaluating long-form responses\\\",\\n  \\\"PromptingAdvice\\\": \\\"Follow the specific prompt format required by the model\\\",\\n  \\\"Output\\\": \\\"Detailed feedback and a score between 1 and 5\\\",\\n  \\\"PromptTuningAvailability\\\": true,\\n  \\\"RegionalAvailability\\\": \\\"Not specified\\\",\\n  \\\"License\\\": \\\"Apache 2.0\\\"\\n}\"  \n",
    "\n",
    "Score: 0.5865578055381775\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the given information and the provided models, I would recommend the \"Granite 13 Billion Model (granite.13b)\" from IBM. This model meets all the specified requirements: it is an English text classification model with an encoder-only architecture, and it has been trained on relevant data including finance data. It also supports supervised fine-tuning and contrastive fine tuning, which could be beneficial for categorizing complaints into specific types. Additionally, it is available worldwide and is optimized for business use.\n"
     ]
    }
   ],
   "source": [
    "final_recommendation = prompt_to_llm(recommendation_prompt, top_n_models=top_n_models)\n",
    "\n",
    "print(final_recommendation)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
