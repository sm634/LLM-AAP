Model,Size (parameters),ID,Provider,Architecture,Context Length,Price,Languages,Instruction-tuned,Training Data,Uses supported,Optimised for,Prompting Advice,Output
granite-13b-chat-v2,13 000 000 000,'ibm/granite-13b-chat-v2',IBM,Decoder-only,8192,0.0018,English,Yes,"Trained on enterprise-relevant data sets from five domains: internet, academic, code, legal, and finance. Data used to train the models first undergoes IBM data governance reviews and is filtered of text that is flagged for hate, abuse, or profanity by the IBM-developed HAP filter.","Q&A, Summarization, Classification, Generation, Extraction, Retrieval-Augmented Generation",Dialogue use cases including virtual agent and chat applications.,Uses a model-specific prompt format. Includes a keyword in its output that can be used as a stop sequence to produce succinct answers.,Generates dialogue output like a chatbot.
granite-13b-instruct-v2,13 000 000 000 ,'ibm/granite-13b-instruct-v2',IBM,Decoder-only,8192,0.0018,English,Yes,"Trained on enterprise-relevant data sets from five domains: internet, academic, code, legal, and finance. Data used to train the models first undergoes IBM data governance reviews and is filtered of text that is flagged for hate, abuse, or profanity by the IBM-developed HAP filter.","Q&A, Summarization, Classification, Generation, Extraction, Retrieval-Augmented Generation","Finance tasks and providing sentiment scores for stock and earnings call transcripts, classifying news headlines, extracting credit risk assessments, summarizing financial long-form text, and answering financial or insurance-related questions. Supports extraction, summarization, and classification tasks.",Uses a model-specific prompt format.,"Accepts special characters, which can be used for generating structured output."
llama-2-70b-chat,70 000 000 000,'meta-llama/llama-2-70b-chat',Meta,Encoder-Decoder,4096,0.0018,English,Yes,Pretrained on 2 trillion tokens of data from publicly available sources. Fine-tuning data includes publicly available instruction data sets and more than one million new examples that were annotated by humans. The tuned versions use supervised fine-tuning and reinforcement learning with human feedback.,"Q&A, Summarization, Classification, Generation, Extraction, Retrieval-Augmented Generation",not specified,Uses a model-specific prompt format. ,Generates dialogue output like a chatbot. 
llama-2-13b-chat,13 000 000 000,'meta-llama/llama-2-13b-chat',Meta,Encoder-Decoder,4096,0.0006,English,Yes,Llama 2 was pretrained on 2 trillion tokens of data from publicly available sources. The fine-tuning data includes publicly available instruction data sets and more than one million new examples that were annotated by humans. The tuned versions use supervised fine-tuning and reinforcement learning with human feedback.,"Q&A, Summarization, Classification, Generation, Extraction, Retrieval-Augmented Generation",not specified,Uses a model-specific prompt format.,Generates dialogue output like a chatbot. 
flan-t5-xl-3b,3 000 000 000,'google/flan-t5-xl',Google,Encoder-Decoder,4096,0.0006,"English, German, French",Yes,The model was fine-tuned on tasks that involve multiple-step reasoning from chain-of-thought data in addition to traditional natural language processing tasks. ,"Q&A, Summarization, Classification, Generation, Extraction, Retrieval-Augmented Generation",Reasoning tasks (due to fine-tuning with chain-of-thought data).,Great zero- and few-shot performance (due to instruction fine-tuning) ,not specified
flan-t5-xxl-11b,11 000 000 000,'google/flan-t5-xxl',Google,Encoder-Decoder,4096,0.0018,"English, German, French",Yes,The model was fine-tuned on tasks that involve multiple-step reasoning from chain-of-thought data in addition to traditional natural language processing tasks.,"Q&A, Summarization, Classification, Generation, Extraction, Retrieval-Augmented Generation",not specified,Great zero- and few-shot performance (due to instruction fine-tuning) ,not specified
flan-ul2-20b,20 000 000 000,'google/flan-ul2',Google,Encoder-Decoder,4096,0.005,English,Yes,Pretrained on the cleaned version of Common Crawl's web crawl corpus and fine-tuned with multiple pretraining objectives to optimize it for various natural language processing tasks.,"Q&A, Summarization, Classification, Generation, Extraction, Retrieval-Augmented Generation","The model is optimized for language generation, language understanding, text classification, question answering, common sense reasoning, long text reasoning, structured-knowledge grounding, and information retrieval, in-context learning.","Great for zero-shot prompting, and one-shot prompting.",not specified
elyza-japanese-llama-2-7b-instruct,7 000 000 000,'elyza/elyza-japanese-llama-2-7b-instruct',ELYZA,Decoder-only,4096,0.0018,"Japanese, English",Yes,"For Japanese language training, Japanese text from many sources were used, including Wikipedia and the Open Super-large Crawled ALMAnaCH coRpus (a multilingual corpus that is generated by classifying and filtering language in the Common Crawl corpus). The model was fine-tuned on a dataset that was created by ELYZA. The ELYZA Tasks 100 dataset contains 100 diverse and complex tasks that were created manually and evaluated by humans. The ELYZA Tasks 100 dataset is publicly available from HuggingFace.","Q&A, Summarization, Classification, Generation, Extraction, Retrieval-Augmented Generation tasks,  Translation",Works well for classification and extraction in Japanese and for translation between English and Japanese. ,Made for general use with zero- or few-shot prompts. Performs best when prompted in Japanese.,not specified
gpt-neox-20b,20 000 000 000,'eleutherai/gpt-neox-20b',EleutherAI,Decoder-only,8192,0.005,English,No,Has not been fine-tuned for downstream tasks.,"Generation, Summarization, Classification, Structured output and special characters",not specified,Works best with few-shot prompts.," Accepts special characters, which can be used for generating structured output. The data set used for training contains profanity and offensive text. Be sure to curate any output from the model before using it in an application."
mpt-7b-instruct2,7 000 000 000,ibm/mpt-7b-instruct2',"Mosaic, fine-tuned by IBM",Encoder-decoder,2048,0.0006,English,Yes,"The dataset that was used to train this model is a combination of the Dolly dataset from Databrick and a filtered subset of the Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback training data from Anthropic. During filtering, parts of dialog exchanges that contain instruction-following steps were extracted to be used as samples.","Q&A, Generation",not specified,Optimised for following short-form instructions.,not specified
mt0-xxl-13b,13 000 000 000,'bigscience/mt0-xxl',BigScience,Encoder-Decoder,4096,0.0018,Multilingual,Yes,Pretrained on multilingual data in 108 languages and fine-tuned with multilingual data in 46 languages to perform multilingual tasks.,"Q&A, Summarization, Classification, Generation",Language generation and translation tasks with English in many different languages. ,"Supports multilingual prompts. For translation tasks, include a period to indicate the end of the text you want translated or the model might continue the sentence rather than translate it. For general use with zero- or few-shot prompts.",not specified
starcoder-15.5b ,15 500 000 000 ,bigcode/starcoder',BigCode,Decoder-only,8192,0.0018,Programming languages,No,Not instruction-tuned.,"Code generation, Code conversion, Translating code from natural language prompt","This model can generate code and convert code from one programming language to another. A filter was applied to exclude from the training data any licensed code or code that is marked with opt-out requests. Nevertheless, the model's output might include code from its training data that requires attribution. ",Include examples in your prompt.,not specified