["{\\n  \\"ModelName\\": \\"Code Llama\\",\\n  \\"Size\\": \\"70B\\",\\n  \\"ID\\": \\"CodeLlama - 70B\\",\\n  \\"Provider\\": \\"Meta\\",\\n  \\"Architecture\\": \\"Decoder-only\\",\\n  \\"ContextLength\\": 100000,\\n  \\"Price\\": \\"Free\\",\\n  \\"Languages\\": [\\"Python\\", \\"C++\\", \\"Java\\", \\"PHP\\", \\"Typescript (Javascript)\\", \\"C#\\", \\"Bash\\"],\\n  \\"TunningInformation\\": \\"Instruction-tuned\\",\\n  \\"TrainingData\\": \\"Code and code-related data\\",\\n  \\"UsesSupported\\": [\\"Code generation\\", \\"natural language about code\\", \\"debugging\\"],\\n  \\"OptimisedFor\\": \\"Low latency, real-time code completion, long inputs\\",\\n  \\"PromptingAdvice\\": \\"Provide the model with code or natural language prompts\\",\\n  \\"Output\\": \\"Code and natural language about code\\",\\n  \\"PromptTuningAvailability\\": true,\\n  \\"RegionalAvailability\\": \\"Not specified\\",\\n  \\"License\\": \\"Community license as Llama 2\\"\\n}", "{\\n  \\"ModelName\\": \\"Deepseek Coder\\",\\n  \\"Size\\": \\"33B\\",\\n  \\"ID\\": \\"deepseek-coder-33b-instruct\\",\\n  \\"Provider\\": \\"deepseek-ai\\",\\n  \\"Architecture\\": \\"Decoder-only\\",\\n  \\"ContextLength\\": 16000,\\n  \\"Price\\": \\"Not specified\\",\\n  \\"Languages\\": [\\"Python\\", \\"English\\", \\"Chinese\\"],\\n  \\"TunningInformation\\": \\"Instruction-tuned\\",\\n  \\"TrainingData\\": \\"Project-level code corpus\\",\\n  \\"UsesSupported\\": [\\"Code generation\\", \\"Code completion\\", \\"Code infilling\\"],\\n  \\"OptimisedFor\\": \\"Project-level code completion and infilling\\",\\n  \\"PromptingAdvice\\": \\"Provide the model with code or natural language prompts\\",\\n  \\"Output\\": \\"Code and natural language\\",\\n  \\"PromptTuningAvailability\\": false,\\n  \\"RegionalAvailability\\": \\"Not specified\\",\\n  \\"License\\": \\"Other (see the LICENSE-MODEL for more details)\\"\\n}", "{\\n  \\"ModelName\\": \\"Falcon-180B\\",\\n  \\"Size\\": \\"180B\\",\\n  \\"ID\\": \\"tiiuae/falcon-180B\\",\\n  \\"Provider\\": \\"TII\\",\\n  \\"Architecture\\": \\"Decoder-only\\",\\n  \\"ContextLength\\": 2048,\\n  \\"Price\\": \\"Unknown\\",\\n  \\"Languages\\": [\\"English\\", \\"German\\", \\"Spanish\\", \\"French\\"],\\n  \\"TunningInformation\\": \\"Raw, pretrained model\\",\\n  \\"TrainingData\\": \\"RefinedWeb enhanced with curated corpora\\",\\n  \\"UsesSupported\\": [\\"Research\\", \\"Finetuning\\", \\"Specialization\\"],\\n  \\"OptimisedFor\\": \\"Inference\\",\\n  \\"PromptingAdvice\\": \\"Consider finetuning for specific usecases\\",\\n  \\"Output\\": \\"Text generation\\",\\n  \\"PromptTuningAvailability\\": false,\\n  \\"RegionalAvailability\\": \\"Publicly accessible\\",\\n  \\"License\\": \\"Falcon-180B TII License and Acceptable Use Policy\\"\\n}", "{\\n  \\"ModelName\\": \\"Falcon-40B\\",\\n  \\"Size\\": \\"40B\\",\\n  \\"ID\\": \\"tiiuae/falcon-40b\\",\\n  \\"Provider\\": \\"TII\\",\\n  \\"Architecture\\": \\"Decoder-only\\",\\n  \\"ContextLength\\": 2048,\\n  \\"Price\\": \\"Not specified\\",\\n  \\"Languages\\": [\\"English\\", \\"German\\", \\"Spanish\\", \\"French\\", \\"Italian\\", \\"Portuguese\\", \\"Polish\\", \\"Dutch\\", \\"Romanian\\", \\"Czech\\", \\"Swedish\\"],\\n  \\"TunningInformation\\": \\"Raw, pretrained model\\",\\n  \\"TrainingData\\": \\"1,000B tokens of RefinedWeb enhanced with curated corpora\\",\\n  \\"UsesSupported\\": [\\"Research\\", \\"Finetuning\\", \\"Specialization\\"],\\n  \\"OptimisedFor\\": \\"Inference\\",\\n  \\"PromptingAdvice\\": \\"Consider finetuning for specific usecases\\",\\n  \\"Output\\": \\"Text generation\\",\\n  \\"PromptTuningAvailability\\": false,\\n  \\"RegionalAvailability\\": \\"Not specified\\",\\n  \\"License\\": \\"Apache 2.0 license\\"\\n}", "{\\n  \\"ModelName\\": \\"FLAN-T5 XL\\",\\n  \\"Size\\": \\"2.85B params\\",\\n  \\"ID\\": \\"google/flan-t5-xl\\",\\n  \\"Provider\\": \\"Hugging Face\\",\\n  \\"Architecture\\": \\"Transformers\\",\\n  \\"ContextLength\\": 512,\\n  \\"Price\\": \\"Not specified\\",\\n  \\"Languages\\": [\\"English\\", \\"Spanish\\", \\"Japanese\\", \\"Persian\\", \\"Hindi\\", \\"French\\", \\"Chinese\\", \\"Bengali\\", \\"Gujarati\\", \\"German\\", \\"Telugu\\", \\"Italian\\", \\"Arabic\\", \\"Polish\\", \\"Tamil\\", \\"Marathi\\", \\"Malayalam\\", \\"Oriya\\", \\"Panjabi\\", \\"Portuguese\\", \\"Urdu\\", \\"Galician\\", \\"Hebrew\\", \\"Korean\\", \\"Catalan\\", \\"Thai\\", \\"Dutch\\", \\"Indonesian\\", \\"Vietnamese\\", \\"Bulgarian\\", \\"Filipino\\", \\"Central Khmer\\", \\"Lao\\", \\"Turkish\\", \\"Russian\\", \\"Croatian\\", \\"Swedish\\", \\"Yoruba\\", \\"Kurdish\\", \\"Burmese\\", \\"Malay\\", \\"Czech\\", \\"Finnish\\", \\"Somali\\", \\"Tagalog\\", \\"Swahili\\", \\"Sinhala\\", \\"Kannada\\", \\"Zhuang\\", \\"Igbo\\", \\"Xhosa\\", \\"Romanian\\", \\"Haitian\\", \\"Estonian\\", \\"Slovak\\", \\"Lithuanian\\", \\"Greek\\", \\"Nepali\\", \\"Assamese\\", \\"Norwegian\\"],\\n  \\"TunningInformation\\": \\"Instruction-tuned\\",\\n  \\"TrainingData\\": \\"A mixture of tasks including those in the table below (from the original paper, figure 2)\\",\\n  \\"UsesSupported\\": [\\"Research on zero-shot NLP tasks and in-context few-shot learning\\", \\"NLP tasks such as reasoning, and question answering\\", \\"Advancing fairness and safety research\\", \\"Understanding limitations of current large language models\\"],\\n  \\"OptimisedFor\\": \\"Not specified\\",\\n  \\"PromptingAdvice\\": \\"Provide the model with appropriate prompts\\",\\n  \\"Output\\": \\"Text\\",\\n  \\"PromptTuningAvailability\\": true,\\n  \\"RegionalAvailability\\": \\"Not specified\\",\\n  \\"License\\": \\"Apache 2.0\\"\\n}", "{\\n  \\"ModelName\\": \\"FLAN-T5 XXL\\",\\n  \\"Size\\": \\"11.3B params\\",\\n  \\"ID\\": \\"google/flan-t5-xxl\\",\\n  \\"Provider\\": \\"Google\\",\\n  \\"Architecture\\": \\"Transformer\\",\\n  \\"ContextLength\\": 512,\\n  \\"Price\\": \\"Not specified\\",\\n  \\"Languages\\": [\\"English\\", \\"German\\", \\"French\\"],\\n  \\"TunningInformation\\": \\"Instruction-tuned\\",\\n  \\"TrainingData\\": \\"A mixture of tasks including reasoning, question answering, and more\\",\\n  \\"UsesSupported\\": [\\"Research on language models\\", \\"Zero-shot NLP tasks\\", \\"In-context few-shot learning\\", \\"Advancing fairness and safety research\\"],\\n  \\"OptimisedFor\\": \\"Not specified\\",\\n  \\"PromptingAdvice\\": \\"Not specified\\",\\n  \\"Output\\": \\"Language model generation\\",\\n  \\"PromptTuningAvailability\\": false,\\n  \\"RegionalAvailability\\": \\"Not specified\\",\\n  \\"License\\": \\"Apache 2.0\\"\\n}", "{\\n  \\"ModelName\\": \\"Flan-UL2\\",\\n  \\"Size\\": \\"20B\\",\\n  \\"ID\\": \\"google/flan-ul2\\",\\n  \\"Provider\\": \\"Google\\",\\n  \\"Architecture\\": \\"Encoder-decoder\\",\\n  \\"ContextLength\\": 2048,\\n  \\"Price\\": \\"Not specified\\",\\n  \\"Languages\\": [\\"English\\", \\"French\\", \\"German\\", \\"Spanish\\", \\"Italian\\"],\\n  \\"TunningInformation\\": \\"Flan Prompting\\",\\n  \\"TrainingData\\": \\"C4, Flan datasets\\",\\n  \\"UsesSupported\\": [\\"Text generation\\", \\"Question answering\\", \\"Translation\\", \\"Summarization\\"],\\n  \\"OptimisedFor\\": \\"Few-shot in-context learning\\",\\n  \\"PromptingAdvice\\": \\"Provide the model with instruction-based prompts\\",\\n  \\"Output\\": \\"Text\\",\\n  \\"PromptTuningAvailability\\": true,\\n  \\"RegionalAvailability\\": \\"Not specified\\",\\n  \\"License\\": \\"Apache-2.0\\"\\n}", "{\\n  \\"ModelName\\": \\"Jais-13b-chat\\",\\n  \\"Size\\": \\"13B\\",\\n  \\"ID\\": \\"jais-13b-chat\\",\\n  \\"Provider\\": \\"core42\\",\\n  \\"Architecture\\": \\"Decoder-only\\",\\n  \\"ContextLength\\": 2048,\\n  \\"Price\\": \\"Not specified\\",\\n  \\"Languages\\": [\\"Arabic\\", \\"English\\"],\\n  \\"TunningInformation\\": \\"Finetuned\\",\\n  \\"TrainingData\\": \\"Arabic and English prompt-response pairs\\",\\n  \\"UsesSupported\\": [\\"Conversational AI\\"],\\n  \\"OptimisedFor\\": \\"Bilingual conversations\\",\\n  \\"PromptingAdvice\\": \\"Follow the specific prompt format\\",\\n  \\"Output\\": \\"Text responses\\",\\n  \\"PromptTuningAvailability\\": false,\\n  \\"RegionalAvailability\\": \\"Not specified\\",\\n  \\"License\\": \\"Apache 2.0\\"\\n}", "{\\n  \\"ModelName\\": \\"ELYZA-japanese-Llama-2-7b-instruct\\",\\n  \\"Size\\": \\"6.27B\\",\\n  \\"ID\\": \\"ELYZA-japanese-Llama-2-7b-instruct\\",\\n  \\"Provider\\": \\"Meta\\",\\n  \\"Architecture\\": \\"Decoder-only\\",\\n  \\"ContextLength\\": 256,\\n  \\"Price\\": \\"Not specified\\",\\n  \\"Languages\\": [\\"Japanese\\", \\"English\\"],\\n  \\"TunningInformation\\": \\"Instruction-tuned\\",\\n  \\"TrainingData\\": \\"Not specified\\",\\n  \\"UsesSupported\\": [\\"Text generation\\"],\\n  \\"OptimisedFor\\": \\"Not specified\\",\\n  \\"PromptingAdvice\\": \\"Provide the model with text prompts\\",\\n  \\"Output\\": \\"Text\\",\\n  \\"PromptTuningAvailability\\": false,\\n  \\"RegionalAvailability\\": \\"Not specified\\",\\n  \\"License\\": \\"Llama 2 Community License\\"\\n}", "{\\n  \\"ModelName\\": \\"Llama-2\\",\\n  \\"Size\\": \\"13B\\",\\n  \\"ID\\": \\"meta-llama/Llama-2-13b-hf\\",\\n  \\"Provider\\": \\"Meta\\",\\n  \\"Architecture\\": \\"Transformer\\",\\n  \\"ContextLength\\": 4096,\\n  \\"Price\\": \\"Not specified\\",\\n  \\"Languages\\": [\\"English\\"],\\n  \\"TunningInformation\\": \\"Supervised fine-tuning (SFT) and reinforcement learning with human feedback (RLHF)\\",\\n  \\"TrainingData\\": \\"A new mix of publicly available online data\\",\\n  \\"UsesSupported\\": [\\"Assistant-like chat\\"],\\n  \\"OptimisedFor\\": \\"Not specified\\",\\n  \\"PromptingAdvice\\": \\"Follow specific formatting, including INST and > tags, BOS and EOS tokens, and whitespaces and breaklines\\",\\n  \\"Output\\": \\"Text\\",\\n  \\"PromptTuningAvailability\\": false,\\n  \\"RegionalAvailability\\": \\"Not specified\\",\\n  \\"License\\": \\"Custom commercial license available\\"\\n}", "{\\n  \\"ModelName\\": \\"llama2-dpo-v7\\",\\n  \\"Size\\": \\"13.2B\\",\\n  \\"ID\\": \\"mncai/llama2-13b-dpo-v7\\",\\n  \\"Provider\\": \\"MindsAndCompany\\",\\n  \\"Architecture\\": \\"Decoder-only\\",\\n  \\"ContextLength\\": 2048,\\n  \\"Price\\": \\"Not specified\\",\\n  \\"Languages\\": [\\"English\\", \\"Korean\\"],\\n  \\"TunningInformation\\": \\"Instruction-tuned and Differential Privacy Optimized\\",\\n  \\"TrainingData\\": \\"Not specified\\",\\n  \\"UsesSupported\\": [\\"Text generation\\"],\\n  \\"OptimisedFor\\": \\"Not specified\\",\\n  \\"PromptingAdvice\\": \\"Provide the model with text prompts\\",\\n  \\"Output\\": \\"Text\\",\\n  \\"PromptTuningAvailability\\": false,\\n  \\"RegionalAvailability\\": \\"Not specified\\",\\n  \\"License\\": \\"Llama 2 license\\"\\n}", "{\\n  \\"ModelName\\": \\"Mistral-7B-Instruct-v0.2\\",\\n  \\"Size\\": \\"7B\\",\\n  \\"ID\\": \\"Mistral-7B-Instruct-v0.2\\",\\n  \\"Provider\\": \\"Mistral AI\\",\\n  \\"Architecture\\": \\"Transformers\\",\\n  \\"ContextLength\\": 32000,\\n  \\"Price\\": 2.09,\\n  \\"Languages\\": [\\"English\\"],\\n  \\"TunningInformation\\": \\"Finetuned conversational inference\\",\\n  \\"TrainingData\\": \\"Text generation inference\\",\\n  \\"UsesSupported\\": [\\"Text generation\\"],\\n  \\"OptimisedFor\\": \\"Inference Endpoints\\",\\n  \\"PromptingAdvice\\": \\"Surround your prompt with [INST] and [/]\\",\\n  \\"Output\\": \\"Text\\",\\n  \\"PromptTuningAvailability\\": true,\\n  \\"RegionalAvailability\\": \\"Not specified\\",\\n  \\"License\\": \\"Apache-2.0\\"\\n}", "{\\n  \\"ModelName\\": \\"Mixtral-8x7B-Instruct-v0.1\\",\\n  \\"Size\\": \\"8x7B\\",\\n  \\"ID\\": \\"mistralai/Mixtral-8x7B-Instruct-v0.1\\",\\n  \\"Provider\\": \\"Mistral AI\\",\\n  \\"Architecture\\": \\"Sparse Mixture of Experts\\",\\n  \\"ContextLength\\": 2048,\\n  \\"Price\\": 3.74,\\n  \\"Languages\\": [\\"English\\", \\"French\\", \\"German\\", \\"Spanish\\", \\"Italian\\"],\\n  \\"TunningInformation\\": \\"Instruction-tuned\\",\\n  \\"TrainingData\\": \\"Not specified\\",\\n  \\"UsesSupported\\": [\\"Text generation\\"],\\n  \\"OptimisedFor\\": \\"Not specified\\",\\n  \\"PromptingAdvice\\": \\"Use the specific instruction format\\",\\n  \\"Output\\": \\"Text\\",\\n  \\"PromptTuningAvailability\\": true,\\n  \\"RegionalAvailability\\": \\"Not specified\\",\\n  \\"License\\": \\"Apache-2.0\\"\\n}", "{\\n  \\"ModelName\\": \\"Mixtral-8x7B\\",\\n  \\"Size\\": \\"8B\\",\\n  \\"ID\\": \\"TheBloke/Mixtral-8x7B-v0.1-GPTQ\\",\\n  \\"Provider\\": \\"Hugging Face\\",\\n  \\"Architecture\\": \\"Transformer\\",\\n  \\"ContextLength\\": 4096,\\n  \\"Price\\": \\"Not specified\\",\\n  \\"Languages\\": [\\"English\\", \\"French\\", \\"German\\", \\"Italian\\", \\"Spanish\\"],\\n  \\"TunningInformation\\": \\"GPTQ\\",\\n  \\"TrainingData\\": \\"Not specified\\",\\n  \\"UsesSupported\\": [\\"Text generation\\"],\\n  \\"OptimisedFor\\": \\"4-bit precision\\",\\n  \\"PromptingAdvice\\": \\"No specific template\\",\\n  \\"Output\\": \\"Text\\",\\n  \\"PromptTuningAvailability\\": true,\\n  \\"RegionalAvailability\\": \\"Not specified\\",\\n  \\"License\\": \\"Apache-2.0\\"\\n}", "{\\n  \\"ModelName\\": \\"Multilingual-E5-large\\",\\n  \\"Size\\": \\"24 layers, embedding size 1024\\",\\n  \\"ID\\": \\"intfloat/multilingual-e5-large\\",\\n  \\"Provider\\": \\"Hugging Face\\",\\n  \\"Architecture\\": \\"Transformer\\",\\n  \\"ContextLength\\": 512,\\n  \\"Price\\": \\"Not specified\\",\\n  \\"Languages\\": [\\n    \\"Afrikaans\\",\\n    \\"Albanian\\",\\n    \\"Amharic\\",\\n    \\"Arabic\\",\\n    \\"Armenian\\",\\n    \\"Assamese\\",\\n    \\"Azerbaijani\\",\\n    \\"Bashkir\\",\\n    \\"Basque\\",\\n    \\"Belarusian\\",\\n    \\"Bengali\\",\\n    \\"Bosnian\\",\\n    \\"Bulgarian\\",\\n    \\"Burmese\\",\\n    \\"Catalan\\",\\n    \\"Central Khmer\\",\\n    \\"Chinese\\",\\n    \\"Croatian\\",\\n    \\"Czech\\",\\n    \\"Danish\\",\\n    \\"Dutch\\",\\n    \\"English\\",\\n    \\"Esperanto\\",\\n    \\"Estonian\\",\\n    \\"Finnish\\",\\n    \\"French\\",\\n    \\"Galician\\",\\n    \\"Georgian\\",\\n    \\"German\\",\\n    \\"Greek\\",\\n    \\"Gujarati\\",\\n    \\"Hausa\\",\\n    \\"Hebrew\\",\\n    \\"Hindi\\",\\n    \\"Hungarian\\",\\n    \\"Icelandic\\",\\n    \\"Indonesian\\",\\n    \\"Interlingua\\",\\n    \\"Irish\\",\\n    \\"Italian\\",\\n    \\"Japanese\\",\\n    \\"Javanese\\",\\n    \\"Kannada\\",\\n    \\"Kazakh\\",\\n    \\"Khmer\\",\\n    \\"Korean\\",\\n    \\"Kurdish\\",\\n    \\"Kyrgyz\\",\\n    \\"Lao\\",\\n    \\"Latin\\",\\n    \\"Latvian\\",\\n    \\"Lithuanian\\",\\n    \\"Malagasy\\",\\n    \\"Malay\\",\\n    \\"Malayalam\\",\\n    \\"Marathi\\",\\n    \\"Moldavian\\",\\n    \\"Mongolian\\",\\n    \\"Nepali\\",\\n    \\"Norwegian\\",\\n    \\"Persian\\",\\n    \\"Polish\\",\\n    \\"Portuguese\\",\\n    \\"Romanian\\",\\n    \\"Russian\\",\\n    \\"Sanskrit\\",\\n    \\"Serbian\\",\\n    \\"Sindhi\\",\\n    \\"Sinhala\\",\\n    \\"Slovak\\",\\n    \\"Slovenian\\",\\n    \\"Somali\\",\\n    \\"Spanish\\",\\n    \\"Swahili\\",\\n    \\"Swedish\\",\\n    \\"Tajik\\",\\n    \\"Tamil\\",\\n    \\"Telugu\\",\\n    \\"Thai\\",\\n    \\"Tibetan\\",\\n    \\"Turkish\\",\\n    \\"Ukrainian\\",\\n    \\"Urdu\\",\\n    \\"Uzbek\\",\\n    \\"Vietnamese\\",\\n    \\"Welsh\\",\\n    \\"Yiddish\\"\\n  ],\\n  \\"TunningInformation\\": \\"Contrastive pre-training with weak supervision, supervised fine-tuning\\",\\n  \\"TrainingData\\": \\"Filtered mC4 (title, page content), 400M CC News (title, news content), NLLB translation pairs, Wikipedia (hierarchical section title, passage), Filtered Reddit (comment, response), S2ORC (title, abstract) and citation pairs, Stackexchange (question, answer), xP3 (input prompt, response)\\",\\n  \\"UsesSupported\\": [\\n    \\"Feature extraction\\",\\n    \\"Sentence similarity\\",\\n    \\"Inference endpoints\\"\\n  ],\\n  \\"OptimisedFor\\": \\"Multilingual understanding\\",\\n  \\"PromptingAdvice\\": \\"Use \'query:\' and \'passage:\' prefixes for input texts\\",\\n  \\"Output\\": \\"Sentence embeddings\\",\\n  \\"PromptTuningAvailability\\": true,\\n  \\"RegionalAvailability\\": \\"Not specified\\",\\n  \\"License\\": \\"MIT\\"\\n}", "{\\n  \\"ModelName\\": \\"Prometheus\\",\\n  \\"Size\\": \\"13B\\",\\n  \\"ID\\": \\"Prometheus-13b-v1.0\\",\\n  \\"Provider\\": \\"kaist-ai\\",\\n  \\"Architecture\\": \\"Transformers\\",\\n  \\"ContextLength\\": 2048,\\n  \\"Price\\": 0.12,\\n  \\"Languages\\": [\\"English\\"],\\n  \\"TunningInformation\\": \\"Fine-tuned on 100K feedback within the Feedback Collection\\",\\n  \\"TrainingData\\": \\"Not specified\\",\\n  \\"UsesSupported\\": [\\"Text2Text Generation\\", \\"Inference Endpoints\\"],\\n  \\"OptimisedFor\\": \\"Language model using Llama-2-Chat as a base model and fine-tuned for evaluating long-form responses\\",\\n  \\"PromptingAdvice\\": \\"Follow the specific prompt format required by the model\\",\\n  \\"Output\\": \\"Detailed feedback and a score between 1 and 5\\",\\n  \\"PromptTuningAvailability\\": true,\\n  \\"RegionalAvailability\\": \\"Not specified\\",\\n  \\"License\\": \\"Apache 2.0\\"\\n}", "{\\n  \\"ModelName\\": \\"Granite 13 Billion Model (granite.13b)\\",\\n  \\"Size\\": \\"13 Billion\\",\\n  \\"ID\\": \\"Granite13B\\",\\n  \\"Provider\\": \\"IBM\\",\\n  \\"Architecture\\": \\"Decoder-only\\",\\n  \\"ContextLength\\": 8000,\\n  \\"Price\\": 0.0,\\n  \\"Languages\\": [\\"English\\"],\\n  \\"TunningInformation\\": \\"Supervised Fine-Tuning (SFT) and Contrastive Fine Tuning (CFT)\\",\\n  \\"TrainingData\\": \\"Internet, academic, code, legal, and finance data\\",\\n  \\"UsesSupported\\": [\\"Question and answer\\", \\"Generation\\", \\"Extraction\\", \\"Summarization\\", \\"Classification\\"],\\n  \\"OptimisedFor\\": \\"Business use\\",\\n  \\"PromptingAdvice\\": \\"Follow the guidelines provided by IBM\\",\\n  \\"Output\\": \\"Text generation\\",\\n  \\"PromptTuningAvailability\\": true,\\n  \\"RegionalAvailability\\": \\"Worldwide\\",\\n  \\"License\\": \\"Available only through IBM products and offerings. Contact IBM for licensing terms.\\"\\n}", "{\\n  \\"ModelName\\": \\"Granite (13B) Instruct V2.0\\",\\n  \\"Size\\": \\"13 Billion\\",\\n  \\"ID\\": \\"granite.13b.v2.instruct\\",\\n  \\"Provider\\": \\"IBM\\",\\n  \\"Architecture\\": \\"Decoder-only\\",\\n  \\"ContextLength\\": 8192,\\n  \\"Price\\": \\"Not specified\\",\\n  \\"Languages\\": [\\"English\\"],\\n  \\"TunningInformation\\": \\"Instruction-tuned\\",\\n  \\"TrainingData\\": \\"2.5T Tokens, MMLU (5-shot), granite.13b.2500b.chat\\",\\n  \\"UsesSupported\\": [\\"English based classification\\", \\"extraction\\", \\"summarization\\"],\\n  \\"OptimisedFor\\": \\"Not specified\\",\\n  \\"PromptingAdvice\\": \\"The model benefits from flan-templated data in its alignment step and therefore should perform well with flan-style prompt templates.\\",\\n  \\"Output\\": \\"Not specified\\",\\n  \\"PromptTuningAvailability\\": true,\\n  \\"RegionalAvailability\\": \\"Not specified\\",\\n  \\"License\\": \\"Available only through IBM products and offerings.\\"\\n}", "{\\n  \\"ModelName\\": \\"Granite Chat\\",\\n  \\"Size\\": \\"13 Billion\\",\\n  \\"ID\\": \\"granite-13b-chat-v2\\",\\n  \\"Provider\\": \\"IBM\\",\\n  \\"Architecture\\": \\"Decoder-only\\",\\n  \\"ContextLength\\": 8000,\\n  \\"Price\\": \\"Contact IBM for licensing terms\\",\\n  \\"Languages\\": [\\"English\\"],\\n  \\"TunningInformation\\": \\"Massive Multitask Language Understanding (MMLU) and Multi-Turn(MT) Bench benchmarks, novel alignment technique for LLMs using large-scale targeted alignment for a generalist LLM\\",\\n  \\"TrainingData\\": \\"2.5 Trillion Tokens, IBM\'s curated pre-training dataset\\",\\n  \\"UsesSupported\\": [\\"Question and Answering (RAG)\\", \\"summarization\\", \\"generation\\", \\"extraction\\", \\"classification\\"],\\n  \\"OptimisedFor\\": \\"English-based closed-domain Question and Answering, longer responses preferred in RAG-like use cases\\",\\n  \\"PromptingAdvice\\": \\"Proper English text, no tagging or media, text only evaluation datasets\\",\\n  \\"Output\\": \\"English text\\",\\n  \\"PromptTuningAvailability\\": false,\\n  \\"RegionalAvailability\\": \\"Not specified\\",\\n  \\"License\\": \\"Available only through IBM products and offerings. Contact IBM for licensing terms.\\"\\n}"]